# backend/agents/recommend/agent/nodes.py

from __future__ import annotations

import logging
import time
import asyncio
from typing import Any, Dict, Optional
from dataclasses import asdict
from backend.agents.recommend.agent.state import RecommendState
from backend.core.github_core import RepoSnapshot
from backend.agents.recommend.core.ingest.summarizer import ContentSummarizer

summarizer_instance = ContentSummarizer()

logger = logging.getLogger(__name__)

def fetch_snapshot_node(state: RecommendState) -> Dict[str, Any]:
    """
    GitHub ì €ì¥ì†Œ ìŠ¤ëƒ…ìƒ· ìˆ˜ì§‘ ë…¸ë“œ.
    """
    
    # 1. ì¬ì‚¬ìš© ì²´í¬
    if state.repo_snapshot:
        logger.info("Reusing existing repo snapshot")
        return {"step": state.step + 1} 
    
    # 2. í•„ìˆ˜ ì…ë ¥ê°’ ê²€ì¦
    owner = state.owner
    repo = state.repo
    ref = getattr(state, 'ref', 'main') # ref í•„ë“œê°€ ìˆì„ ê²½ìš° ì‚¬ìš©

    # owner, repoê°€ Stateì— ì—†ëŠ” ê²½ìš° ì—ëŸ¬ ì²˜ë¦¬
    if not owner or not repo:
        error_msg = "Owner or repository name is missing in state. (Pre-analysis failure)"
        logger.error(f"Failed to fetch details: {error_msg}")
        return {
            "error": error_msg,
            "failed_step": "fetch_snapshot_node",
            "step": state.step + 1,
        }
    
    from backend.core.github_core import fetch_repo_snapshot

    start_time = time.time()
    
    try:
        
        snapshot = fetch_repo_snapshot(state.owner, state.repo, state.ref)
        
        snapshot_dict = snapshot.model_dump() if hasattr(snapshot, "model_dump") else asdict(snapshot)

        elapsed = round(time.time() - start_time, 3)
        timings = dict(state.timings)
        timings["fetch_snapshot"] = elapsed
        
        logger.info(f"Fetched snapshot for {state.owner}/{state.repo} in {elapsed}s")
        
        return {
            "repo_snapshot": snapshot_dict,
            "timings": timings,
            "step": state.step + 1,
        }
        
    except Exception as e:
        logger.error(f"Failed to fetch snapshot: {e}")
        return { 
            "error": str(e),
            "failed_step": "fetch_snapshot_node",
            "step": state.step + 1,
        }
    
async def analyze_readme_summary_node(state: RecommendState) -> Dict[str, Any]:
    """
    README ë¶„ì„ ë° LLM ìš”ì•½ ë…¸ë“œ.
    
    repo_snapshotì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¡°í™”ëœ LLM ìš”ì•½ì„ ìƒì„±í•˜ê³  DocsCoreResultë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
    """
    
    # 1. ì¬ì‚¬ìš© ì²´í¬ ë° í•„ìˆ˜ ì„ í–‰ ì¡°ê±´ ì²´í¬
    if state.readme_summary:
        logger.info("Reusing existing ingest analysis result")
        return {"step": state.step + 1} 
    
    if not state.repo_snapshot:
        error_msg = "No repo_snapshot available for README analysis."
        logger.error(f"Failed to analyze README: {error_msg}")
        return {
            "error": error_msg,
            "failed_step": "analyze_readme_summary_node",
            "step": state.step + 1,
        }
    
    from backend.core.docs_core import analyze_docs, extract_and_structure_summary_input
    
    start_time = time.time()
    
    try:
        # 2. Stateì—ì„œ ìŠ¤ëƒ…ìƒ· ì¶”ì¶œ
        snapshot_dict = state.repo_snapshot
        readme_content = snapshot_dict.get("readme_content", "")

        # Dict â†’ RepoSnapshot ë³€í™˜
        snapshot_obj = RepoSnapshot(**snapshot_dict)

        # 3. ë¬¸ì„œ ë¶„ì„ (DocsCoreResult dataclass)
        docs_result = analyze_docs(snapshot_obj)

        # dataclass â†’ dict ë³€í™˜
        docs_result_dict = asdict(docs_result)

        # 4. LLM ì…ë ¥ êµ¬ì„±
        llm_input_text = extract_and_structure_summary_input(readme_content)

        # 5. LLM ìš”ì•½ ì‹¤í–‰ (ë¹„ë™ê¸°)
        final_summary = "No summary generated."
        if llm_input_text:
            final_summary = await summarizer_instance.summarize(llm_input_text)
            logger.info("LLM summary generated successfully.")
        else:
            logger.warning("Skipping LLM summary: No structured input generated.")

        # 6. ê²°ê³¼ í†µí•©
        ingest_result = {
            "final_summary": final_summary,
            "docs_analysis": docs_result_dict,
            "readme_word_count": len(readme_content.split()),
            "documentation_quality": docs_result_dict.get("total_score", 0)
        }

        # 7. ìƒíƒœ ì—…ë°ì´íŠ¸ ë°˜í™˜
        elapsed = round(time.time() - start_time, 3)
        timings = dict(state.timings)
        timings["analyze_readme_summary"] = elapsed

        return {
            "readme_summary": ingest_result,
            "timings": timings,
            "step": state.step + 1,
            "failed_step": None,
            "error": None,
        }

    except Exception as e:
        logger.error(f"Failed to analyze and summarize README: {e}")
        return {
            "error": str(e),
            "failed_step": "analyze_readme_summary_node",
            "step": state.step + 1,
        }
    

async def generate_search_query_node(state: RecommendState) -> Dict[str, Any]:
    """
    RAG ê²€ìƒ‰ ì¿¼ë¦¬ ë° í•„í„° ìƒì„± ë…¸ë“œ.
    
    ìƒí™©ì— ë”°ë¼ ë‘ ê°€ì§€ ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤:
    1. URL ë¶„ì„ ë°ì´í„°(repo_snapshot)ê°€ ìˆìŒ -> 'url_analysis' ëª¨ë“œ (ìœ ì‚¬ë„ ê²€ìƒ‰ + Fallback ë¡œì§ ì ìš©)
    2. URL ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŒ -> 'semantic_search' ëª¨ë“œ (ì¼ë°˜ ê²€ìƒ‰)
    """

    from backend.agents.recommend.core.search.rag_query_generator import generate_rag_query_and_filters
    
    start_time = time.time()
    logger.info("ğŸš€ Starting RAG Query Generation Node")

    # 1. ëª¨ë“œ ê²°ì • ë° ë¶„ì„ ë°ì´í„° ì¤€ë¹„
    # readme_summaryê°€ ì—†ë”ë¼ë„, repo_snapshot(ê¸°ë³¸ ì •ë³´)ë§Œ ìˆìœ¼ë©´ ë¶„ì„ ëª¨ë“œë¡œ ì§„ì…í•©ë‹ˆë‹¤.
    if state.repo_snapshot:
        mode = "url_analysis"
        
        # Core í•¨ìˆ˜ì— ë„˜ê²¨ì¤„ ë°ì´í„° íŒ¨í‚¤ì§•
        # state.readme_summaryê°€ Noneì¼ ê²½ìš° ë¹ˆ dictë¡œ ì²˜ë¦¬í•˜ì—¬ ì—ëŸ¬ ë°©ì§€
        analyzed_data = {
            "repo_snapshot": state.repo_snapshot,
            "readme_summary": state.readme_summary if state.readme_summary else {}
        }
    else:
        # ìŠ¤ëƒ…ìƒ·ì¡°ì°¨ ì—†ìœ¼ë©´ ì¼ë°˜ ê²€ìƒ‰ ëª¨ë“œ
        mode = "semantic_search"
        analyzed_data = None

    # 2. ì‚¬ìš©ì ìš”ì²­ í…ìŠ¤íŠ¸ (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì„¤ì •)
    user_input = state.user_request if state.user_request.strip() else "Find similar projects."

    try:
        # 3. Core ë¡œì§ í˜¸ì¶œ (LLM ìˆ˜í–‰)
        result = await generate_rag_query_and_filters(
            user_request=user_input,
            category=mode,
            analyzed_data=analyzed_data
        )

        elapsed = round(time.time() - start_time, 3)
        timings = dict(state.timings)
        timings["generate_query"] = elapsed

        logger.info(f"âœ… Query Generated ({mode}): {result['query']} (Time: {elapsed}s)")

        return {
            "search_query": result.get("query", ""),
            "search_keywords": result.get("keywords", []),
            "search_filters": result.get("filters", {}),
            "timings": timings,
            "step": state.step + 1,
            "error": None
        }

    except Exception as e:
        logger.error(f"âŒ Failed to generate search query: {e}")
        return {
            "error": str(e),
            "failed_step": "generate_search_query_node",
            "step": state.step + 1
        }
    
    
def check_ingest_error_node(state: RecommendState) -> Dict[str, Any]:
    """
    ì—ëŸ¬ ì²´í¬ ë° ë³µêµ¬ ë…¸ë“œ.
    
    í˜„ì¬ ì—ëŸ¬ ìƒíƒœë¥¼ í™•ì¸í•˜ê³  ì¬ì‹œë„ ê°€ëŠ¥ ì—¬ë¶€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
    """
    # 1. ì—ëŸ¬ê°€ ì—†ìœ¼ë©´ ë‹¤ìŒ ë‹¨ê³„(ì—¬ê¸°ì„œëŠ” ì¶œë ¥)ë¡œ ì´ë™
    if not state.error:
        # ì—ëŸ¬ê°€ ì—†ëŠ”ë° ì´ ë…¸ë“œì— ë„ì°©í–ˆë‹¤ë©´, ì¼ë°˜ì ìœ¼ë¡œëŠ” ìµœì¢… ì¶œë ¥ ë…¸ë“œë¡œ ì´ë™í•´ì•¼ í•©ë‹ˆë‹¤.
        # í•˜ì§€ë§Œ, ê·¸ë˜í”„ì˜ ëì´ ëª…í™•í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ì¼ë‹¨ stepë§Œ ì¦ê°€ì‹œí‚µë‹ˆë‹¤.
        return {"step": state.step + 1} 
    
    failed_step = state.failed_step or "unknown"
    retry_count = state.retry_count
    
    logger.warning(f"Error detected in {failed_step}: {state.error}, retry={retry_count}/{state.max_retry}")
    
    # 2. ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ í™•ì¸
    if retry_count >= state.max_retry:
        logger.error(f"Max retries reached for {failed_step}. Cannot recover.")
        # ë³µêµ¬ ë¶ˆê°€ -> Graph ì¢…ë£Œ ë˜ëŠ” ìµœì¢… ì¶œë ¥ìœ¼ë¡œ ë¼ìš°íŒ…
        return {"step": state.step + 1} 
    
    # 3. ì¬ì‹œë„ ê°€ëŠ¥í•œ ë‹¨ê³„ ê²°ì •
    retryable_steps = ["fetch_snapshot_node", "analyze_readme_summary_node"]
    
    if failed_step in retryable_steps:
        logger.info(f"Scheduling retry for {failed_step}")
        return {
            "error": None,          # ì—ëŸ¬ ìƒíƒœ í´ë¦¬ì–´
            "failed_step": failed_step, # ì¬ì‹œë„ í›„ ì´ ë‹¨ê³„ë¡œ ëŒì•„ê°€ë„ë¡ failed_step ìœ ì§€ (ë¼ìš°íŒ…ìš©)
            "retry_count": retry_count + 1,
            "step": state.step + 1, 
        }
    
    # ì¬ì‹œë„ ëª©ë¡ì— ì—†ëŠ” ì—ëŸ¬
    return {"step": state.step + 1}

def route_after_fetch(state: RecommendState) -> str:
    """ìŠ¤ëƒ…ìƒ· ìˆ˜ì§‘ í›„ ë¼ìš°íŒ…."""
    if state.error:
        return "check_ingest_error_node"
    # ì„±ê³µ ì‹œ: ë‹¤ìŒ í•µì‹¬ ë‹¨ê³„ì¸ README ë¶„ì„ìœ¼ë¡œ ì´ë™
    return "analyze_readme_summary_node"


def route_after_analysis(state: RecommendState) -> str:
    """README ë¶„ì„ ë° ìš”ì•½ í›„ ë¼ìš°íŒ…."""
    if state.error:
        return "check_ingest_error_node"
    return "__end__" 


def route_after_error_check(state: RecommendState) -> str:
    """ì—ëŸ¬ ì²´í¬ í›„ ë¼ìš°íŒ…."""
    
    # 1. ì—ëŸ¬ê°€ ë‚¨ì•„ìˆë‹¤ë©´ (ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼)
    if state.error:
        # ë³µêµ¬ ë¶ˆê°€ -> ê·¸ë˜í”„ ì¢…ë£Œ
        return "__end__"
    
    # 2. ì—ëŸ¬ê°€ í´ë¦¬ì–´ë˜ê³  ì¬ì‹œë„ê°€ í•„ìš”í•œ ë‹¨ê³„ê°€ ë‚¨ì•„ìˆëŠ” ê²½ìš°
    failed_step = state.failed_step
    
    if failed_step == "fetch_snapshot_node":
        # fetch_snapshot_node ë…¸ë“œë¡œ ëŒì•„ê°€ ì¬ì‹œë„
        return "fetch_snapshot_node"
    elif failed_step == "analyze_readme_summary_node":
        # analyze_readme_summary_node ë…¸ë“œë¡œ ëŒì•„ê°€ ì¬ì‹œë„
        return "analyze_readme_summary_node"
    
    # 3. ëª¨ë“  ì—ëŸ¬ê°€ ë³µêµ¬ë˜ì—ˆê±°ë‚˜ ì¬ì‹œë„ê°€ ë¶ˆí•„ìš”í•œ ê²½ìš°
    return "__end__"