import logging
import time
import asyncio
from typing import Dict, Any, Optional, List
from dataclasses import asdict
from langgraph.graph import END
from pydantic import ValidationError

from backend.agents.recommend.agent.state import RecommendState
from backend.agents.recommend.core.ingest.summarizer import ContentSummarizer
from backend.core.models import RepoSnapshot
from backend.agents.recommend.core.search.vector_search import vector_search_engine
from backend.agents.recommend.core.analysis.match_score import RepoScorer
from backend.agents.recommend.core.intent_parsing import extract_initial_metadata
from backend.agents.recommend.core.trend.get_trend import trend_service
from backend.agents.recommend.agent.state import CandidateRepo
from backend.agents.recommend.core.search.github_search import GitHubSearch

from langchain_openai import ChatOpenAI
from backend.agents.recommend.config.setting import settings

# â­ï¸ ìˆ˜ì •ë¨: ë¶ˆí•„ìš”í•œ ì„í¬íŠ¸ ì œê±°
# from backend.agents.recommend.core.analysis.final_summary_generator import generate_summary 


# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(name)s | %(message)s')
logger = logging.getLogger("TestRealAgent")

# ------------------------------------------------------------------
# 1. Global Instances
# ------------------------------------------------------------------
try:
    globals()['llm'] = ChatOpenAI(
        base_url=settings.llm.api_base,
        api_key=settings.llm.api_key,
        model=settings.llm.model_name,
        temperature=0
    )

    github_search_instance = GitHubSearch()
    summarizer_instance = ContentSummarizer()
    scorer_instance = RepoScorer()
    logger.info("âœ… Global instances initialized.")
except Exception as e:
    logger.error(f"âŒ Failed to init global instances: {e}")
    exit(1)

# ------------------------------------------------------------------
# 2. Nodes Definition
# ------------------------------------------------------------------

async def parse_initial_request_node(state: RecommendState) -> Dict[str, Any]:
    """
    [ì²« ì‹¤í–‰ ë…¸ë“œ] ì‚¬ìš©ì ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ì˜ë„ì™€ ì •ëŸ‰ì  í•„í„° ì¡°ê±´ë§Œ ì¶”ì¶œí•˜ê³  ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    """
    
    user_request = state.user_request
    repo_url = state.repo_url

    try:
        llm_client = globals()['llm'] 
    except KeyError:
        logger.error("âŒ LLM client ('llm') not initialized in global scope.")
        return {"user_intent": "semantic_search", "quantitative_filters": []}

    try:
        # 2. í•µì‹¬ ë¡œì§ í˜¸ì¶œ (Core Logic ì‹¤í–‰)
        result = await extract_initial_metadata(
            llm_client=llm_client, 
            user_request=user_request,
            repo_url=repo_url
        )
        
        logger.info(f"âœ… Initial Parsing Result: Intent={result.user_intent}, Filters={len(result.quantitative_filters)}")
        
        # 3. LangGraph ìƒíƒœ ì—…ë°ì´íŠ¸ìš© ë§µ ë°˜í™˜
        return {
            "user_intent": result.user_intent,
            "quantitative_filters": result.quantitative_filters
        }
    
    except Exception as e:
        logger.error(f"âŒ Node Execution Failed (parse_initial_request_node): {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ í´ë°± ì²˜ë¦¬
        return {
            "user_intent": "semantic_search",
            "error": f"Initial parsing failed: {e.__class__.__name__}",
            "failed_step": "parse_initial_request_node"
        }

def fetch_snapshot_node(state: RecommendState) -> Dict[str, Any]:
    """GitHub ì €ì¥ì†Œ ìŠ¤ëƒ…ìƒ· ìˆ˜ì§‘"""
    if state.repo_snapshot:
        logger.info("Reusing existing repo snapshot")
        return {"step": state.step + 1} 
    
    owner, repo = state.owner, state.repo
    if not owner or not repo:
        return {"error": "Missing owner/repo", "step": state.step + 1}
    
    from backend.core.github_core import fetch_repo_snapshot
    start_time = time.time()
    
    try:
        snapshot = fetch_repo_snapshot(owner, repo, getattr(state, 'ref', 'main'))
        snapshot_dict = snapshot.model_dump() if hasattr(snapshot, "model_dump") else asdict(snapshot)
        
        elapsed = round(time.time() - start_time, 3)
        timings = dict(state.timings)
        timings["fetch_snapshot"] = elapsed
        
        logger.info(f"Fetched snapshot for {owner}/{repo} in {elapsed}s")
        return {"repo_snapshot": snapshot_dict, "timings": timings, "step": state.step + 1}
        
    except Exception as e:
        logger.error(f"Failed to fetch snapshot: {e}")
        return {"error": str(e), "failed_step": "fetch_snapshot_node", "step": state.step + 1}

async def analyze_readme_summary_node(state: RecommendState) -> Dict[str, Any]:
    """README ë¶„ì„ ë° ìš”ì•½"""
    if state.readme_summary: return {"step": state.step + 1}
    if not state.repo_snapshot: 
        return {"error": "No snapshot", "failed_step": "analyze_readme_summary_node", "step": state.step + 1}
    
    from backend.core.docs_core import analyze_docs, extract_and_structure_summary_input
    start_time = time.time()
    
    try:
        snapshot_dict = state.repo_snapshot
        readme_content = snapshot_dict.get("readme_content") or "" 
        snapshot_obj = RepoSnapshot(**snapshot_dict)

        docs_result = analyze_docs(snapshot_obj)
        docs_result_dict = asdict(docs_result)
        llm_input_text = extract_and_structure_summary_input(readme_content)

        final_summary = "No summary generated."
        if llm_input_text and len(readme_content) > 50:
            final_summary = await summarizer_instance.summarize(llm_input_text)
            logger.info("LLM summary generated successfully.")
        else:
            logger.warning("Skipping LLM summary: README too short/empty.")

        ingest_result = {
            "final_summary": final_summary,
            "docs_analysis": docs_result_dict,
            "readme_word_count": len(readme_content.split()),
            "documentation_quality": docs_result_dict.get("total_score", 0)
        }

        elapsed = round(time.time() - start_time, 3)
        timings = dict(state.timings)
        timings["analyze_readme_summary"] = elapsed

        return {"readme_summary": ingest_result, "timings": timings, "step": state.step + 1, "error": None}

    except Exception as e:
        logger.error(f"Failed to analyze README: {e}")
        return {"error": str(e), "failed_step": "analyze_readme_summary_node", "step": state.step + 1}

async def generate_rag_search_query_node(state: RecommendState) -> Dict[str, Any]:
    """RAG ì¿¼ë¦¬ ìƒì„± (Fallback ë¡œì§ í¬í•¨)"""

    from backend.agents.recommend.core.search.rag_query_generator import generate_rag_query_and_filters
    
    start_time = time.time()
    logger.info("ğŸš€ Starting RAG Query Generation")

    # [ì¤‘ìš”] READMEê°€ ì—†ì–´ë„ ê¸°ë³¸ ì •ë³´ë§Œ ìˆìœ¼ë©´ ë¶„ì„ ëª¨ë“œ ì§„ì…
    if state.repo_snapshot:
        analyzed_data = {
            "repo_snapshot": state.repo_snapshot,
            "readme_summary": state.readme_summary if state.readme_summary else {}
        }
    else:
        analyzed_data = None


    mode = state.user_intent

    user_input = state.user_request if state.user_request.strip() else "Find similar projects."

    try:
        result = await generate_rag_query_and_filters(user_input, mode, analyzed_data)
        
        elapsed = round(time.time() - start_time, 3)
        timings = dict(state.timings)
        timings["generate_query"] = elapsed
        
        logger.info(f"âœ… Query Generated ({mode}): {result['query']}")

        return {
            "search_query": result.get("query", ""),
            "search_keywords": result.get("keywords", []),
            "search_filters": result.get("filters", {}),
            "timings": timings,
            "step": state.step + 1,
            "error": None
        }

    except Exception as e:
        logger.error(f"âŒ Failed to generate query: {e}")
        return {"error": str(e), "failed_step": "generate_rag_search_query_node", "step": state.step + 1}

# =================================================================
# ğŸ‘‡ 4. Vector Search Node (DB ì¡°íšŒ)
# =================================================================
def vector_search_node(state: RecommendState) -> Dict[str, Any]:
    """ìƒì„±ëœ ì¿¼ë¦¬ë¡œ Qdrant ê²€ìƒ‰ ìˆ˜í–‰"""
    
    # ì• ë‹¨ê³„ì—ì„œ ì¿¼ë¦¬ ìƒì„±ì— ì‹¤íŒ¨í–ˆë‹¤ë©´ ì‹¤í–‰ ë¶ˆê°€
    if not state.search_query:
        logger.warning("No search query found. Skipping vector search.")
        return {"step": state.step + 1}
    
    from backend.agents.recommend.agent.state import CandidateRepo

    start_time = time.time()
    logger.info(f"ğŸ” Executing Vector Search for: '{state.search_query}'")

    try:
        # 1. DB ê²€ìƒ‰ ì‹¤í–‰
        result = vector_search_engine.search(
            query=state.search_query,
            filters=state.search_filters,
            target_k=10
        )
        
        raw_recommendations = result.get("final_recommendations", [])
        
        # 2. [í•µì‹¬] Raw Dict -> CandidateRepo ê°ì²´ë¡œ ë³€í™˜ (Mapping)
        structured_results: List[CandidateRepo] = []
        
        for item in raw_recommendations:
            # Qdrant/FlashRank ê²°ê³¼ì—ì„œ í•„ë“œë¥¼ ë§¤í•‘í•˜ì—¬ ê°ì²´ ìƒì„±
            repo_obj = CandidateRepo(
                id=item.get("project_id"),
                name=item.get("name"),
                owner=item.get("owner"),
                description=item.get("description"),
                stars=int(item.get("stars", 0)),
                forks=int(item.get("forks", 0)),
                main_language=item.get("main_language", "UNKNOWN"),
                languages=item.get("languages") or [],
                topics=item.get("topics") or [],
                html_url=item.get("repo_url") or f"https://github.com/{item.get('owner')}/{item.get('name')}",
                
                # ê²€ìƒ‰ ì—”ì§„ì´ ê³„ì‚°í•œ ì ìˆ˜ì™€ ìŠ¤ë‹ˆí«
                score=item.get("rerank_score", 0.0),
                match_snippet=item.get("match_snippet", ""),
                rag_query=state.search_query,
                rag_filters=state.search_filters
            )
            structured_results.append(repo_obj)
        
        elapsed = round(time.time() - start_time, 3)
        timings = dict(state.timings)
        timings["vector_search"] = elapsed
        
        logger.info(f"âœ… Found {len(structured_results)} recommendations in {elapsed}s")

        return {
            "search_results": structured_results,
            "timings": timings,
            "step": state.step + 1,
            "error": None
        }

    except Exception as e:
        logger.error(f"âŒ Vector search failed: {e}")
        return {
            "error": str(e), 
            "failed_step": "vector_search_node", 
            "step": state.step + 1
        }
    
# =================================================================
# ğŸ‘‡ 5. Scoring Node (LLM í‰ê°€)
# =================================================================
async def score_candidates_node(state: RecommendState) -> Dict[str, Any]:
    """LLMì„ ì´ìš©í•œ í›„ë³´êµ° ìƒì„¸ í‰ê°€ (ai_reason ìƒì„±)"""
    
    # 1. í‰ê°€í•  í›„ë³´ê°€ ì—†ìœ¼ë©´ íŒ¨ìŠ¤
    if not state.search_results:
        logger.info("No candidates to score. Skipping.")
        return {"step": state.step + 1}

    start_time = time.time()
    logger.info(f"ğŸ§  Scoring {len(state.search_results)} candidates...")

    try:
        # 2. Stateì— ìˆëŠ” Dict í˜•íƒœì˜ Snapshotì„ ê°ì²´ë¡œ ë³µì› (Scorerê°€ ê°ì²´ë¥¼ ìš”êµ¬í•¨)
        source_snapshot = None
        if state.repo_snapshot:
            source_snapshot = RepoSnapshot(**state.repo_snapshot)
        
        # 3. Readme ìš”ì•½ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        readme_summary_text = ""
        if state.readme_summary and isinstance(state.readme_summary, dict):
            readme_summary_text = state.readme_summary.get("final_summary", "")

        # 4. Scorer ì‹¤í–‰
        # ì´ ë‹¨ê³„ì—ì„œ ê° CandidateRepo ê°ì²´ì˜ ai_scoreì™€ ai_reasonì´ ì±„ì›Œì§‘ë‹ˆë‹¤.
        scored_results = await scorer_instance.evaluate_candidates(
            candidates=state.search_results,
            user_request=state.user_request,
            intent=state.user_intent,
            source_repo=source_snapshot,
            readme_summary=readme_summary_text
        )

        elapsed = round(time.time() - start_time, 3)
        timings = dict(state.timings)
        timings["ai_scoring"] = elapsed

        if scored_results:
            logger.info(f"âœ… Scoring complete. Top 1: {scored_results[0].name})")

        return {
            "search_results": scored_results[:6], # ai_reasonì´ í¬í•¨ëœ ë¦¬ìŠ¤íŠ¸ë¡œ ì—…ë°ì´íŠ¸
            "timings": timings,
            "step": state.step + 1,
            "error": None
        }

    except Exception as e:
        logger.error(f"âŒ Scoring failed: {e}")
        return {"error": str(e), "failed_step": "score_candidates_node", "step": state.step + 1}
    
async def trend_search_node(state: RecommendState) -> Dict[str, Any]:
    """
    [íŠ¸ë Œë“œ ê²€ìƒ‰ ë…¸ë“œ] ìƒíƒœì˜ ì •ëŸ‰ì  í•„í„°(TREND_LANGUAGE, TREND_SINCE)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ íŠ¸ë Œë“œ ê²€ìƒ‰ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    start_time = time.time()
    logger.info("ğŸ” Executing Trend Search via TrendService...")

    try:
        # 1. TrendService í˜¸ì¶œ (stateì—ì„œ quantitative_filters ì „ë‹¬)
        raw_search_results = await trend_service.search_trending_repos(
            filters=state.quantitative_filters
        )
        
        # 2. ê²°ê³¼ ë§¤í•‘ ë° ë³€í™˜ (ParsedTrendingRepo -> CandidateRepo)
        structured_results: List[CandidateRepo] = []
        for item in raw_search_results:
            try:
                repo_obj = CandidateRepo(
                    id=0,
                    name=item.name,
                    owner=item.owner,
                    html_url=item.url,
                    description=item.description,
                    main_language=item.language,
                    stars=item.total_stars,
                    rank=item.rank,
                    stars_since=item.stars_since,
                    score=0.0,
                    match_snippet=f"Trending Rank: {item.rank}, Stars this period: {item.stars_since}"
                )
                structured_results.append(repo_obj)
            except (KeyError, ValidationError, AttributeError) as ve:
                logger.warning(f"âš ï¸ Failed to map Trend result to CandidateRepo: {ve}")


        elapsed = round(time.time() - start_time, 3)
        timings = dict(state.timings)
        timings["trend_search"] = elapsed
        
        logger.info(f"âœ… Trend Search Found {len(structured_results)} candidates in {elapsed}s")

        # 3. ìƒíƒœ ì—…ë°ì´íŠ¸
        return {
            "search_results": structured_results, # í™•ì¥ëœ CandidateRepo ê°ì²´ ë¦¬ìŠ¤íŠ¸ ì €ì¥
            "timings": timings,
            "search_query": f"Trending repositories based on filters.",
            "step": state.step + 1,
            "error": None
        }

    except Exception as e:
        logger.error(f"âŒ Trend search failed: {e}")
        return {
            "error": str(e), 
            "failed_step": "trend_search_node", 
            "step": state.step + 1
        }
    
async def generate_api_search_query_node(state: RecommendState) -> Dict[str, Any]:
    """
    [ì¿¼ë¦¬ ìƒì„± ë…¸ë“œ] ì‚¬ìš©ì ì˜ë„(search_criteria)ì™€ í•„í„° ì¡°ê±´ì„ ê¸°ë°˜ìœ¼ë¡œ
    GitHub Search APIì— ì í•©í•œ ìµœì¢… ì¿¼ë¦¬ ë¬¸ìì—´ê³¼ í•„í„° íŒŒë¼ë¯¸í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    
    mode = state.user_intent 
    if mode != "search_criteria":
        logger.warning(f"Query generation called for invalid mode: {mode}. Skipping.")
        return {"step": state.step + 1}
    
    if not state.user_request:
        return {"step": state.step + 1}
    
    from backend.agents.recommend.core.search.search_query_generator import search_query_generator
        
    start_time = time.time()
    logger.info(f"ğŸš€ Starting Search Query Generation (Mode: {mode})")
    
    try:
        result_params = await search_query_generator(user_input=state.user_request)
        
        final_query_str = result_params.get("q", "")
        
        final_filters = {
            "q": final_query_str,
            "sort": result_params.get("sort"),
            "order": result_params.get("order")
        }
        
        elapsed = round(time.time() - start_time, 3)
        timings = dict(state.timings)
        timings["generate_api_query"] = elapsed
        
        logger.info(f"âœ… Query Generated ({mode}): Q='{final_filters}...' | Filters set.")

        return {
            "github_seach_query": final_filters,
            "timings": timings,
            "step": state.step + 1,
            "error": None
        }

    except Exception as e:
        logger.error(f"âŒ Failed to generate query: {e}")
        return {"error": str(e), "failed_step": "generate_api_search_query_node", "step": state.step + 1}
    
async def github_search_node(state: RecommendState) -> Dict[str, Any]:
    """
    [GitHub API ê²€ìƒ‰ ë…¸ë“œ] LLMì´ ìƒì„±í•œ ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ë¡œ GitHub Search APIë¥¼ í˜¸ì¶œí•˜ê³ 
    ê²°ê³¼ë¥¼ search_resultsì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    
    if not state.github_seach_query or not state.github_seach_query.get("q"):
        logger.warning("No valid search query params found. Skipping GitHub API search.")
        return {"step": state.step + 1}
        
    start_time = time.time()
    
    try:
        raw_results = github_search_instance.search_repositories(state.github_seach_query)
        
        structured_results: List[CandidateRepo] = []
        for item in raw_results:
            try:
                repo_obj = CandidateRepo(
                    id=getattr(item, "id", 0),
                    name=getattr(item, "name"),
                    owner=getattr(item, "owner"),
                    html_url=getattr(item, "html_url"),
                    description=getattr(item, "description", "GitHub API search result."),
                    main_language=getattr(item, "main_language", "Unknown"),
                    stars=int(getattr(item, "stars", 0)),
                    match_snippet=getattr(item, "match_snippet", "API result."),
                    search_query=state.github_seach_query
                )
                structured_results.append(repo_obj)
            except Exception as ve:
                logger.error(f"Failed to map API result to CandidateRepo: {ve}")

        elapsed = round(time.time() - start_time, 3)
        timings = dict(state.timings)
        timings["github_api_search"] = elapsed

        logger.info(f"âœ… Found {len(structured_results)} candidates from GitHub API in {elapsed}s")

        return {
            "search_results": structured_results,
            "timings": timings,
            "step": state.step + 1,
            "error": None
        }

    except Exception as e:
        logger.error(f"âŒ Node Execution Failed (github_search_node): {e}")
        return {"error": str(e), "failed_step": "github_search_node", "step": state.step + 1}


def check_ingest_error_node(state: RecommendState) -> Dict[str, Any]:
    """ì—ëŸ¬ ë°œìƒ ì‹œ ì¬ì‹œë„ íšŸìˆ˜ë¥¼ í™•ì¸í•˜ê³  ë³µêµ¬ ë˜ëŠ” ì¢…ë£Œí•©ë‹ˆë‹¤."""
    if not state.error: return {"step": state.step + 1}
    if state.retry_count < state.max_retry:
        return {"error": None, "failed_step": state.failed_step, "retry_count": state.retry_count + 1, "step": state.step + 1}
    return {"step": state.step + 1}

# ------------------------------------------------------------------
# 3. Routing Logic (ë¼ìš°íŒ… ë¡œì§ - ìµœì¢… ì •ë¦¬)
# ------------------------------------------------------------------

def route_after_parsing(state: RecommendState) -> str:
    """ì´ˆê¸° ì˜ë„ íŒŒì•… í›„ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•©ë‹ˆë‹¤."""
    if state.error: return "check_ingest_error_node"
    intent = state.user_intent
    
    if intent == "url_analysis": return "fetch_snapshot_node" 
    elif intent == "trend_analysis": return "trend_search_node"
    elif intent == "semantic_search": return "generate_rag_query_node"
    elif intent == "search_criteria": return "generate_api_search_query_node"
    else: return "generate_rag_query_node"


def route_after_fetch(state: RecommendState) -> str:
    if state.error: return "check_ingest_error_node"
    return "analyze_readme_summary_node"

def route_after_analysis(state: RecommendState) -> str:
    if state.error: return "check_ingest_error_node"
    return "generate_rag_query_node" 


def route_after_rag_query_gen(state: RecommendState) -> str:
    if state.error: return "check_ingest_error_node"
    return "vector_search_node"

def route_after_api_query_gen(state: RecommendState) -> str:
    if state.error: return "check_ingest_error_node"
    return "github_search_node" 

def route_after_github_search(state: RecommendState) -> str:
    if state.error: return "check_ingest_error_node"
    return "vector_search_node"

def route_after_vector_search(state: RecommendState) -> str:
    if state.error: return "check_ingest_error_node"
    return "score_candidates_node"

# â­ï¸ ìˆ˜ì •ë¨: score_candidates_node í›„ -> ë°”ë¡œ END
def route_after_scoring(state: RecommendState) -> str:
    """Scoring í›„, ê°œë³„ ai_reasonì„ ìƒì„±í–ˆìœ¼ë¯€ë¡œ ë°”ë¡œ ENDë¡œ ì´ë™í•©ë‹ˆë‹¤."""
    if state.error: return "check_ingest_error_node"
    return END 

# â­ï¸ route_after_final_summary_gen í•¨ìˆ˜ëŠ” ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.


# â­ï¸ ìˆ˜ì •ë¨: íŠ¸ë Œë“œ ê²€ìƒ‰ í›„ -> score_candidates_nodeë¡œ ì´ë™ (ai_reason ìƒì„±)
def route_after_trend_search(state: RecommendState) -> str:
    """íŠ¸ë Œë“œ ê²€ìƒ‰ í›„ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•©ë‹ˆë‹¤."""
    if state.error: 
        return "check_ingest_error_node"
    return "score_candidates_node"

# â­ï¸ ìˆ˜ì •ë¨: ì—ëŸ¬ ë³µêµ¬ ë§µì—ì„œ generate_final_summary_node ì œê±°
def route_after_error_check(state: RecommendState) -> str:
    if state.error: return END 
    step_map = {
        "parse_initial_request_node": "parse_initial_request_node", 
        "fetch_snapshot_node": "fetch_snapshot_node",
        "analyze_readme_summary_node": "analyze_readme_summary_node",
        "generate_rag_query_node": "generate_rag_query_node",
        "generate_api_search_query_node": "generate_api_search_query_node",
        "github_search_node": "github_search_node",
        "vector_search_node": "vector_search_node",
        "score_candidates_node": "score_candidates_node",
        "trend_search_node": "trend_search_node",
        # generate_final_summary_node ì œê±°ë¨
    }
    return step_map.get(state.failed_step, END)