# backend/agents/recommend/agent/nodes.py

from __future__ import annotations

import logging
import time
import asyncio
from typing import Any, Dict, Optional, List
from dataclasses import asdict
from backend.agents.recommend.agent.state import RecommendState
from backend.core.github_core import RepoSnapshot
from backend.agents.recommend.core.ingest.summarizer import ContentSummarizer
from backend.agents.recommend.core.analysis.match_score import RepoScorer

logger = logging.getLogger(__name__)

try:
    summarizer_instance = ContentSummarizer()
    scorer_instance = RepoScorer() # Scorer ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ë¹„ìš© ì ˆì•½ì„ ìœ„í•´ ì¬ì‚¬ìš©)
    logger.info("âœ… Global instances initialized.")
except Exception as e:
    logger.error(f"âŒ Failed to init global instances: {e}")
    exit(1)

def fetch_snapshot_node(state: RecommendState) -> Dict[str, Any]:
    """
    GitHub ì €ì¥ì†Œ ìŠ¤ëƒ…ìƒ· ìˆ˜ì§‘ ë…¸ë“œ.
    """
    
    # 1. ì¬ì‚¬ìš© ì²´í¬
    if state.repo_snapshot:
        logger.info("Reusing existing repo snapshot")
        return {"step": state.step + 1} 
    
    # 2. í•„ìˆ˜ ì…ë ¥ê°’ ê²€ì¦
    owner = state.owner
    repo = state.repo
    ref = getattr(state, 'ref', 'main') # ref í•„ë“œê°€ ìˆì„ ê²½ìš° ì‚¬ìš©

    # owner, repoê°€ Stateì— ì—†ëŠ” ê²½ìš° ì—ëŸ¬ ì²˜ë¦¬
    if not owner or not repo:
        error_msg = "Owner or repository name is missing in state. (Pre-analysis failure)"
        logger.error(f"Failed to fetch details: {error_msg}")
        return {
            "error": error_msg,
            "failed_step": "fetch_snapshot_node",
            "step": state.step + 1,
        }
    
    from backend.core.github_core import fetch_repo_snapshot

    start_time = time.time()
    
    try:
        
        snapshot = fetch_repo_snapshot(state.owner, state.repo, state.ref)
        
        snapshot_dict = snapshot.model_dump() if hasattr(snapshot, "model_dump") else asdict(snapshot)

        elapsed = round(time.time() - start_time, 3)
        timings = dict(state.timings)
        timings["fetch_snapshot"] = elapsed
        
        logger.info(f"Fetched snapshot for {state.owner}/{state.repo} in {elapsed}s")
        
        return {
            "repo_snapshot": snapshot_dict,
            "timings": timings,
            "step": state.step + 1,
        }
        
    except Exception as e:
        logger.error(f"Failed to fetch snapshot: {e}")
        return { 
            "error": str(e),
            "failed_step": "fetch_snapshot_node",
            "step": state.step + 1,
        }
    
async def analyze_readme_summary_node(state: RecommendState) -> Dict[str, Any]:
    """
    README ë¶„ì„ ë° LLM ìš”ì•½ ë…¸ë“œ.
    
    repo_snapshotì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¡°í™”ëœ LLM ìš”ì•½ì„ ìƒì„±í•˜ê³  DocsCoreResultë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
    """
    
    # 1. ì¬ì‚¬ìš© ì²´í¬ ë° í•„ìˆ˜ ì„ í–‰ ì¡°ê±´ ì²´í¬
    if state.readme_summary:
        logger.info("Reusing existing ingest analysis result")
        return {"step": state.step + 1} 
    
    if not state.repo_snapshot:
        error_msg = "No repo_snapshot available for README analysis."
        logger.error(f"Failed to analyze README: {error_msg}")
        return {
            "error": error_msg,
            "failed_step": "analyze_readme_summary_node",
            "step": state.step + 1,
        }
    
    from backend.core.docs_core import analyze_docs, extract_and_structure_summary_input
    
    start_time = time.time()
    
    try:
        # 2. Stateì—ì„œ ìŠ¤ëƒ…ìƒ· ì¶”ì¶œ
        snapshot_dict = state.repo_snapshot
        readme_content = snapshot_dict.get("readme_content", "")

        # Dict â†’ RepoSnapshot ë³€í™˜
        snapshot_obj = RepoSnapshot(**snapshot_dict)

        # 3. ë¬¸ì„œ ë¶„ì„ (DocsCoreResult dataclass)
        docs_result = analyze_docs(snapshot_obj)

        # dataclass â†’ dict ë³€í™˜
        docs_result_dict = asdict(docs_result)

        # 4. LLM ì…ë ¥ êµ¬ì„±
        llm_input_text = extract_and_structure_summary_input(readme_content)

        # 5. LLM ìš”ì•½ ì‹¤í–‰ (ë¹„ë™ê¸°)
        final_summary = "No summary generated."
        if llm_input_text:
            final_summary = await summarizer_instance.summarize(llm_input_text)
            logger.info("LLM summary generated successfully.")
        else:
            logger.warning("Skipping LLM summary: No structured input generated.")

        # 6. ê²°ê³¼ í†µí•©
        ingest_result = {
            "final_summary": final_summary,
            "docs_analysis": docs_result_dict,
            "readme_word_count": len(readme_content.split()),
            "documentation_quality": docs_result_dict.get("total_score", 0)
        }

        # 7. ìƒíƒœ ì—…ë°ì´íŠ¸ ë°˜í™˜
        elapsed = round(time.time() - start_time, 3)
        timings = dict(state.timings)
        timings["analyze_readme_summary"] = elapsed

        return {
            "readme_summary": ingest_result,
            "timings": timings,
            "step": state.step + 1,
            "failed_step": None,
            "error": None,
        }

    except Exception as e:
        logger.error(f"Failed to analyze and summarize README: {e}")
        return {
            "error": str(e),
            "failed_step": "analyze_readme_summary_node",
            "step": state.step + 1,
        }
    

async def generate_search_query_node(state: RecommendState) -> Dict[str, Any]:
    """
    RAG ê²€ìƒ‰ ì¿¼ë¦¬ ë° í•„í„° ìƒì„± ë…¸ë“œ.
    
    ìƒí™©ì— ë”°ë¼ ë‘ ê°€ì§€ ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤:
    1. URL ë¶„ì„ ë°ì´í„°(repo_snapshot)ê°€ ìˆìŒ -> 'url_analysis' ëª¨ë“œ (ìœ ì‚¬ë„ ê²€ìƒ‰ + Fallback ë¡œì§ ì ìš©)
    2. URL ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŒ -> 'semantic_search' ëª¨ë“œ (ì¼ë°˜ ê²€ìƒ‰)
    """

    from backend.agents.recommend.core.search.rag_query_generator import generate_rag_query_and_filters
    
    start_time = time.time()
    logger.info("ğŸš€ Starting RAG Query Generation Node")

    # 1. ëª¨ë“œ ê²°ì • ë° ë¶„ì„ ë°ì´í„° ì¤€ë¹„
    # readme_summaryê°€ ì—†ë”ë¼ë„, repo_snapshot(ê¸°ë³¸ ì •ë³´)ë§Œ ìˆìœ¼ë©´ ë¶„ì„ ëª¨ë“œë¡œ ì§„ì…í•©ë‹ˆë‹¤.
    if state.repo_snapshot:
        analyzed_data = {
            "repo_snapshot": state.repo_snapshot,
            "readme_summary": state.readme_summary if state.readme_summary else {}
        }
    else:
        analyzed_data = None

    mode = state.user_intent

    # 2. ì‚¬ìš©ì ìš”ì²­ í…ìŠ¤íŠ¸ (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì„¤ì •)
    user_input = state.user_request if state.user_request.strip() else "Find similar projects."

    try:
        # 3. Core ë¡œì§ í˜¸ì¶œ (LLM ìˆ˜í–‰)
        result = await generate_rag_query_and_filters(
            user_request=user_input,
            category=mode,
            analyzed_data=analyzed_data
        )

        elapsed = round(time.time() - start_time, 3)
        timings = dict(state.timings)
        timings["generate_query"] = elapsed

        logger.info(f"âœ… Query Generated ({mode}): {result['query']} (Time: {elapsed}s)")

        return {
            "search_query": result.get("query", ""),
            "search_keywords": result.get("keywords", []),
            "search_filters": result.get("filters", {}),
            "timings": timings,
            "step": state.step + 1,
            "error": None
        }

    except Exception as e:
        logger.error(f"âŒ Failed to generate search query: {e}")
        return {
            "error": str(e),
            "failed_step": "generate_search_query_node",
            "step": state.step + 1
        }
    
def vector_search_node(state: RecommendState) -> Dict[str, Any]:
    """ìƒì„±ëœ ì¿¼ë¦¬ë¡œ Qdrant ê²€ìƒ‰ ìˆ˜í–‰"""
    
    # ì• ë‹¨ê³„ì—ì„œ ì¿¼ë¦¬ ìƒì„±ì— ì‹¤íŒ¨í–ˆë‹¤ë©´ ì‹¤í–‰ ë¶ˆê°€
    if not state.search_query:
        logger.warning("No search query found. Skipping vector search.")
        return {"step": state.step + 1}
    
    from backend.agents.recommend.agent.state import CandidateRepo
    from backend.agents.recommend.core.search.vector_search import vector_search_engine

    start_time = time.time()
    logger.info(f"ğŸ” Executing Vector Search for: '{state.search_query}'")

    try:
        # 1. DB ê²€ìƒ‰ ì‹¤í–‰
        result = vector_search_engine.search(
            query=state.search_query,
            filters=state.search_filters,
            target_k=10
        )
        
        raw_recommendations = result.get("final_recommendations", [])
        
        # 2. [í•µì‹¬] Raw Dict -> CandidateRepo ê°ì²´ë¡œ ë³€í™˜ (Mapping)
        structured_results: List[CandidateRepo] = []
        
        for item in raw_recommendations:
            # Qdrant/FlashRank ê²°ê³¼ì—ì„œ í•„ë“œë¥¼ ë§¤í•‘í•˜ì—¬ ê°ì²´ ìƒì„±
            repo_obj = CandidateRepo(
                id=item.get("project_id"),
                name=item.get("name"),
                owner=item.get("owner"),
                description=item.get("description"),
                stars=int(item.get("stars", 0)),
                forks=int(item.get("forks", 0)),
                main_language=item.get("main_language", "UNKNOWN"),
                languages=item.get("languages") or [],
                topics=item.get("topics") or [],
                html_url=item.get("repo_url") or f"https://github.com/{item.get('owner')}/{item.get('name')}",
                
                # ê²€ìƒ‰ ì—”ì§„ì´ ê³„ì‚°í•œ ì ìˆ˜ì™€ ìŠ¤ë‹ˆí«
                score=item.get("rerank_score", 0.0),
                match_snippet=item.get("match_snippet", ""),
            )
            structured_results.append(repo_obj)
        
        elapsed = round(time.time() - start_time, 3)
        timings = dict(state.timings)
        timings["vector_search"] = elapsed
        
        logger.info(f"âœ… Found {len(structured_results)} recommendations in {elapsed}s")

        return {
            "search_results": structured_results,
            "timings": timings,
            "step": state.step + 1,
            "error": None
        }

    except Exception as e:
        logger.error(f"âŒ Vector search failed: {e}")
        return {
            "error": str(e), 
            "failed_step": "vector_search_node", 
            "step": state.step + 1
        }
    
async def score_candidates_node(state: RecommendState) -> Dict[str, Any]:
    """LLMì„ ì´ìš©í•œ í›„ë³´êµ° ìƒì„¸ í‰ê°€"""
    
    # 1. í‰ê°€í•  í›„ë³´ê°€ ì—†ìœ¼ë©´ íŒ¨ìŠ¤
    if not state.search_results:
        logger.info("No candidates to score. Skipping.")
        return {"step": state.step + 1}

    start_time = time.time()
    logger.info(f"ğŸ§  Scoring {len(state.search_results)} candidates...")

    try:
        # 2. Stateì— ìˆëŠ” Dict í˜•íƒœì˜ Snapshotì„ ê°ì²´ë¡œ ë³µì› (Scorerê°€ ê°ì²´ë¥¼ ìš”êµ¬í•¨)
        source_snapshot = None
        if state.repo_snapshot:
            source_snapshot = RepoSnapshot(**state.repo_snapshot)
        
        # 3. Readme ìš”ì•½ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        readme_summary_text = ""
        if state.readme_summary and isinstance(state.readme_summary, dict):
            readme_summary_text = state.readme_summary.get("final_summary", "")

        # 4. Scorer ì‹¤í–‰
        scored_results = await scorer_instance.evaluate_candidates(
            candidates=state.search_results,     # vector_search ê²°ê³¼
            user_request=state.user_request,
            intent=state.user_intent,            # "semantic_search" or "url_analysis"
            source_repo=source_snapshot,         # ì›ë³¸ ê°ì²´
            readme_summary=readme_summary_text   # ìš”ì•½ë³¸ ìŠ¤íŠ¸ë§
        )

        elapsed = round(time.time() - start_time, 3)
        timings = dict(state.timings)
        timings["ai_scoring"] = elapsed

        logger.info(f"âœ… Scoring complete. Top 1: {scored_results[0].name} (Score: {scored_results[0].ai_score})")

        return {
            "search_results": scored_results, # ì ìˆ˜ê°€ ë§¤ê²¨ì§„ ë¦¬ìŠ¤íŠ¸ë¡œ ì—…ë°ì´íŠ¸
            "timings": timings,
            "step": state.step + 1,
            "error": None
        }

    except Exception as e:
        logger.error(f"âŒ Scoring failed: {e}")
        # ì—ëŸ¬ê°€ ë‚˜ë„ í”„ë¡œì„¸ìŠ¤ëŠ” ê³„ì† ì§„í–‰ (í‰ê°€ë§Œ ì‹¤íŒ¨)
        return {"error": str(e), "failed_step": "score_candidates_node", "step": state.step + 1}
    
    
def check_ingest_error_node(state: RecommendState) -> Dict[str, Any]:
    """
    ì—ëŸ¬ ì²´í¬ ë° ë³µêµ¬ ë…¸ë“œ.
    
    í˜„ì¬ ì—ëŸ¬ ìƒíƒœë¥¼ í™•ì¸í•˜ê³  ì¬ì‹œë„ ê°€ëŠ¥ ì—¬ë¶€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
    """
    # 1. ì—ëŸ¬ê°€ ì—†ìœ¼ë©´ ë‹¤ìŒ ë‹¨ê³„(ì—¬ê¸°ì„œëŠ” ì¶œë ¥)ë¡œ ì´ë™
    if not state.error:
        # ì—ëŸ¬ê°€ ì—†ëŠ”ë° ì´ ë…¸ë“œì— ë„ì°©í–ˆë‹¤ë©´, ì¼ë°˜ì ìœ¼ë¡œëŠ” ìµœì¢… ì¶œë ¥ ë…¸ë“œë¡œ ì´ë™í•´ì•¼ í•©ë‹ˆë‹¤.
        # í•˜ì§€ë§Œ, ê·¸ë˜í”„ì˜ ëì´ ëª…í™•í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ì¼ë‹¨ stepë§Œ ì¦ê°€ì‹œí‚µë‹ˆë‹¤.
        return {"step": state.step + 1} 
    
    failed_step = state.failed_step or "unknown"
    retry_count = state.retry_count
    
    logger.warning(f"Error detected in {failed_step}: {state.error}, retry={retry_count}/{state.max_retry}")
    
    # 2. ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ í™•ì¸
    if retry_count >= state.max_retry:
        logger.error(f"Max retries reached for {failed_step}. Cannot recover.")
        # ë³µêµ¬ ë¶ˆê°€ -> Graph ì¢…ë£Œ ë˜ëŠ” ìµœì¢… ì¶œë ¥ìœ¼ë¡œ ë¼ìš°íŒ…
        return {"step": state.step + 1} 
    
    # 3. ì¬ì‹œë„ ê°€ëŠ¥í•œ ë‹¨ê³„ ê²°ì •
    retryable_steps = ["fetch_snapshot_node", "analyze_readme_summary_node"]
    
    if failed_step in retryable_steps:
        logger.info(f"Scheduling retry for {failed_step}")
        return {
            "error": None,          # ì—ëŸ¬ ìƒíƒœ í´ë¦¬ì–´
            "failed_step": failed_step, # ì¬ì‹œë„ í›„ ì´ ë‹¨ê³„ë¡œ ëŒì•„ê°€ë„ë¡ failed_step ìœ ì§€ (ë¼ìš°íŒ…ìš©)
            "retry_count": retry_count + 1,
            "step": state.step + 1, 
        }
    
    # ì¬ì‹œë„ ëª©ë¡ì— ì—†ëŠ” ì—ëŸ¬
    return {"step": state.step + 1}

def route_after_fetch(state: RecommendState) -> str:
    """ìŠ¤ëƒ…ìƒ· ìˆ˜ì§‘ í›„ ë¼ìš°íŒ…."""
    if state.error:
        return "check_ingest_error_node"
    # ì„±ê³µ ì‹œ: ë‹¤ìŒ í•µì‹¬ ë‹¨ê³„ì¸ README ë¶„ì„ìœ¼ë¡œ ì´ë™
    return "analyze_readme_summary_node"


def route_after_analysis(state: RecommendState) -> str:
    """README ë¶„ì„ ë° ìš”ì•½ í›„ ë¼ìš°íŒ…."""
    if state.error:
        return "check_ingest_error_node"
    return "__end__" 


def route_after_error_check(state: RecommendState) -> str:
    """ì—ëŸ¬ ì²´í¬ í›„ ë¼ìš°íŒ…."""
    
    # 1. ì—ëŸ¬ê°€ ë‚¨ì•„ìˆë‹¤ë©´ (ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼)
    if state.error:
        # ë³µêµ¬ ë¶ˆê°€ -> ê·¸ë˜í”„ ì¢…ë£Œ
        return "__end__"
    
    # 2. ì—ëŸ¬ê°€ í´ë¦¬ì–´ë˜ê³  ì¬ì‹œë„ê°€ í•„ìš”í•œ ë‹¨ê³„ê°€ ë‚¨ì•„ìˆëŠ” ê²½ìš°
    failed_step = state.failed_step
    
    if failed_step == "fetch_snapshot_node":
        # fetch_snapshot_node ë…¸ë“œë¡œ ëŒì•„ê°€ ì¬ì‹œë„
        return "fetch_snapshot_node"
    elif failed_step == "analyze_readme_summary_node":
        # analyze_readme_summary_node ë…¸ë“œë¡œ ëŒì•„ê°€ ì¬ì‹œë„
        return "analyze_readme_summary_node"
    
    # 3. ëª¨ë“  ì—ëŸ¬ê°€ ë³µêµ¬ë˜ì—ˆê±°ë‚˜ ì¬ì‹œë„ê°€ ë¶ˆí•„ìš”í•œ ê²½ìš°
    return "__end__"