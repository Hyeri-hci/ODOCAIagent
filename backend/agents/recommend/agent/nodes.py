import logging
import time
import asyncio
from typing import Dict, Any, Optional, List
from dataclasses import asdict
from langgraph.graph import StateGraph, END
from pydantic import BaseModel, ValidationError

from backend.agents.recommend.agent.state import RecommendState
from backend.agents.recommend.core.ingest.summarizer import ContentSummarizer
from backend.core.models import RepoSnapshot
from backend.agents.recommend.core.search.vector_search import vector_search_engine
from backend.agents.recommend.core.analysis.match_score import RepoScorer
from backend.agents.recommend.core.intent_parsing import extract_initial_metadata
from backend.agents.recommend.core.trend.get_trend import trend_service
from backend.agents.recommend.agent.state import CandidateRepo
from backend.agents.recommend.core.search.github_search import GitHubSearch

from langchain_openai import ChatOpenAI
from backend.agents.recommend.config.setting import settings

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(name)s | %(message)s')
logger = logging.getLogger("TestRealAgent")

# ------------------------------------------------------------------
# 1. Global Instances
# ------------------------------------------------------------------
try:
    globals()['llm'] = ChatOpenAI(
        base_url=settings.llm.api_base,
        api_key=settings.llm.api_key,
        model=settings.llm.model_name,
        temperature=0
    )

    github_search_instance = GitHubSearch()
    summarizer_instance = ContentSummarizer()
    scorer_instance = RepoScorer() # Scorer ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ë¹„ìš© ì ˆì•½ì„ ìœ„í•´ ì¬ì‚¬ìš©)
    logger.info("âœ… Global instances initialized.")
except Exception as e:
    logger.error(f"âŒ Failed to init global instances: {e}")
    exit(1)

# ------------------------------------------------------------------
# 2. Nodes Definition
# ------------------------------------------------------------------

async def parse_initial_request_node(state: RecommendState) -> Dict[str, Any]:
    """
    [ì²« ì‹¤í–‰ ë…¸ë“œ] ì‚¬ìš©ì ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ì˜ë„ì™€ ì •ëŸ‰ì  í•„í„° ì¡°ê±´ë§Œ ì¶”ì¶œí•˜ê³  ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    (í•µì‹¬ ë¡œì§ì€ core/intent_parsing.pyì˜ extract_initial_metadataë¥¼ í˜¸ì¶œ)
    """
    
    user_request = state.user_request
    repo_url = state.repo_url
    
    try:
        llm_client = globals()['llm'] 
    except KeyError:
        logger.error("âŒ LLM client ('llm') not initialized in global scope.")
        return {"user_intent": "semantic_search", "quantitative_filters": []}

    if not user_request and not repo_url:
        logger.warning("Request is empty. Defaulting to semantic_search.")
        return {"user_intent": "semantic_search", "quantitative_filters": []}

    try:
        # 2. í•µì‹¬ ë¡œì§ í˜¸ì¶œ (Core Logic ì‹¤í–‰)
        result = await extract_initial_metadata(
            llm_client=llm_client, 
            user_request=user_request,
            repo_url=repo_url
        )
        
        logger.info(f"âœ… Initial Parsing Result: Intent={result.user_intent}, Filters={len(result.quantitative_filters)}")
        
        # 3. LangGraph ìƒíƒœ ì—…ë°ì´íŠ¸ìš© ë§µ ë°˜í™˜
        # ë°˜í™˜ëœ ë”•ì…”ë„ˆë¦¬ì˜ í‚¤(user_intent, quantitative_filters)ê°€ RecommendStateì˜ í•„ë“œë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        return {
            "user_intent": result.user_intent,
            "quantitative_filters": result.quantitative_filters
        }
    
    except Exception as e:
        logger.error(f"âŒ Node Execution Failed (parse_initial_request_node): {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ í´ë°± ì²˜ë¦¬
        return {
            "user_intent": "semantic_search",
            "error": f"Initial parsing failed: {e.__class__.__name__}",
            "failed_step": "parse_initial_request_node"
        }

def fetch_snapshot_node(state: RecommendState) -> Dict[str, Any]:
    """GitHub ì €ì¥ì†Œ ìŠ¤ëƒ…ìƒ· ìˆ˜ì§‘"""
    if state.repo_snapshot:
        logger.info("Reusing existing repo snapshot")
        return {"step": state.step + 1} 
    
    owner, repo = state.owner, state.repo
    if not owner or not repo:
        return {"error": "Missing owner/repo", "step": state.step + 1}
    
    from backend.core.github_core import fetch_repo_snapshot
    start_time = time.time()
    
    try:
        snapshot = fetch_repo_snapshot(owner, repo, getattr(state, 'ref', 'main'))
        snapshot_dict = snapshot.model_dump() if hasattr(snapshot, "model_dump") else asdict(snapshot)
        
        elapsed = round(time.time() - start_time, 3)
        timings = dict(state.timings)
        timings["fetch_snapshot"] = elapsed
        
        logger.info(f"Fetched snapshot for {owner}/{repo} in {elapsed}s")
        return {"repo_snapshot": snapshot_dict, "timings": timings, "step": state.step + 1}
        
    except Exception as e:
        logger.error(f"Failed to fetch snapshot: {e}")
        return {"error": str(e), "failed_step": "fetch_snapshot_node", "step": state.step + 1}

async def analyze_readme_summary_node(state: RecommendState) -> Dict[str, Any]:
    """README ë¶„ì„ ë° ìš”ì•½"""
    if state.readme_summary: return {"step": state.step + 1}
    if not state.repo_snapshot: 
        return {"error": "No snapshot", "failed_step": "analyze_readme_summary_node", "step": state.step + 1}
    
    from backend.core.docs_core import analyze_docs, extract_and_structure_summary_input
    start_time = time.time()
    
    try:
        snapshot_dict = state.repo_snapshot
        readme_content = snapshot_dict.get("readme_content") or "" 
        snapshot_obj = RepoSnapshot(**snapshot_dict)

        docs_result = analyze_docs(snapshot_obj)
        docs_result_dict = asdict(docs_result)
        llm_input_text = extract_and_structure_summary_input(readme_content)

        final_summary = "No summary generated."
        if llm_input_text and len(readme_content) > 50:
            final_summary = await summarizer_instance.summarize(llm_input_text)
            logger.info("LLM summary generated successfully.")
        else:
            logger.warning("Skipping LLM summary: README too short/empty.")

        ingest_result = {
            "final_summary": final_summary,
            "docs_analysis": docs_result_dict,
            "readme_word_count": len(readme_content.split()),
            "documentation_quality": docs_result_dict.get("total_score", 0)
        }

        elapsed = round(time.time() - start_time, 3)
        timings = dict(state.timings)
        timings["analyze_readme_summary"] = elapsed

        return {"readme_summary": ingest_result, "timings": timings, "step": state.step + 1, "error": None}

    except Exception as e:
        logger.error(f"Failed to analyze README: {e}")
        return {"error": str(e), "failed_step": "analyze_readme_summary_node", "step": state.step + 1}

async def generate_rag_search_query_node(state: RecommendState) -> Dict[str, Any]:
    """RAG ì¿¼ë¦¬ ìƒì„± (Fallback ë¡œì§ í¬í•¨)"""

    from backend.agents.recommend.core.search.rag_query_generator import generate_rag_query_and_filters
    
    start_time = time.time()
    logger.info("ğŸš€ Starting RAG Query Generation")

    # [ì¤‘ìš”] READMEê°€ ì—†ì–´ë„ ê¸°ë³¸ ì •ë³´ë§Œ ìˆìœ¼ë©´ ë¶„ì„ ëª¨ë“œ ì§„ì…
    if state.repo_snapshot:
        analyzed_data = {
            "repo_snapshot": state.repo_snapshot,
            "readme_summary": state.readme_summary if state.readme_summary else {}
        }
    else:
        analyzed_data = None


    mode = state.user_intent

    user_input = state.user_request if state.user_request.strip() else "Find similar projects."

    try:
        result = await generate_rag_query_and_filters(user_input, mode, analyzed_data)
        
        elapsed = round(time.time() - start_time, 3)
        timings = dict(state.timings)
        timings["generate_query"] = elapsed
        
        logger.info(f"âœ… Query Generated ({mode}): {result['query']}")

        return {
            "search_query": result.get("query", ""),
            "search_keywords": result.get("keywords", []),
            "search_filters": result.get("filters", {}),
            "timings": timings,
            "step": state.step + 1,
            "error": None
        }

    except Exception as e:
        logger.error(f"âŒ Failed to generate query: {e}")
        return {"error": str(e), "failed_step": "generate_rag_search_query_node", "step": state.step + 1}

# =================================================================
# ğŸ‘‡ [NEW] 4. Vector Search Node (DB ì¡°íšŒ)
# =================================================================
def vector_search_node(state: RecommendState) -> Dict[str, Any]:
    """ìƒì„±ëœ ì¿¼ë¦¬ë¡œ Qdrant ê²€ìƒ‰ ìˆ˜í–‰"""
    
    # ì• ë‹¨ê³„ì—ì„œ ì¿¼ë¦¬ ìƒì„±ì— ì‹¤íŒ¨í–ˆë‹¤ë©´ ì‹¤í–‰ ë¶ˆê°€
    if not state.search_query:
        logger.warning("No search query found. Skipping vector search.")
        return {"step": state.step + 1}
    
    from backend.agents.recommend.agent.state import CandidateRepo

    start_time = time.time()
    logger.info(f"ğŸ” Executing Vector Search for: '{state.search_query}'")

    try:
        # 1. DB ê²€ìƒ‰ ì‹¤í–‰
        result = vector_search_engine.search(
            query=state.search_query,
            filters=state.search_filters,
            target_k=10
        )
        
        raw_recommendations = result.get("final_recommendations", [])
        
        # 2. [í•µì‹¬] Raw Dict -> CandidateRepo ê°ì²´ë¡œ ë³€í™˜ (Mapping)
        structured_results: List[CandidateRepo] = []
        
        for item in raw_recommendations:
            # Qdrant/FlashRank ê²°ê³¼ì—ì„œ í•„ë“œë¥¼ ë§¤í•‘í•˜ì—¬ ê°ì²´ ìƒì„±
            repo_obj = CandidateRepo(
                id=item.get("project_id"),
                name=item.get("name"),
                owner=item.get("owner"),
                description=item.get("description"),
                stars=int(item.get("stars", 0)),
                forks=int(item.get("forks", 0)),
                main_language=item.get("main_language", "UNKNOWN"),
                languages=item.get("languages") or [],
                topics=item.get("topics") or [],
                html_url=item.get("repo_url") or f"https://github.com/{item.get('owner')}/{item.get('name')}",
                
                # ê²€ìƒ‰ ì—”ì§„ì´ ê³„ì‚°í•œ ì ìˆ˜ì™€ ìŠ¤ë‹ˆí«
                score=item.get("rerank_score", 0.0),
                match_snippet=item.get("match_snippet", ""),
            )
            structured_results.append(repo_obj)
        
        elapsed = round(time.time() - start_time, 3)
        timings = dict(state.timings)
        timings["vector_search"] = elapsed
        
        logger.info(f"âœ… Found {len(structured_results)} recommendations in {elapsed}s")

        return {
            "search_results": structured_results,
            "timings": timings,
            "step": state.step + 1,
            "error": None
        }

    except Exception as e:
        logger.error(f"âŒ Vector search failed: {e}")
        return {
            "error": str(e), 
            "failed_step": "vector_search_node", 
            "step": state.step + 1
        }
    
# =================================================================
# ğŸ‘‡ [NEW] 5. Scoring Node (LLM í‰ê°€)
# =================================================================
async def score_candidates_node(state: RecommendState) -> Dict[str, Any]:
    """LLMì„ ì´ìš©í•œ í›„ë³´êµ° ìƒì„¸ í‰ê°€"""
    
    # 1. í‰ê°€í•  í›„ë³´ê°€ ì—†ìœ¼ë©´ íŒ¨ìŠ¤
    if not state.search_results:
        logger.info("No candidates to score. Skipping.")
        return {"step": state.step + 1}

    start_time = time.time()
    logger.info(f"ğŸ§  Scoring {len(state.search_results)} candidates...")

    try:
        # 2. Stateì— ìˆëŠ” Dict í˜•íƒœì˜ Snapshotì„ ê°ì²´ë¡œ ë³µì› (Scorerê°€ ê°ì²´ë¥¼ ìš”êµ¬í•¨)
        source_snapshot = None
        if state.repo_snapshot:
            source_snapshot = RepoSnapshot(**state.repo_snapshot)
        
        # 3. Readme ìš”ì•½ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        readme_summary_text = ""
        if state.readme_summary and isinstance(state.readme_summary, dict):
            readme_summary_text = state.readme_summary.get("final_summary", "")

        # 4. Scorer ì‹¤í–‰
        scored_results = await scorer_instance.evaluate_candidates(
            candidates=state.search_results,     # vector_search ê²°ê³¼
            user_request=state.user_request,
            intent=state.user_intent,            # "semantic_search" or "url_analysis"
            source_repo=source_snapshot,         # ì›ë³¸ ê°ì²´
            readme_summary=readme_summary_text   # ìš”ì•½ë³¸ ìŠ¤íŠ¸ë§
        )

        elapsed = round(time.time() - start_time, 3)
        timings = dict(state.timings)
        timings["ai_scoring"] = elapsed

        logger.info(f"âœ… Scoring complete. Top 1: {scored_results[0].name} (Score: {scored_results[0].ai_score})")

        return {
            "search_results": scored_results, # ì ìˆ˜ê°€ ë§¤ê²¨ì§„ ë¦¬ìŠ¤íŠ¸ë¡œ ì—…ë°ì´íŠ¸
            "timings": timings,
            "step": state.step + 1,
            "error": None
        }

    except Exception as e:
        logger.error(f"âŒ Scoring failed: {e}")
        # ì—ëŸ¬ê°€ ë‚˜ë„ í”„ë¡œì„¸ìŠ¤ëŠ” ê³„ì† ì§„í–‰ (í‰ê°€ë§Œ ì‹¤íŒ¨)
        return {"error": str(e), "failed_step": "score_candidates_node", "step": state.step + 1}
    
async def trend_search_node(state: RecommendState) -> Dict[str, Any]:
    """
    [íŠ¸ë Œë“œ ê²€ìƒ‰ ë…¸ë“œ] ìƒíƒœì˜ ì •ëŸ‰ì  í•„í„°(TREND_LANGUAGE, TREND_SINCE)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ íŠ¸ë Œë“œ ê²€ìƒ‰ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    start_time = time.time()
    logger.info("ğŸ” Executing Trend Search via TrendService...")

    try:
        # 1. TrendService í˜¸ì¶œ (stateì—ì„œ quantitative_filters ì „ë‹¬)
        raw_search_results = await trend_service.search_trending_repos(
            filters=state.quantitative_filters
        )
        
        # 2. ê²°ê³¼ ë§¤í•‘ ë° ë³€í™˜ (ParsedTrendingRepo -> CandidateRepo)
        structured_results: List[CandidateRepo] = []
        for item in raw_search_results:
            # itemì€ ParsedTrendingRepo ê°ì²´ì´ê±°ë‚˜ Dict í˜•íƒœì…ë‹ˆë‹¤.
            # CandidateRepoê°€ rank, stars_since í•„ë“œë¥¼ í¬í•¨í•˜ë„ë¡ í™•ì¥ë˜ì—ˆìœ¼ë¯€ë¡œ ì§ì ‘ ë³€í™˜ ê°€ëŠ¥í•©ë‹ˆë‹¤.
            try:
                # í•„ë“œê°€ ì¼ì¹˜í•œë‹¤ê³  ê°€ì •í•˜ê³  ë³€í™˜ (stars=total_stars, score=stars_sinceë¥¼ ì„ì‹œë¡œ ì‚¬ìš©)
                repo_obj = CandidateRepo(
                    id=0,
                    name=item.name,
                    owner=item.owner,
                    html_url=item.url,
                    description=item.description,
                    main_language=item.language,
                    stars=item.total_stars,
                    # íŠ¸ë Œë“œ í•„ë“œ ë§¤í•‘
                    rank=item.rank,
                    stars_since=item.stars_since,
                    
                    # RAG í•„ë“œëŠ” 0 ë˜ëŠ” ë¹ˆ ê°’
                    score=0.0,
                    match_snippet=f"Trending Rank: {item.rank}, Stars this period: {item.stars_since}"
                )
                structured_results.append(repo_obj)
            except (KeyError, ValidationError, AttributeError) as ve:
                 logger.warning(f"âš ï¸ Failed to map Trend result to CandidateRepo: {ve}")


        elapsed = round(time.time() - start_time, 3)
        timings = dict(state.timings)
        timings["trend_search"] = elapsed
        
        logger.info(f"âœ… Trend Search Found {len(structured_results)} candidates in {elapsed}s")

        # 3. ìƒíƒœ ì—…ë°ì´íŠ¸
        return {
            "search_results": structured_results, # í™•ì¥ëœ CandidateRepo ê°ì²´ ë¦¬ìŠ¤íŠ¸ ì €ì¥
            "timings": timings,
            "search_query": f"Trending repositories based on filters.",
            "step": state.step + 1,
            "error": None
        }

    except Exception as e:
        logger.error(f"âŒ Trend search failed: {e}")
        return {
            "error": str(e), 
            "failed_step": "trend_search_node", 
            "step": state.step + 1
        }
    
async def generate_api_search_query_node(state: RecommendState) -> Dict[str, Any]:
    """
    [ì¿¼ë¦¬ ìƒì„± ë…¸ë“œ] ì‚¬ìš©ì ì˜ë„(search_criteria)ì™€ í•„í„° ì¡°ê±´ì„ ê¸°ë°˜ìœ¼ë¡œ
    GitHub Search APIì— ì í•©í•œ ìµœì¢… ì¿¼ë¦¬ ë¬¸ìì—´ê³¼ í•„í„° íŒŒë¼ë¯¸í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    
    # â­ï¸ ì¿¼ë¦¬ ìƒì„±ì´ í•„ìš” ì—†ëŠ” ì˜ë„ëŠ” ì´ ë…¸ë“œë¡œ ë¼ìš°íŒ…ë˜ì§€ ì•Šì•„ì•¼ í•©ë‹ˆë‹¤.
    mode = state.user_intent 
    if mode != "search_criteria":
        logger.warning(f"Query generation called for invalid mode: {mode}. Skipping.")
        return {"step": state.step + 1}
    
    if not state.user_request:
        return {"step": state.step + 1}
    
    from backend.agents.recommend.core.search.search_query_generator import search_query_generator
        
    start_time = time.time()
    logger.info(f"ğŸš€ Starting Search Query Generation (Mode: {mode})")
    
    try:
        result_params = await search_query_generator(user_input=state.user_request)
        
        # ê²°ê³¼ ì¶”ì¶œ ë° ìƒíƒœ ì—…ë°ì´íŠ¸ ì¤€ë¹„
        final_query_str = result_params.get("q", "")
        
        # â­ï¸ search_filters ë”•ì…”ë„ˆë¦¬ì— API í˜¸ì¶œì— í•„ìš”í•œ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
        final_filters = {
            "q": final_query_str, # ì´ í•„ë“œëŠ” GitHub API ê²€ìƒ‰ì— ì‚¬ìš©ë©ë‹ˆë‹¤.
            "sort": result_params.get("sort"),
            "order": result_params.get("order")
        }
        
        elapsed = round(time.time() - start_time, 3)
        timings = dict(state.timings)
        timings["generate_api_query"] = elapsed
        
        logger.info(f"âœ… Query Generated ({mode}): Q='{final_filters}...' | Filters set.")

        return {
            # â­ï¸ search_queryëŠ” LLMì´ ìƒì„±í•œ ìµœì¢… ì¿¼ë¦¬ ë¬¸ìì—´ (API ê²€ìƒ‰ìš©)
            "github_seach_query": final_filters,
            "timings": timings,
            "step": state.step + 1,
            "error": None
        }

    except Exception as e:
        logger.error(f"âŒ Failed to generate query: {e}")
        return {"error": str(e), "failed_step": "generate_api_search_query_node", "step": state.step + 1}
    
async def github_search_node(state: RecommendState) -> Dict[str, Any]:
    """
    [GitHub API ê²€ìƒ‰ ë…¸ë“œ] LLMì´ ìƒì„±í•œ ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ë¡œ GitHub Search APIë¥¼ í˜¸ì¶œí•˜ê³ 
    ê²°ê³¼ë¥¼ search_resultsì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    
    # 1. í•„ìˆ˜ ì…ë ¥ê°’ ì²´í¬
    if not state.github_seach_query or not state.github_seach_query.get("q"):
        logger.warning("No valid search query params found. Skipping GitHub API search.")
        return {"step": state.step + 1}
        
    start_time = time.time()
    
    # 2. GitHub Search í˜¸ì¶œ (ë™ê¸° ì½”ë“œë¥¼ async ë…¸ë“œì—ì„œ í˜¸ì¶œ)
    try:
        raw_results = github_search_instance.search_repositories(state.github_seach_query)
        
        # 3. ê²°ê³¼ íŒŒì‹± ë° CandidateRepoë¡œ ë³€í™˜
        structured_results: List[CandidateRepo] = []
        for item in raw_results:
            # ğŸš¨ NOTE: ì´ ë¶€ë¶„ì€ GitHubSearchì˜ Step 3 (GitHubParser)ì´ ë‹´ë‹¹í•´ì•¼ í•¨
            # ì—¬ê¸°ì„œëŠ” DUMMY ë§¤í•‘ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
            try:
                repo_obj = CandidateRepo(
                    id=getattr(item, "id", 0),
                    name=getattr(item, "name"),
                    owner=getattr(item, "owner"),
                    html_url=getattr(item, "html_url"),
                    description=getattr(item, "description", "GitHub API search result."),
                    main_language=getattr(item, "main_language", "Unknown"),
                    stars=int(getattr(item, "stars", 0)),
                    match_snippet=getattr(item, "match_snippet", "API result.")
                )
                structured_results.append(repo_obj)
            except Exception as ve:
                logger.error(f"Failed to map API result to CandidateRepo: {ve}")

        elapsed = round(time.time() - start_time, 3)
        timings = dict(state.timings)
        timings["github_api_search"] = elapsed

        logger.info(f"âœ… Found {len(structured_results)} candidates from GitHub API in {elapsed}s")

        return {
            "search_results": structured_results,
            "timings": timings,
            "step": state.step + 1,
            "error": None
        }

    except Exception as e:
        logger.error(f"âŒ Node Execution Failed (github_search_node): {e}")
        return {"error": str(e), "failed_step": "github_search_node", "step": state.step + 1}


def check_ingest_error_node(state: RecommendState) -> Dict[str, Any]:
    # ... (check_ingest_error_node êµ¬í˜„ ìœ ì§€) ...
    if not state.error: return {"step": state.step + 1}
    if state.retry_count < state.max_retry:
        return {"error": None, "failed_step": state.failed_step, "retry_count": state.retry_count + 1, "step": state.step + 1}
    return {"step": state.step + 1}

# ------------------------------------------------------------------
# 3. Routing Logic (ë¼ìš°íŒ… ë¡œì§)
# ------------------------------------------------------------------

# â­ï¸ ìƒˆë¡œìš´ ë¼ìš°í„°: parse_initial_request_node ì´í›„
def route_after_parsing(state: RecommendState) -> str:
    """ì´ˆê¸° ì˜ë„ íŒŒì•… í›„ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•©ë‹ˆë‹¤."""
    if state.error:
        return "check_ingest_error_node"
        
    intent = state.user_intent
    
    if intent == "url_analysis":
        logger.info("ğŸš¦ Intent: url_analysis. Routing to fetch_snapshot_node.")
        return "fetch_snapshot_node" 
    
    elif intent == "trend_analysis":
        logger.info("ğŸš¦ Intent: trend_analysis. Routing to trend_search_node.")
        return "trend_search_node"
    
    elif intent == "semantic_search":
        # â­ï¸ semantic_search: RAG ì¿¼ë¦¬ ìƒì„±ìœ¼ë¡œ ì´ë™
        logger.info(f"ğŸš¦ Intent: {intent}. Routing directly to generate_rag_query_node.")
        return "generate_rag_query_node"
    
    elif intent == "search_criteria":
        # â­ï¸ search_criteria: API ì¿¼ë¦¬ ìƒì„±ìœ¼ë¡œ ì´ë™
        logger.info(f"ğŸš¦ Intent: search_criteria. Routing directly to generate_api_search_query_node.")
        return "generate_api_search_query_node"
    
    else:
        logger.warning(f"ğŸš¦ Unknown intent ({intent}). Defaulting to RAG query.")
        return "generate_rag_query_node"


def route_after_fetch(state: RecommendState) -> str:
    if state.error: return "check_ingest_error_node"
    return "analyze_readme_summary_node"

def route_after_analysis(state: RecommendState) -> str:
    if state.error: return "check_ingest_error_node"
    # url_analysis íë¦„: ë¶„ì„ í›„ RAG ì¿¼ë¦¬ ìƒì„±ìœ¼ë¡œ ì´ë™
    return "generate_rag_query_node" # â­ï¸ ìˆ˜ì •


# â­ï¸ route_after_rag_query_gen (RAG ì¿¼ë¦¬ ìƒì„± í›„ Vector DBë¡œ ì§í–‰)
def route_after_rag_query_gen(state: RecommendState) -> str:
    if state.error: return "check_ingest_error_node"
    return "vector_search_node"

# â­ï¸ route_after_api_query_gen (API ì¿¼ë¦¬ ìƒì„± í›„ GitHub Searchë¡œ ì§í–‰)
def route_after_api_query_gen(state: RecommendState) -> str:
    if state.error: return "check_ingest_error_node"
    return "github_search_node" # â­ï¸ GitHub API ê²€ìƒ‰ìœ¼ë¡œ ì´ë™

# â­ï¸ route_after_github_search (GitHub API ê²€ìƒ‰ í›„ Vector DBë¡œ í•©ë¥˜)
def route_after_github_search(state: RecommendState) -> str:
    if state.error: return "check_ingest_error_node"
    return "vector_search_node"

def route_after_vector_search(state: RecommendState) -> str:
    if state.error: return "check_ingest_error_node"
    return "score_candidates_node"

def route_after_scoring(state: RecommendState) -> str:
    if state.error: return "check_ingest_error_node"
    return END

def route_after_error_check(state: RecommendState) -> str:
    if state.error: return END 
    step_map = {
        "parse_initial_request_node": "parse_initial_request_node", 
        "fetch_snapshot_node": "fetch_snapshot_node",
        "analyze_readme_summary_node": "analyze_readme_summary_node",
        "generate_rag_query_node": "generate_rag_query_node",
        "generate_api_search_query_node": "generate_api_search_query_node",
        "github_search_node": "github_search_node",
        "vector_search_node": "vector_search_node",
        "score_candidates_node": "score_candidates_node"
    }
    return step_map.get(state.failed_step, END)

def route_after_trend_search(state: RecommendState) -> str:
    """íŠ¸ë Œë“œ ê²€ìƒ‰ í›„ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•©ë‹ˆë‹¤."""
    if state.error: 
        return "check_ingest_error_node"
    return END


# ------------------------------------------------------------------
# 4. Graph Construction & Execution (build_graph ìˆ˜ì •)
# ------------------------------------------------------------------

def build_graph():
    workflow = StateGraph(RecommendState)
    
    # ë…¸ë“œ ë“±ë¡
    workflow.add_node("parse_initial_request_node", parse_initial_request_node)
    workflow.add_node("fetch_snapshot_node", fetch_snapshot_node)
    workflow.add_node("analyze_readme_summary_node", analyze_readme_summary_node)
    
    # â­ï¸ RAG ì¿¼ë¦¬ ë…¸ë“œì™€ API ì¿¼ë¦¬ ë…¸ë“œë¥¼ ë¶„ë¦¬ ë“±ë¡
    workflow.add_node("generate_rag_query_node", generate_rag_search_query_node)
    workflow.add_node("generate_api_search_query_node", generate_api_search_query_node) 
    
    # â­ï¸ github_search_node ë“±ë¡
    workflow.add_node("github_search_node", github_search_node) 
    
    workflow.add_node("vector_search_node", vector_search_node)
    workflow.add_node("score_candidates_node", score_candidates_node)
    workflow.add_node("check_ingest_error_node", check_ingest_error_node)
    workflow.add_node("trend_search_node", trend_search_node)
    
    # ì§„ì…ì  ì„¤ì •
    workflow.set_entry_point("parse_initial_request_node")
    
    # ì—£ì§€ ì—°ê²°
    workflow.add_conditional_edges("parse_initial_request_node", route_after_parsing)
    workflow.add_conditional_edges("fetch_snapshot_node", route_after_fetch)
    workflow.add_conditional_edges("analyze_readme_summary_node", route_after_analysis)
    
    # â­ï¸ RAG ì¿¼ë¦¬ ë¼ìš°íŒ…: RAG ì¿¼ë¦¬ ìƒì„± í›„ Vector DBë¡œ ì§í–‰
    workflow.add_conditional_edges("generate_rag_query_node", route_after_rag_query_gen) 
    
    # â­ï¸ API ì¿¼ë¦¬ ë¼ìš°íŒ…: API ì¿¼ë¦¬ ìƒì„± í›„ GitHub Searchë¡œ ì§í–‰
    workflow.add_conditional_edges("generate_api_search_query_node", route_after_api_query_gen) 
    
    # â­ï¸ GitHub Search í›„ Vector DBë¡œ í•©ë¥˜
    workflow.add_conditional_edges("github_search_node", route_after_github_search)
    
    workflow.add_conditional_edges("vector_search_node", route_after_vector_search)
    workflow.add_conditional_edges("score_candidates_node", route_after_scoring) 
    workflow.add_conditional_edges("check_ingest_error_node", route_after_error_check)
    
    workflow.add_conditional_edges("trend_search_node", route_after_trend_search)
    
    return workflow.compile()