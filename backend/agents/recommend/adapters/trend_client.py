import aiohttp
import logging
from typing import List, Dict, Any, Optional
from enum import Enum
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)

class TrendingPeriod(Enum):
    DAILY = "daily"
    WEEKLY = "weekly"
    MONTHLY = "monthly"

class GitHubTrendClient:
    """
    í•˜ì´ë¸Œë¦¬ë“œ í´ë¼ì´ì–¸íŠ¸:
    1. OSS Insight API ì‹œë„ (ë¹ ë¦„)
    2. ì‹¤íŒ¨ ì‹œ GitHub Trending í˜ì´ì§€ í¬ë¡¤ë§ (ì•ˆì •ì , Fallback)
    """
    
    # API URL (ëì— ìŠ¬ë˜ì‹œ í¬í•¨)
    API_URL = "https://api.ossinsight.io/v1/trends/repos/"
    # í¬ë¡¤ë§ URL
    CRAWL_URL = "https://github.com/trending"

    async def get_trending_repos(
        self, 
        language: Optional[str] = None, 
        period: TrendingPeriod = TrendingPeriod.WEEKLY,
        limit: int = 10
    ) -> List[Dict[str, Any]]:
        """
        ë©”ì¸ ë©”ì„œë“œ: API ìš°ì„  ì‹œë„ -> ì‹¤íŒ¨ ì‹œ í¬ë¡¤ë§
        """
        # 1. API ì‹œë„
        try:
            logger.info(f"ğŸ“¡ 1ì°¨ ì‹œë„: OSS Insight API ìš”ì²­ (URL: {self.API_URL})")
            results = await self._fetch_from_api(language, period, limit)
            if results:
                return results[:limit]
        except Exception as e:
            logger.warning(f"âš ï¸ API í˜¸ì¶œ ì‹¤íŒ¨ ({e}). í¬ë¡¤ë§ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")

        # 2. í¬ë¡¤ë§ ì‹œë„ (Fallback)
        try:
            results = await self._fetch_from_crawling(language, period)
            logger.info(f"ğŸ•·ï¸ 2ì°¨ ì‹œë„: GitHub í˜ì´ì§€ í¬ë¡¤ë§")

            return results[:limit]
        except Exception as e:
            logger.error(f"âŒ í¬ë¡¤ë§ë§ˆì € ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
            return []

    # =================================================================
    # [Logic 1] API í˜¸ì¶œ (OSS Insight)
    # =================================================================
    async def _fetch_from_api(self, language: str, period: TrendingPeriod, limit: int) -> List[Dict[str, Any]]:
        period_map = {
            TrendingPeriod.DAILY: "past_24_hours",
            TrendingPeriod.WEEKLY: "past_week",
            TrendingPeriod.MONTHLY: "past_month",
        }
        
        # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì„¤ì •
        params = {
            "period": period_map.get(period, "past_week"),
            "limit": limit  # ğŸ’¡ API ìš”ì²­ì— limit íŒŒë¼ë¯¸í„° ì¶”ê°€
        }
        
        if language and language.lower() != "all":
            # ğŸ› ï¸ [Fix] APIê°€ ì†Œë¬¸ì(python)ë¥¼ ì—ëŸ¬ ì²˜ë¦¬í•˜ëŠ” ê²½ìš°ê°€ ìˆì–´ ëŒ€ë¬¸ì(Python)ë¡œ ë³€í™˜
            params["language"] = language.capitalize()

        headers = {
            "Accept": "application/json"
        }

        async with aiohttp.ClientSession() as session:
            async with session.get(self.API_URL, headers=headers, params=params, timeout=5) as response:
                if response.status != 200:
                    # ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ìƒì„¸íˆ ë¡œê¹…í•˜ê¸° ìœ„í•´ ë³¸ë¬¸ ì½ê¸°
                    error_text = await response.text()
                    raise Exception(f"Status: {response.status}, Msg: {error_text}")
                
                data = await response.json()
                return self._parse_api_response(data)

    def _parse_api_response(self, data: dict) -> List[Dict[str, Any]]:
        # ë°ì´í„° êµ¬ì¡° ìœ ì—°í•˜ê²Œ ì²˜ë¦¬ (data.rows ë˜ëŠ” data.data.rows)
        rows = data.get("data", {}).get("rows", [])
        
        # êµ¬ì¡°ê°€ ë‹¤ë¥¼ ê²½ìš° ëŒ€ë¹„ ({ "type": "sql_endpoint", "data": { "rows": ... } })
        if not rows and "data" in data and "rows" in data["data"]:
             rows = data["data"]["rows"]

        if not rows:
            return []
            
        parsed = []
        for idx, item in enumerate(rows):
            full_name = item.get("repo_name", "")
            
            # Owner/Name ìª¼ê°œê¸° (í•„ìˆ˜)
            owner = "Unknown"
            name = full_name
            
            if full_name and "/" in full_name:
                try:
                    owner, name = full_name.split("/", 1)
                except ValueError:
                    pass

            if not name:
                continue

            # ìˆ«ì í•„ë“œ ì•ˆì „ ë³€í™˜ (ë¬¸ìì—´ë¡œ ì˜¬ ìˆ˜ ìˆìŒ)
            try:
                stars = int(item.get("stars", 0))
            except (ValueError, TypeError):
                stars = 0
                
            try:
                # API ë°ì´í„°ì˜ total_score ë“±ì„ stars_sinceë¡œ ëŒ€ì²´
                # (APIì— ì •í™•í•œ period_starsê°€ ì—†ì„ ê²½ìš° ìŠ¤ì½”ì–´ë¥¼ ì‚¬ìš©)
                stars_since = int(float(item.get("total_score", 0))) 
            except (ValueError, TypeError):
                stars_since = 0

            parsed.append({
                "rank": idx + 1,
                "owner": owner,
                "name": name,
                "url": f"https://github.com/{full_name}",
                "description": item.get("description"),
                "language": item.get("primary_language"), # API í•„ë“œëª… ë§¤í•‘
                "total_stars": stars,
                "stars_since": stars_since
            })
        return parsed

    # =================================================================
    # [Logic 2] í¬ë¡¤ë§ (GitHub Web Parsing)
    # =================================================================
    async def _fetch_from_crawling(self, language: str, period: TrendingPeriod) -> List[Dict[str, Any]]:
        url = self.CRAWL_URL
        if language and language.lower() != "all":
            url += f"/{language}"
            
        params = {"since": period.value} # daily, weekly, monthly ê·¸ëŒ€ë¡œ ì‚¬ìš©

        async with aiohttp.ClientSession() as session:
            async with session.get(url, params=params) as response:
                if response.status != 200:
                    raise Exception(f"GitHub Page Error: {response.status}")
                
                html = await response.text()
                return self._parse_html(html)

    def _parse_html(self, html: str) -> List[Dict[str, Any]]:
        soup = BeautifulSoup(html, "html.parser")
        repos = []
        articles = soup.select("article.Box-row")
        
        for article in articles:
            try:
                # Title & URL
                h2 = article.select_one("h2.h3 a")
                if not h2: continue
                full_name = h2.get_text(strip=True).replace(" ", "")
                owner, name = full_name.split("/")
                relative_url = h2["href"]

                # Description
                p = article.select_one("p.col-9")
                description = p.get_text(strip=True) if p else ""
                
                # Language
                lang_span = article.select_one("[itemprop='programmingLanguage']")
                language = lang_span.get_text(strip=True) if lang_span else "Unknown"
                
                # Total Stars
                total_stars_tag = article.select_one(f"a[href='{relative_url}/stargazers']")
                total_stars_str = total_stars_tag.get_text(strip=True).replace(",", "") if total_stars_tag else "0"
                
                # Stars Since (ì˜¤ëŠ˜/ì´ë²ˆì£¼ íšë“ ìŠ¤íƒ€)
                stars_since_span = article.select_one(".float-sm-right")
                stars_since_str = "0"
                if stars_since_span:
                    text = stars_since_span.get_text(strip=True)
                    # "123 stars today" -> "123"
                    stars_since_str = text.split(" ")[0].replace(",", "")

                repos.append({
                    "rank": len(repos) + 1,
                    "owner": owner,
                    "name": name,
                    "url": f"https://github.com{relative_url}",
                    "description": description,
                    "language": language,
                    "total_stars": int(total_stars_str) if total_stars_str.isdigit() else 0,
                    "stars_since": int(stars_since_str) if stars_since_str.isdigit() else 0
                })
            except Exception:
                continue
        return repos

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
trend_client = GitHubTrendClient()