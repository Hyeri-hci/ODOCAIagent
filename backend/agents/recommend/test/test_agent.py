import logging
import time
import asyncio
from typing import Dict, Any, Optional, List
from dataclasses import asdict
from langgraph.graph import StateGraph, END

# [Import] State ë° Core ë¡œì§
from backend.agents.recommend.agent.state import RecommendState
from backend.agents.recommend.core.ingest.summarizer import ContentSummarizer
from backend.core.models import RepoSnapshot
# [Import] ê²€ìƒ‰ ì—”ì§„ (ë°©ê¸ˆ ë§Œë“  ì½”ë“œ)
from backend.agents.recommend.core.search.vector_search import vector_search_engine

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(name)s | %(message)s')
logger = logging.getLogger("TestRealAgent")

# ------------------------------------------------------------------
# 1. Global Instances
# ------------------------------------------------------------------
try:
    summarizer_instance = ContentSummarizer()
    logger.info("âœ… ContentSummarizer initialized.")
except Exception as e:
    logger.error(f"âŒ Failed to init ContentSummarizer: {e}")
    exit(1)

# ------------------------------------------------------------------
# 2. Nodes Definition
# ------------------------------------------------------------------

def fetch_snapshot_node(state: RecommendState) -> Dict[str, Any]:
    """GitHub ì €ì¥ì†Œ ìŠ¤ëƒ…ìƒ· ìˆ˜ì§‘"""
    if state.repo_snapshot:
        logger.info("Reusing existing repo snapshot")
        return {"step": state.step + 1} 
    
    owner, repo = state.owner, state.repo
    if not owner or not repo:
        return {"error": "Missing owner/repo", "step": state.step + 1}
    
    from backend.core.github_core import fetch_repo_snapshot
    start_time = time.time()
    
    try:
        snapshot = fetch_repo_snapshot(owner, repo, getattr(state, 'ref', 'main'))
        snapshot_dict = snapshot.model_dump() if hasattr(snapshot, "model_dump") else asdict(snapshot)
        
        elapsed = round(time.time() - start_time, 3)
        timings = dict(state.timings)
        timings["fetch_snapshot"] = elapsed
        
        logger.info(f"Fetched snapshot for {owner}/{repo} in {elapsed}s")
        return {"repo_snapshot": snapshot_dict, "timings": timings, "step": state.step + 1}
        
    except Exception as e:
        logger.error(f"Failed to fetch snapshot: {e}")
        return {"error": str(e), "failed_step": "fetch_snapshot_node", "step": state.step + 1}

async def analyze_readme_summary_node(state: RecommendState) -> Dict[str, Any]:
    """README ë¶„ì„ ë° ìš”ì•½"""
    if state.readme_summary: return {"step": state.step + 1}
    if not state.repo_snapshot: 
        return {"error": "No snapshot", "failed_step": "analyze_readme_summary_node", "step": state.step + 1}
    
    from backend.core.docs_core import analyze_docs, extract_and_structure_summary_input
    start_time = time.time()
    
    try:
        snapshot_dict = state.repo_snapshot
        readme_content = snapshot_dict.get("readme_content") or "" 
        snapshot_obj = RepoSnapshot(**snapshot_dict)

        docs_result = analyze_docs(snapshot_obj)
        docs_result_dict = asdict(docs_result)
        llm_input_text = extract_and_structure_summary_input(readme_content)

        final_summary = "No summary generated."
        if llm_input_text and len(readme_content) > 50:
            final_summary = await summarizer_instance.summarize(llm_input_text)
            logger.info("LLM summary generated successfully.")
        else:
            logger.warning("Skipping LLM summary: README too short/empty.")

        ingest_result = {
            "final_summary": final_summary,
            "docs_analysis": docs_result_dict,
            "readme_word_count": len(readme_content.split()),
            "documentation_quality": docs_result_dict.get("total_score", 0)
        }

        elapsed = round(time.time() - start_time, 3)
        timings = dict(state.timings)
        timings["analyze_readme_summary"] = elapsed

        return {"readme_summary": ingest_result, "timings": timings, "step": state.step + 1, "error": None}

    except Exception as e:
        logger.error(f"Failed to analyze README: {e}")
        return {"error": str(e), "failed_step": "analyze_readme_summary_node", "step": state.step + 1}

async def generate_search_query_node(state: RecommendState) -> Dict[str, Any]:
    """RAG ì¿¼ë¦¬ ìƒì„± (Fallback ë¡œì§ í¬í•¨)"""
    from backend.agents.recommend.core.search.rag_query_generator import generate_rag_query_and_filters
    
    start_time = time.time()
    logger.info("ğŸš€ Starting RAG Query Generation")

    # [ì¤‘ìš”] READMEê°€ ì—†ì–´ë„ ê¸°ë³¸ ì •ë³´ë§Œ ìˆìœ¼ë©´ ë¶„ì„ ëª¨ë“œ ì§„ì…
    if state.repo_snapshot:
        analyzed_data = {
            "repo_snapshot": state.repo_snapshot,
            "readme_summary": state.readme_summary if state.readme_summary else {}
        }
    else:
        analyzed_data = None


    mode = state.user_intent

    user_input = state.user_request if state.user_request.strip() else "Find similar projects."

    try:
        result = await generate_rag_query_and_filters(user_input, mode, analyzed_data)
        
        elapsed = round(time.time() - start_time, 3)
        timings = dict(state.timings)
        timings["generate_query"] = elapsed
        
        logger.info(f"âœ… Query Generated ({mode}): {result['query']}")

        return {
            "search_query": result.get("query", ""),
            "search_keywords": result.get("keywords", []),
            "search_filters": result.get("filters", {}),
            "timings": timings,
            "step": state.step + 1,
            "error": None
        }

    except Exception as e:
        logger.error(f"âŒ Failed to generate query: {e}")
        return {"error": str(e), "failed_step": "generate_search_query_node", "step": state.step + 1}

# =================================================================
# ğŸ‘‡ [NEW] 4. Vector Search Node (DB ì¡°íšŒ)
# =================================================================
def vector_search_node(state: RecommendState) -> Dict[str, Any]:
    """ìƒì„±ëœ ì¿¼ë¦¬ë¡œ Qdrant ê²€ìƒ‰ ìˆ˜í–‰"""
    
    # ì• ë‹¨ê³„ì—ì„œ ì¿¼ë¦¬ ìƒì„±ì— ì‹¤íŒ¨í–ˆë‹¤ë©´ ì‹¤í–‰ ë¶ˆê°€
    if not state.search_query:
        logger.warning("No search query found. Skipping vector search.")
        return {"step": state.step + 1}
    
    from backend.agents.recommend.agent.state import RecommendState, CandidateRepo

    start_time = time.time()
    logger.info(f"ğŸ” Executing Vector Search for: '{state.search_query}'")

    try:
        # 1. DB ê²€ìƒ‰ ì‹¤í–‰
        result = vector_search_engine.search(
            query=state.search_query,
            filters=state.search_filters,
            target_k=10
        )
        
        raw_recommendations = result.get("final_recommendations", [])
        
        # 2. [í•µì‹¬] Raw Dict -> CandidateRepo ê°ì²´ë¡œ ë³€í™˜ (Mapping)
        structured_results: List[CandidateRepo] = []
        
        for item in raw_recommendations:
            # Qdrant/FlashRank ê²°ê³¼ì—ì„œ í•„ë“œë¥¼ ë§¤í•‘í•˜ì—¬ ê°ì²´ ìƒì„±
            repo_obj = CandidateRepo(
                id=item.get("project_id"),
                name=item.get("name"),
                owner=item.get("owner"),
                description=item.get("description"),
                stars=int(item.get("stars", 0)),
                forks=int(item.get("forks", 0)),
                main_language=item.get("main_language", "UNKNOWN"),
                language=item.get("languages") or [],
                topics=item.get("topics") or [],
                html_url=item.get("repo_url") or f"https://github.com/{item.get('owner')}/{item.get('name')}",
                
                # ê²€ìƒ‰ ì—”ì§„ì´ ê³„ì‚°í•œ ì ìˆ˜ì™€ ìŠ¤ë‹ˆí«
                score=item.get("rerank_score", 0.0),
                match_snippet=item.get("match_snippet", ""),
            )
            structured_results.append(repo_obj)
        
        elapsed = round(time.time() - start_time, 3)
        timings = dict(state.timings)
        timings["vector_search"] = elapsed
        
        logger.info(f"âœ… Found {len(structured_results)} recommendations in {elapsed}s")

        return {
            "search_results": structured_results, # â­ï¸ ì´ì œ ê°ì²´ ë¦¬ìŠ¤íŠ¸ê°€ ì €ì¥ë¨
            "timings": timings,
            "step": state.step + 1,
            "error": None
        }

    except Exception as e:
        logger.error(f"âŒ Vector search failed: {e}")
        return {
            "error": str(e), 
            "failed_step": "vector_search_node", 
            "step": state.step + 1
        }


def check_ingest_error_node(state: RecommendState) -> Dict[str, Any]:
    """ì—ëŸ¬ ë³µêµ¬ ë¡œì§"""
    if not state.error: return {"step": state.step + 1}
    
    logger.warning(f"âš ï¸ Error in {state.failed_step}. Retry {state.retry_count}/{state.max_retry}")
    
    if state.retry_count < state.max_retry:
        return {"error": None, "failed_step": state.failed_step, "retry_count": state.retry_count + 1, "step": state.step + 1}
    
    logger.error("ğŸš« Max retries reached.")
    return {"step": state.step + 1}

# ------------------------------------------------------------------
# 3. Routing Logic
# ------------------------------------------------------------------

def route_after_fetch(state: RecommendState) -> str:
    if state.error: return "check_ingest_error_node"
    return "analyze_readme_summary_node"

def route_after_analysis(state: RecommendState) -> str:
    if state.error: return "check_ingest_error_node"
    return "generate_search_query_node"

def route_after_query_gen(state: RecommendState) -> str:
    if state.error: return "check_ingest_error_node"
    # âœ… [ë³€ê²½] ì¿¼ë¦¬ ìƒì„± ì„±ê³µ ì‹œ -> ê²€ìƒ‰ ë…¸ë“œë¡œ ì´ë™
    return "vector_search_node"

def route_after_vector_search(state: RecommendState) -> str:
    """ê²€ìƒ‰ í›„ ë¼ìš°íŒ…"""
    if state.error: return "check_ingest_error_node"
    # ëª¨ë“  ì‘ì—… ì™„ë£Œ -> ì¢…ë£Œ
    return END

def route_after_error_check(state: RecommendState) -> str:
    if state.error: return END 
    
    # ì¬ì‹œë„ ë¼ìš°íŒ…
    step_map = {
        "fetch_snapshot_node": "fetch_snapshot_node",
        "analyze_readme_summary_node": "analyze_readme_summary_node",
        "generate_search_query_node": "generate_search_query_node",
        "vector_search_node": "vector_search_node" # ê²€ìƒ‰ ì¬ì‹œë„ ì¶”ê°€
    }
    return step_map.get(state.failed_step, END)

# ------------------------------------------------------------------
# 4. Graph Construction & Execution
# ------------------------------------------------------------------

def build_graph():
    workflow = StateGraph(RecommendState)
    
    workflow.add_node("fetch_snapshot_node", fetch_snapshot_node)
    workflow.add_node("analyze_readme_summary_node", analyze_readme_summary_node)
    workflow.add_node("generate_search_query_node", generate_search_query_node)
    # ğŸ‘‡ [ì¶”ê°€] DB ê²€ìƒ‰ ë…¸ë“œ
    workflow.add_node("vector_search_node", vector_search_node)
    workflow.add_node("check_ingest_error_node", check_ingest_error_node)
    
    workflow.set_entry_point("fetch_snapshot_node")
    
    workflow.add_conditional_edges("fetch_snapshot_node", route_after_fetch)
    workflow.add_conditional_edges("analyze_readme_summary_node", route_after_analysis)
    workflow.add_conditional_edges("generate_search_query_node", route_after_query_gen)
    # ğŸ‘‡ [ì¶”ê°€] ê²€ìƒ‰ í›„ ì¢…ë£Œ ì—£ì§€
    workflow.add_conditional_edges("vector_search_node", route_after_vector_search)
    workflow.add_conditional_edges("check_ingest_error_node", route_after_error_check)
    
    return workflow.compile()

async def main():
    target_owner = "Hyeri-hci"
    target_repo = "ODOCAIagent" 
    
    user_request_scenario = "ì´ í”„ë¡œì íŠ¸ë‘ ê¸°ëŠ¥ì€ ë¹„ìŠ·í•œë°, ì–¸ì–´ëŠ” Pythonìœ¼ë¡œ ëœ í”„ë¡œì íŠ¸ ì°¾ì•„ì¤˜."

    print(f"\n======== ğŸ§ª TESTING REAL AGENT : {target_owner}/{target_repo} ========")
    print(f"ğŸ“ User Request: {user_request_scenario}\n")
    
    graph = build_graph()
    initial_state = RecommendState(
        repo_url=f"https://github.com/{target_owner}/{target_repo}",
        owner=target_owner,
        repo=target_repo,
        user_request=user_request_scenario
    )
    
    final_state = {} 
    
    async for event in graph.astream(initial_state):
        for key, value in event.items():
            print(f" -> Node Completed: {key}")
            # [ìˆ˜ì • 2] ë®ì–´ì“°ì§€ ì•Šê³  ê¸°ì¡´ ì •ë³´ì— í•©ì¹©ë‹ˆë‹¤ (State Accumulation)
            final_state.update(value) 

    print("\n======== ğŸ“Š FINAL RESULT ========")
    if final_state:
        # 0. README ë¶„ì„ ê²°ê³¼
        summary_data = final_state.get("readme_summary", {})
        print(f"\nğŸ“„ [README Analysis]")
        print(f"   - Quality Score: {summary_data.get('documentation_quality', 0)}")
        print(f"   - Summary Result: {summary_data.get('final_summary', 'N/A')}")

        # 1. ì¿¼ë¦¬ ì •ë³´
        print(f"\nğŸ” [Generated Search Params]")
        # ì´ì œ ëˆ„ì ëœ ì •ë³´ ë•ë¶„ì— Queryê°€ Noneì´ ì•„ë‹ ê²ë‹ˆë‹¤.
        print(f"   - Query: {final_state.get('search_query')}") 
        print(f"   - Filters: {final_state.get('search_filters')}")
        
        # 2. ì¶”ì²œ ê²°ê³¼ ê²€ì¦
        results = final_state.get("search_results", [])
        print(f"\nğŸ† [Recommended Projects] Found: {len(results)}")
        
        for idx, item in enumerate(results, 1):
            # [ê²€ì¦ í¬ì¸íŠ¸] ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹ˆë¼ ê°ì²´ì´ë¯€ë¡œ ì (.)ìœ¼ë¡œ ì ‘ê·¼í•´ì•¼ í•©ë‹ˆë‹¤.
            # ë§Œì•½ ì—¬ê¸°ì„œ ì—ëŸ¬ê°€ ì•ˆ ë‚˜ê³  ì¶œë ¥ëœë‹¤ë©´ Stateì— ê°ì²´ë¡œ ì˜ ì €ì¥ëœ ê²ƒì…ë‹ˆë‹¤.
            
            print(f"   {idx}. {item.name} (â­ {item.stars})")  # item['name'] ì•„ë‹˜!
            print(f"      - ID: {item.id} | Lang: {item.language}")
            
            # íƒ€ì… í™•ì¸ìš© ë¡œê·¸ (í…ŒìŠ¤íŠ¸ë‹ˆê¹Œ ì°ì–´ë´„)
            # print(f"      - [Type Check]: {type(item)}") 
            
            snippet = item.match_snippet
            clean_snippet = snippet.replace("\n", " ")
            print(f"      - Match: {clean_snippet[:100]}..." if len(clean_snippet) > 100 else f"      - Match: {clean_snippet}")
            print(f"      - Score: {item.score:.4f}")
            print()
            
        print(f"ğŸ”¹ Timings: {final_state.get('timings')}")
    else:
        print("âŒ Analysis Failed.")

if __name__ == "__main__":
    asyncio.run(main())