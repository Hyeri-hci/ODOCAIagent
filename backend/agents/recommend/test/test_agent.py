import logging
import time
import asyncio
from typing import Dict, Any, Optional, List
from dataclasses import asdict
from langgraph.graph import StateGraph, END
from pydantic import BaseModel, ValidationError

# [Import] State ë° Core ë¡œì§
from backend.agents.recommend.agent.state import RecommendState
from backend.agents.recommend.core.ingest.summarizer import ContentSummarizer
from backend.core.models import RepoSnapshot
# [Import] ê²€ìƒ‰ ì—”ì§„ (ë°©ê¸ˆ ë§Œë“  ì½”ë“œ)
from backend.agents.recommend.core.search.vector_search import vector_search_engine
from backend.agents.recommend.core.analysis.match_score import RepoScorer
from backend.agents.recommend.core.intent_parsing import extract_initial_metadata
from backend.agents.recommend.core.trend.get_trend import trend_service, ParsedTrendingRepo
from backend.agents.recommend.agent.state import CandidateRepo

from langchain_openai import ChatOpenAI
from backend.agents.recommend.config.setting import settings

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(name)s | %(message)s')
logger = logging.getLogger("TestRealAgent")

# ------------------------------------------------------------------
# 1. Global Instances
# ------------------------------------------------------------------
try:
    globals()['llm'] = ChatOpenAI(
        base_url=settings.llm.api_base,
        api_key=settings.llm.api_key,
        model=settings.llm.model_name,
        temperature=0
    )

    summarizer_instance = ContentSummarizer()
    scorer_instance = RepoScorer() # Scorer ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ë¹„ìš© ì ˆì•½ì„ ìœ„í•´ ì¬ì‚¬ìš©)
    logger.info("âœ… Global instances initialized.")
except Exception as e:
    logger.error(f"âŒ Failed to init global instances: {e}")
    exit(1)

# ------------------------------------------------------------------
# 2. Nodes Definition
# ------------------------------------------------------------------

async def parse_initial_request_node(state: RecommendState) -> Dict[str, Any]:
    """
    [ì²« ì‹¤í–‰ ë…¸ë“œ] ì‚¬ìš©ì ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ì˜ë„ì™€ ì •ëŸ‰ì  í•„í„° ì¡°ê±´ë§Œ ì¶”ì¶œí•˜ê³  ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    (í•µì‹¬ ë¡œì§ì€ core/intent_parsing.pyì˜ extract_initial_metadataë¥¼ í˜¸ì¶œ)
    """
    
    user_request = state.user_request
    repo_url = state.repo_url
    
    try:
        llm_client = globals()['llm'] 
    except KeyError:
        logger.error("âŒ LLM client ('llm') not initialized in global scope.")
        return {"user_intent": "semantic_search", "quantitative_filters": []}

    if not user_request and not repo_url:
        logger.warning("Request is empty. Defaulting to semantic_search.")
        return {"user_intent": "semantic_search", "quantitative_filters": []}

    try:
        # 2. í•µì‹¬ ë¡œì§ í˜¸ì¶œ (Core Logic ì‹¤í–‰)
        result = await extract_initial_metadata(
            llm_client=llm_client, 
            user_request=user_request,
            repo_url=repo_url
        )
        
        logger.info(f"âœ… Initial Parsing Result: Intent={result.user_intent}, Filters={len(result.quantitative_filters)}")
        
        # 3. LangGraph ìƒíƒœ ì—…ë°ì´íŠ¸ìš© ë§µ ë°˜í™˜
        # ë°˜í™˜ëœ ë”•ì…”ë„ˆë¦¬ì˜ í‚¤(user_intent, quantitative_filters)ê°€ RecommendStateì˜ í•„ë“œë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        return {
            "user_intent": result.user_intent,
            "quantitative_filters": result.quantitative_filters
        }
    
    except Exception as e:
        logger.error(f"âŒ Node Execution Failed (parse_initial_request_node): {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ í´ë°± ì²˜ë¦¬
        return {
            "user_intent": "semantic_search",
            "error": f"Initial parsing failed: {e.__class__.__name__}",
            "failed_step": "parse_initial_request_node"
        }

def fetch_snapshot_node(state: RecommendState) -> Dict[str, Any]:
    """GitHub ì €ì¥ì†Œ ìŠ¤ëƒ…ìƒ· ìˆ˜ì§‘"""
    if state.repo_snapshot:
        logger.info("Reusing existing repo snapshot")
        return {"step": state.step + 1} 
    
    owner, repo = state.owner, state.repo
    if not owner or not repo:
        return {"error": "Missing owner/repo", "step": state.step + 1}
    
    from backend.core.github_core import fetch_repo_snapshot
    start_time = time.time()
    
    try:
        snapshot = fetch_repo_snapshot(owner, repo, getattr(state, 'ref', 'main'))
        snapshot_dict = snapshot.model_dump() if hasattr(snapshot, "model_dump") else asdict(snapshot)
        
        elapsed = round(time.time() - start_time, 3)
        timings = dict(state.timings)
        timings["fetch_snapshot"] = elapsed
        
        logger.info(f"Fetched snapshot for {owner}/{repo} in {elapsed}s")
        return {"repo_snapshot": snapshot_dict, "timings": timings, "step": state.step + 1}
        
    except Exception as e:
        logger.error(f"Failed to fetch snapshot: {e}")
        return {"error": str(e), "failed_step": "fetch_snapshot_node", "step": state.step + 1}

async def analyze_readme_summary_node(state: RecommendState) -> Dict[str, Any]:
    """README ë¶„ì„ ë° ìš”ì•½"""
    if state.readme_summary: return {"step": state.step + 1}
    if not state.repo_snapshot: 
        return {"error": "No snapshot", "failed_step": "analyze_readme_summary_node", "step": state.step + 1}
    
    from backend.core.docs_core import analyze_docs, extract_and_structure_summary_input
    start_time = time.time()
    
    try:
        snapshot_dict = state.repo_snapshot
        readme_content = snapshot_dict.get("readme_content") or "" 
        snapshot_obj = RepoSnapshot(**snapshot_dict)

        docs_result = analyze_docs(snapshot_obj)
        docs_result_dict = asdict(docs_result)
        llm_input_text = extract_and_structure_summary_input(readme_content)

        final_summary = "No summary generated."
        if llm_input_text and len(readme_content) > 50:
            final_summary = await summarizer_instance.summarize(llm_input_text)
            logger.info("LLM summary generated successfully.")
        else:
            logger.warning("Skipping LLM summary: README too short/empty.")

        ingest_result = {
            "final_summary": final_summary,
            "docs_analysis": docs_result_dict,
            "readme_word_count": len(readme_content.split()),
            "documentation_quality": docs_result_dict.get("total_score", 0)
        }

        elapsed = round(time.time() - start_time, 3)
        timings = dict(state.timings)
        timings["analyze_readme_summary"] = elapsed

        return {"readme_summary": ingest_result, "timings": timings, "step": state.step + 1, "error": None}

    except Exception as e:
        logger.error(f"Failed to analyze README: {e}")
        return {"error": str(e), "failed_step": "analyze_readme_summary_node", "step": state.step + 1}

async def generate_search_query_node(state: RecommendState) -> Dict[str, Any]:
    """RAG ì¿¼ë¦¬ ìƒì„± (Fallback ë¡œì§ í¬í•¨)"""

    from backend.agents.recommend.core.search.rag_query_generator import generate_rag_query_and_filters
    
    start_time = time.time()
    logger.info("ğŸš€ Starting RAG Query Generation")

    # [ì¤‘ìš”] READMEê°€ ì—†ì–´ë„ ê¸°ë³¸ ì •ë³´ë§Œ ìˆìœ¼ë©´ ë¶„ì„ ëª¨ë“œ ì§„ì…
    if state.repo_snapshot:
        analyzed_data = {
            "repo_snapshot": state.repo_snapshot,
            "readme_summary": state.readme_summary if state.readme_summary else {}
        }
    else:
        analyzed_data = None


    mode = state.user_intent

    user_input = state.user_request if state.user_request.strip() else "Find similar projects."

    try:
        result = await generate_rag_query_and_filters(user_input, mode, analyzed_data)
        
        elapsed = round(time.time() - start_time, 3)
        timings = dict(state.timings)
        timings["generate_query"] = elapsed
        
        logger.info(f"âœ… Query Generated ({mode}): {result['query']}")

        return {
            "search_query": result.get("query", ""),
            "search_keywords": result.get("keywords", []),
            "search_filters": result.get("filters", {}),
            "timings": timings,
            "step": state.step + 1,
            "error": None
        }

    except Exception as e:
        logger.error(f"âŒ Failed to generate query: {e}")
        return {"error": str(e), "failed_step": "generate_search_query_node", "step": state.step + 1}

# =================================================================
# ğŸ‘‡ [NEW] 4. Vector Search Node (DB ì¡°íšŒ)
# =================================================================
def vector_search_node(state: RecommendState) -> Dict[str, Any]:
    """ìƒì„±ëœ ì¿¼ë¦¬ë¡œ Qdrant ê²€ìƒ‰ ìˆ˜í–‰"""
    
    # ì• ë‹¨ê³„ì—ì„œ ì¿¼ë¦¬ ìƒì„±ì— ì‹¤íŒ¨í–ˆë‹¤ë©´ ì‹¤í–‰ ë¶ˆê°€
    if not state.search_query:
        logger.warning("No search query found. Skipping vector search.")
        return {"step": state.step + 1}
    
    from backend.agents.recommend.agent.state import CandidateRepo

    start_time = time.time()
    logger.info(f"ğŸ” Executing Vector Search for: '{state.search_query}'")

    try:
        # 1. DB ê²€ìƒ‰ ì‹¤í–‰
        result = vector_search_engine.search(
            query=state.search_query,
            filters=state.search_filters,
            target_k=10
        )
        
        raw_recommendations = result.get("final_recommendations", [])
        
        # 2. [í•µì‹¬] Raw Dict -> CandidateRepo ê°ì²´ë¡œ ë³€í™˜ (Mapping)
        structured_results: List[CandidateRepo] = []
        
        for item in raw_recommendations:
            # Qdrant/FlashRank ê²°ê³¼ì—ì„œ í•„ë“œë¥¼ ë§¤í•‘í•˜ì—¬ ê°ì²´ ìƒì„±
            repo_obj = CandidateRepo(
                id=item.get("project_id"),
                name=item.get("name"),
                owner=item.get("owner"),
                description=item.get("description"),
                stars=int(item.get("stars", 0)),
                forks=int(item.get("forks", 0)),
                main_language=item.get("main_language", "UNKNOWN"),
                languages=item.get("languages") or [],
                topics=item.get("topics") or [],
                html_url=item.get("repo_url") or f"https://github.com/{item.get('owner')}/{item.get('name')}",
                
                # ê²€ìƒ‰ ì—”ì§„ì´ ê³„ì‚°í•œ ì ìˆ˜ì™€ ìŠ¤ë‹ˆí«
                score=item.get("rerank_score", 0.0),
                match_snippet=item.get("match_snippet", ""),
            )
            structured_results.append(repo_obj)
        
        elapsed = round(time.time() - start_time, 3)
        timings = dict(state.timings)
        timings["vector_search"] = elapsed
        
        logger.info(f"âœ… Found {len(structured_results)} recommendations in {elapsed}s")

        return {
            "search_results": structured_results,
            "timings": timings,
            "step": state.step + 1,
            "error": None
        }

    except Exception as e:
        logger.error(f"âŒ Vector search failed: {e}")
        return {
            "error": str(e), 
            "failed_step": "vector_search_node", 
            "step": state.step + 1
        }
    
# =================================================================
# ğŸ‘‡ [NEW] 5. Scoring Node (LLM í‰ê°€)
# =================================================================
async def score_candidates_node(state: RecommendState) -> Dict[str, Any]:
    """LLMì„ ì´ìš©í•œ í›„ë³´êµ° ìƒì„¸ í‰ê°€"""
    
    # 1. í‰ê°€í•  í›„ë³´ê°€ ì—†ìœ¼ë©´ íŒ¨ìŠ¤
    if not state.search_results:
        logger.info("No candidates to score. Skipping.")
        return {"step": state.step + 1}

    start_time = time.time()
    logger.info(f"ğŸ§  Scoring {len(state.search_results)} candidates...")

    try:
        # 2. Stateì— ìˆëŠ” Dict í˜•íƒœì˜ Snapshotì„ ê°ì²´ë¡œ ë³µì› (Scorerê°€ ê°ì²´ë¥¼ ìš”êµ¬í•¨)
        source_snapshot = None
        if state.repo_snapshot:
            source_snapshot = RepoSnapshot(**state.repo_snapshot)
        
        # 3. Readme ìš”ì•½ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        readme_summary_text = ""
        if state.readme_summary and isinstance(state.readme_summary, dict):
            readme_summary_text = state.readme_summary.get("final_summary", "")

        # 4. Scorer ì‹¤í–‰
        scored_results = await scorer_instance.evaluate_candidates(
            candidates=state.search_results,     # vector_search ê²°ê³¼
            user_request=state.user_request,
            intent=state.user_intent,            # "semantic_search" or "url_analysis"
            source_repo=source_snapshot,         # ì›ë³¸ ê°ì²´
            readme_summary=readme_summary_text   # ìš”ì•½ë³¸ ìŠ¤íŠ¸ë§
        )

        elapsed = round(time.time() - start_time, 3)
        timings = dict(state.timings)
        timings["ai_scoring"] = elapsed

        logger.info(f"âœ… Scoring complete. Top 1: {scored_results[0].name} (Score: {scored_results[0].ai_score})")

        return {
            "search_results": scored_results, # ì ìˆ˜ê°€ ë§¤ê²¨ì§„ ë¦¬ìŠ¤íŠ¸ë¡œ ì—…ë°ì´íŠ¸
            "timings": timings,
            "step": state.step + 1,
            "error": None
        }

    except Exception as e:
        logger.error(f"âŒ Scoring failed: {e}")
        # ì—ëŸ¬ê°€ ë‚˜ë„ í”„ë¡œì„¸ìŠ¤ëŠ” ê³„ì† ì§„í–‰ (í‰ê°€ë§Œ ì‹¤íŒ¨)
        return {"error": str(e), "failed_step": "score_candidates_node", "step": state.step + 1}
    
async def trend_search_node(state: RecommendState) -> Dict[str, Any]:
    """
    [íŠ¸ë Œë“œ ê²€ìƒ‰ ë…¸ë“œ] ìƒíƒœì˜ ì •ëŸ‰ì  í•„í„°(TREND_LANGUAGE, TREND_SINCE)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ íŠ¸ë Œë“œ ê²€ìƒ‰ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    start_time = time.time()
    logger.info("ğŸ” Executing Trend Search via TrendService...")

    try:
        # 1. TrendService í˜¸ì¶œ (stateì—ì„œ quantitative_filters ì „ë‹¬)
        raw_search_results = await trend_service.search_trending_repos(
            filters=state.quantitative_filters
        )
        
        # 2. ê²°ê³¼ ë§¤í•‘ ë° ë³€í™˜ (ParsedTrendingRepo -> CandidateRepo)
        structured_results: List[CandidateRepo] = []
        for item in raw_search_results:
            # itemì€ ParsedTrendingRepo ê°ì²´ì´ê±°ë‚˜ Dict í˜•íƒœì…ë‹ˆë‹¤.
            # CandidateRepoê°€ rank, stars_since í•„ë“œë¥¼ í¬í•¨í•˜ë„ë¡ í™•ì¥ë˜ì—ˆìœ¼ë¯€ë¡œ ì§ì ‘ ë³€í™˜ ê°€ëŠ¥í•©ë‹ˆë‹¤.
            try:
                # í•„ë“œê°€ ì¼ì¹˜í•œë‹¤ê³  ê°€ì •í•˜ê³  ë³€í™˜ (stars=total_stars, score=stars_sinceë¥¼ ì„ì‹œë¡œ ì‚¬ìš©)
                repo_obj = CandidateRepo(
                    id=0,
                    name=item.name,
                    owner=item.owner,
                    html_url=item.url,
                    description=item.description,
                    main_language=item.language,
                    stars=item.total_stars,
                    # íŠ¸ë Œë“œ í•„ë“œ ë§¤í•‘
                    rank=item.rank,
                    stars_since=item.stars_since,
                    
                    # RAG í•„ë“œëŠ” 0 ë˜ëŠ” ë¹ˆ ê°’
                    score=0.0,
                    match_snippet=f"Trending Rank: {item.rank}, Stars this period: {item.stars_since}"
                )
                structured_results.append(repo_obj)
            except (KeyError, ValidationError, AttributeError) as ve:
                 logger.warning(f"âš ï¸ Failed to map Trend result to CandidateRepo: {ve}")


        elapsed = round(time.time() - start_time, 3)
        timings = dict(state.timings)
        timings["trend_search"] = elapsed
        
        logger.info(f"âœ… Trend Search Found {len(structured_results)} candidates in {elapsed}s")

        # 3. ìƒíƒœ ì—…ë°ì´íŠ¸
        return {
            "search_results": structured_results, # í™•ì¥ëœ CandidateRepo ê°ì²´ ë¦¬ìŠ¤íŠ¸ ì €ì¥
            "timings": timings,
            "search_query": f"Trending repositories based on filters.",
            "step": state.step + 1,
            "error": None
        }

    except Exception as e:
        logger.error(f"âŒ Trend search failed: {e}")
        return {
            "error": str(e), 
            "failed_step": "trend_search_node", 
            "step": state.step + 1
        }


def check_ingest_error_node(state: RecommendState) -> Dict[str, Any]:
    # ... (check_ingest_error_node êµ¬í˜„ ìœ ì§€) ...
    if not state.error: return {"step": state.step + 1}
    if state.retry_count < state.max_retry:
        return {"error": None, "failed_step": state.failed_step, "retry_count": state.retry_count + 1, "step": state.step + 1}
    return {"step": state.step + 1}

# ------------------------------------------------------------------
# 3. Routing Logic (ë¼ìš°íŒ… ë¡œì§)
# ------------------------------------------------------------------

# â­ï¸ ìƒˆë¡œìš´ ë¼ìš°í„°: parse_initial_request_node ì´í›„
def route_after_parsing(state: RecommendState) -> str:
    """ì´ˆê¸° ì˜ë„ íŒŒì•… í›„ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•©ë‹ˆë‹¤."""
    if state.error:
        return "check_ingest_error_node"
        
    intent = state.user_intent
    
    if intent == "url_analysis":
        # URL ë¶„ì„ ëª¨ë“œ: ìŠ¤ëƒ…ìƒ· ìˆ˜ì§‘ì´ í•„ìš”í•¨
        logger.info("ğŸš¦ Intent: url_analysis. Routing to fetch_snapshot_node.")
        return "fetch_snapshot_node" 
    
    elif intent == "trend_analysis":
        # â­ï¸ ìˆ˜ì •: íŠ¸ë Œë“œ ë¶„ì„ì€ ì „ìš© ë…¸ë“œë¡œ ë¶„ê¸°
        logger.info("ğŸš¦ Intent: trend_analysis. Routing to trend_search_node.")
        return "trend_search_node" # â­ï¸ ì´ ë…¸ë“œë¡œ ì´ë™!
    
    elif intent in ["semantic_search", "search_criteria"]:
        # ì¼ë°˜ ê²€ìƒ‰/ì¡°ê±´ ë¶„ì„ ëª¨ë“œ: ì¿¼ë¦¬ ìƒì„±ìœ¼ë¡œ ì´ë™
        logger.info(f"ğŸš¦ Intent: {intent}. Routing directly to generate_search_query_node.")
        return "generate_search_query_node"
    
    else:
        logger.warning(f"ğŸš¦ Unknown intent ({intent}). Routing to default search.")
        return "generate_search_query_node"
        return "generate_search_query_node"

def route_after_fetch(state: RecommendState) -> str:
    if state.error: return "check_ingest_error_node"
    return "analyze_readme_summary_node"

def route_after_analysis(state: RecommendState) -> str:
    if state.error: return "check_ingest_error_node"
    return "generate_search_query_node"

def route_after_query_gen(state: RecommendState) -> str:
    if state.error: return "check_ingest_error_node"
    return "vector_search_node"

def route_after_vector_search(state: RecommendState) -> str:
    if state.error: return "check_ingest_error_node"
    return "score_candidates_node"

def route_after_scoring(state: RecommendState) -> str:
    if state.error: return "check_ingest_error_node"
    return END

def route_after_error_check(state: RecommendState) -> str:
    if state.error: return END 
    step_map = {
        "parse_initial_request_node": "parse_initial_request_node", # ì¬ì‹œë„ëŠ” ì˜ë¯¸ ì—†ìŒ
        "fetch_snapshot_node": "fetch_snapshot_node",
        "analyze_readme_summary_node": "analyze_readme_summary_node",
        "generate_search_query_node": "generate_search_query_node",
        "vector_search_node": "vector_search_node",
        "score_candidates_node": "score_candidates_node"
    }
    return step_map.get(state.failed_step, END)

def route_after_trend_search(state: RecommendState) -> str:
    """íŠ¸ë Œë“œ ê²€ìƒ‰ í›„ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•©ë‹ˆë‹¤."""
    if state.error: 
        return "check_ingest_error_node"
    return END

# ------------------------------------------------------------------
# 4. Graph Construction & Execution
# ------------------------------------------------------------------

def build_graph():
    workflow = StateGraph(RecommendState)
    
    # ë…¸ë“œ ë“±ë¡
    workflow.add_node("parse_initial_request_node", parse_initial_request_node)
    workflow.add_node("fetch_snapshot_node", fetch_snapshot_node)
    workflow.add_node("analyze_readme_summary_node", analyze_readme_summary_node)
    workflow.add_node("generate_search_query_node", generate_search_query_node)
    workflow.add_node("vector_search_node", vector_search_node)
    workflow.add_node("score_candidates_node", score_candidates_node)
    workflow.add_node("check_ingest_error_node", check_ingest_error_node)
    workflow.add_node("trend_search_node", trend_search_node) # â­ï¸ íŠ¸ë Œë“œ ë…¸ë“œ ë“±ë¡
    
    # ì§„ì…ì  ì„¤ì •
    workflow.set_entry_point("parse_initial_request_node")
    
    # ì—£ì§€ ì—°ê²°
    workflow.add_conditional_edges("parse_initial_request_node", route_after_parsing)
    workflow.add_conditional_edges("fetch_snapshot_node", route_after_fetch)
    workflow.add_conditional_edges("analyze_readme_summary_node", route_after_analysis)
    workflow.add_conditional_edges("generate_search_query_node", route_after_query_gen)
    workflow.add_conditional_edges("vector_search_node", route_after_vector_search)
    workflow.add_conditional_edges("score_candidates_node", route_after_scoring) 
    workflow.add_conditional_edges("check_ingest_error_node", route_after_error_check)
    
    # â­ï¸ íŠ¸ë Œë“œ ì—£ì§€ ì—°ê²°: íŠ¸ë Œë“œ ê²€ìƒ‰ í›„ í‰ê°€ ë…¸ë“œë¡œ ì´ë™
    workflow.add_conditional_edges("trend_search_node", route_after_trend_search)
    
    return workflow.compile()

async def main():
    #target_owner = "Hyeri-hci"
    #target_repo = "OSSDoctor" 
    
    # í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤: URLì´ ìˆê³ , ìœ ì‚¬ í”„ë¡œì íŠ¸ë¥¼ ìš”ì²­í–ˆìœ¼ë¯€ë¡œ 'url_analysis'ë¡œ ë¶„ë¥˜ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
    #user_request_scenario = "ì´ í”„ë¡œì íŠ¸ë‘ ê¸°ëŠ¥ì€ ë¹„ìŠ·í•œë°, ì–¸ì–´ëŠ” Pythonìœ¼ë¡œ ëœ í”„ë¡œì íŠ¸ ì°¾ì•„ì¤˜."
    user_request_scenario = "2025ë…„ì— ìˆê¸°ìˆì—ˆë˜ íŒŒì´ì¬ í”„ë¡œì íŠ¸ ì•Œë ¤ì¤˜"

    #(f"\n======== ğŸ§ª TESTING REAL AGENT : {target_owner}/{target_repo} ========")
    print(f"ğŸ“ User Request: {user_request_scenario}\n")
    
    graph = build_graph()
    
    # user_intentëŠ” ì´ì œ parse_initial_request_nodeê°€ ê²°ì •í•˜ë¯€ë¡œ ì´ˆê¸°ê°’ì€ ë¹„ì›Œë‘¡ë‹ˆë‹¤.
    initial_state = RecommendState(
        #repo_url=f"https://github.com/{target_owner}/{target_repo}",
        #owner=target_owner,
        #repo=target_repo,
        user_request=user_request_scenario,
        user_intent="", # parse_initial_request_nodeê°€ ì±„ìš¸ í•„ë“œ
    )
    
    final_state = initial_state.model_dump() # ì‹œì‘ ìƒíƒœë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
    
    start_time_total = time.time()
    
    # astreamì„ í†µí•´ ê·¸ë˜í”„ ì‹¤í–‰
    async for event in graph.astream(final_state):
        for key, value in event.items():
            if key != END:
                print(f" -> Node Completed: {key}")
                # LangGraphì˜ ì¶œë ¥(Dict)ìœ¼ë¡œ final_stateë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
                final_state.update(value) 

    elapsed_total = round(time.time() - start_time_total, 3)

    print("\n======== ğŸ“Š FINAL RESULT ========")
    if final_state:
        # Pydantic ëª¨ë¸ë¡œ ìµœì¢… ìƒíƒœë¥¼ ë³µì›í•˜ì—¬ ì ‘ê·¼í•©ë‹ˆë‹¤.
        final_state_obj = RecommendState(**final_state)
        
        # 1. ì¿¼ë¦¬ ë° íƒ€ì´ë° ì •ë³´
        print(f"\nğŸ” [Metadata]")
        print(f" Â  - Intent: {final_state_obj.user_intent}")
        print(f" Â  - Query: {final_state_obj.search_query}")
        print(f" Â  - Filters: {final_state_obj.quantitative_filters}")
        print(f"ğŸ”¹ Total Time: {elapsed_total}s | Timings: {final_state_obj.timings}")
        
        # 2. ì¶”ì²œ ê²°ê³¼ (AI ì ìˆ˜ í¬í•¨)
        results = final_state_obj.search_results
        print(f"\nğŸ† [Recommended Projects] Found: {len(results)}")
        
        for idx, item in enumerate(results, 1):
            print(f" Â  {idx}. {item.name} (â­ {item.stars})")
            print(f" Â  Â  Â - ID: {item.id} | Lang: {item.main_language}")
            print(f" Â  Â  Â - ğŸ¤– AI Score: {item.ai_score} / 100")
            print(f" Â  Â  Â - ğŸ“ Reason: {item.ai_reason}")
            snippet = item.match_snippet
            clean_snippet = snippet.replace("\n", " ") if snippet else "No snippet"
            print(f" Â  Â  Â - Match: {clean_snippet[:80]}..." if len(clean_snippet) > 80 else f" Â  Â  Â - Match: {clean_snippet}")
            print()
    else:
        print("âŒ Analysis Failed.")

if __name__ == "__main__":
    asyncio.run(main())