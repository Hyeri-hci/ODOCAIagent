import logging
import time
import asyncio
from typing import Dict, Any, Optional
from dataclasses import asdict
from langgraph.graph import StateGraph, END

from backend.agents.recommend.agent.state import RecommendState
from backend.agents.recommend.core.ingest.summarizer import ContentSummarizer
from backend.core.models import RepoSnapshot

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(name)s | %(message)s')
logger = logging.getLogger("TestRealAgent")

# ------------------------------------------------------------------
# 1. Global Instances (LLM ë“± ë¬´ê±°ìš´ ê°ì²´ëŠ” í•œ ë²ˆë§Œ ìƒì„±)
# ------------------------------------------------------------------
try:
    # ì‹¤ì œ API Keyê°€ ì„¤ì •ë˜ì–´ ìˆì–´ì•¼ ì‹¤í–‰ë©ë‹ˆë‹¤.
    summarizer_instance = ContentSummarizer()
    logger.info("âœ… ContentSummarizer initialized with Real LLM.")
except Exception as e:
    logger.error(f"âŒ Failed to init ContentSummarizer. Check API Keys: {e}")
    exit(1)

# ------------------------------------------------------------------
# 2. Nodes Definition (ì‚¬ìš©ìë‹˜ì˜ ì‹¤ì œ ë¡œì§ ì ìš©)
# ------------------------------------------------------------------

def fetch_snapshot_node(state: RecommendState) -> Dict[str, Any]:
    """
    GitHub ì €ì¥ì†Œ ìŠ¤ëƒ…ìƒ· ìˆ˜ì§‘ ë…¸ë“œ.
    """
    
    # 1. ì¬ì‚¬ìš© ì²´í¬
    if state.repo_snapshot:
        logger.info("Reusing existing repo snapshot")
        return {"step": state.step + 1} 
    
    # 2. í•„ìˆ˜ ì…ë ¥ê°’ ê²€ì¦
    owner = state.owner
    repo = state.repo
    ref = getattr(state, 'ref', 'main') # ref í•„ë“œê°€ ìˆì„ ê²½ìš° ì‚¬ìš©

    # owner, repoê°€ Stateì— ì—†ëŠ” ê²½ìš° ì—ëŸ¬ ì²˜ë¦¬
    if not owner or not repo:
        error_msg = "Owner or repository name is missing in state. (Pre-analysis failure)"
        logger.error(f"Failed to fetch details: {error_msg}")
        return {
            "error": error_msg,
            "failed_step": "fetch_snapshot_node",
            "step": state.step + 1,
        }
    
    from backend.core.github_core import fetch_repo_snapshot

    start_time = time.time()
    
    try:
        
        snapshot = fetch_repo_snapshot(state.owner, state.repo, state.ref)
        
        snapshot_dict = snapshot.model_dump() if hasattr(snapshot, "model_dump") else asdict(snapshot)


        elapsed = round(time.time() - start_time, 3)
        timings = dict(state.timings)
        timings["fetch_snapshot"] = elapsed
        
        logger.info(f"Fetched snapshot for {state.owner}/{state.repo} in {elapsed}s")
        
        return {
            "repo_snapshot": snapshot_dict,
            "timings": timings,
            "step": state.step + 1,
        }
        
    except Exception as e:
        logger.error(f"Failed to fetch snapshot: {e}")
        return { 
            "error": str(e),
            "failed_step": "fetch_snapshot_node",
            "step": state.step + 1,
        }
    
async def analyze_readme_summary_node(state: RecommendState) -> Dict[str, Any]:
    """
    README ë¶„ì„ ë° LLM ìš”ì•½ ë…¸ë“œ.
    
    repo_snapshotì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¡°í™”ëœ LLM ìš”ì•½ì„ ìƒì„±í•˜ê³  DocsCoreResultë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
    """
    
    # 1. ì¬ì‚¬ìš© ì²´í¬ ë° í•„ìˆ˜ ì„ í–‰ ì¡°ê±´ ì²´í¬
    if state.readme_summary:
        logger.info("Reusing existing ingest analysis result")
        return {"step": state.step + 1} 
    
    if not state.repo_snapshot:
        error_msg = "No repo_snapshot available for README analysis."
        logger.error(f"Failed to analyze README: {error_msg}")
        return {
            "error": error_msg,
            "failed_step": "analyze_readme_summary_node",
            "step": state.step + 1,
        }
    
    from backend.core.docs_core import analyze_docs, extract_and_structure_summary_input
    
    start_time = time.time()
    
    try:
        # 2. Stateì—ì„œ ìŠ¤ëƒ…ìƒ· ì¶”ì¶œ
        snapshot_dict = state.repo_snapshot
        
        # readme_contentê°€ Noneì¼ ê²½ìš° ë¹ˆ ë¬¸ìì—´("")ë¡œ ë³€í™˜í•˜ì—¬ ì•ˆì „ì„± í™•ë³´
        readme_content = snapshot_dict.get("readme_content") or "" 

        # Dict â†’ RepoSnapshot ë³€í™˜
        snapshot_obj = RepoSnapshot(**snapshot_dict)

        # 3. ë¬¸ì„œ ë¶„ì„ (DocsCoreResult dataclass)
        docs_result = analyze_docs(snapshot_obj)

        # dataclass â†’ dict ë³€í™˜
        docs_result_dict = asdict(docs_result)

        # 4. LLM ì…ë ¥ êµ¬ì„±
        llm_input_text = extract_and_structure_summary_input(readme_content)

        # 5. LLM ìš”ì•½ ì‹¤í–‰ (ë¹„ë™ê¸°)
        # ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ìš”ì•½ ìŠ¤í‚µ
        final_summary = "No summary generated."
        
        if llm_input_text and len(readme_content) > 50:
            final_summary = await summarizer_instance.summarize(llm_input_text)
            logger.info("LLM summary generated successfully.")
        else:
            logger.warning("Skipping LLM summary: README is empty or too short.")

        # 6. ê²°ê³¼ í†µí•©
        ingest_result = {
            "final_summary": final_summary,
            "docs_analysis": docs_result_dict,
            # ë¹ˆ ë¬¸ìì—´ì´ì–´ë„ ì—ëŸ¬ ì•ˆ ë‚˜ê²Œ ì²˜ë¦¬ë¨
            "readme_word_count": len(readme_content.split()), 
            "documentation_quality": docs_result_dict.get("total_score", 0)
        }

        # 7. ìƒíƒœ ì—…ë°ì´íŠ¸ ë°˜í™˜
        elapsed = round(time.time() - start_time, 3)
        timings = dict(state.timings)
        timings["analyze_readme_summary"] = elapsed

        return {
            "readme_summary": ingest_result,
            "timings": timings,
            "step": state.step + 1,
            "failed_step": None,
            "error": None,
        }

    except Exception as e:
        logger.error(f"Failed to analyze and summarize README: {e}")
        return {
            "error": str(e),
            "failed_step": "analyze_readme_summary_node",
            "step": state.step + 1,
        }
    
async def generate_search_query_node(state: RecommendState) -> Dict[str, Any]:
    """
    RAG ê²€ìƒ‰ ì¿¼ë¦¬ ë° í•„í„° ìƒì„± ë…¸ë“œ.
    
    ìƒí™©ì— ë”°ë¼ ë‘ ê°€ì§€ ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤:
    1. URL ë¶„ì„ ë°ì´í„°(repo_snapshot)ê°€ ìˆìŒ -> 'url_analysis' ëª¨ë“œ (ìœ ì‚¬ë„ ê²€ìƒ‰ + Fallback ë¡œì§ ì ìš©)
    2. URL ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŒ -> 'semantic_search' ëª¨ë“œ (ì¼ë°˜ ê²€ìƒ‰)
    """

    from backend.agents.recommend.core.search.rag_query_generator import generate_rag_query_and_filters
    
    start_time = time.time()
    logger.info("ğŸš€ Starting RAG Query Generation Node")

    # [ìˆ˜ì •ë¨] 1. ëª¨ë“œ ê²°ì • ë° ë¶„ì„ ë°ì´í„° ì¤€ë¹„
    # readme_summaryê°€ ì—†ë”ë¼ë„, repo_snapshot(ê¸°ë³¸ ì •ë³´)ë§Œ ìˆìœ¼ë©´ ë¶„ì„ ëª¨ë“œë¡œ ì§„ì…í•©ë‹ˆë‹¤.
    if state.repo_snapshot:
        mode = "url_analysis"
        
        # Core í•¨ìˆ˜ì— ë„˜ê²¨ì¤„ ë°ì´í„° íŒ¨í‚¤ì§•
        # state.readme_summaryê°€ Noneì¼ ê²½ìš° ë¹ˆ dictë¡œ ì²˜ë¦¬í•˜ì—¬ ì—ëŸ¬ ë°©ì§€
        analyzed_data = {
            "repo_snapshot": state.repo_snapshot,
            "readme_summary": state.readme_summary if state.readme_summary else {}
        }
    else:
        # ìŠ¤ëƒ…ìƒ·ì¡°ì°¨ ì—†ìœ¼ë©´ ì¼ë°˜ ê²€ìƒ‰ ëª¨ë“œ
        mode = "semantic_search"
        analyzed_data = None

    # 2. ì‚¬ìš©ì ìš”ì²­ í…ìŠ¤íŠ¸ (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì„¤ì •)
    user_input = state.user_request if state.user_request.strip() else "Find similar projects."

    try:
        # 3. Core ë¡œì§ í˜¸ì¶œ (LLM ìˆ˜í–‰)
        result = await generate_rag_query_and_filters(
            user_request=user_input,
            category=mode,
            analyzed_data=analyzed_data
        )

        elapsed = round(time.time() - start_time, 3)
        timings = dict(state.timings)
        timings["generate_query"] = elapsed

        logger.info(f"âœ… Query Generated ({mode}): {result['query']} (Time: {elapsed}s)")

        return {
            "search_query": result.get("query", ""),
            "search_keywords": result.get("keywords", []),
            "search_filters": result.get("filters", {}),
            "timings": timings,
            "step": state.step + 1,
            "error": None
        }

    except Exception as e:
        logger.error(f"âŒ Failed to generate search query: {e}")
        return {
            "error": str(e),
            "failed_step": "generate_search_query_node",
            "step": state.step + 1
        }
    


def check_ingest_error_node(state: RecommendState) -> Dict[str, Any]:
    """ì—ëŸ¬ ë³µêµ¬ ë¡œì§"""
    if not state.error:
        return {"step": state.step + 1}
    
    logger.warning(f"âš ï¸ Error caught in step: {state.failed_step}. Retry {state.retry_count}/{state.max_retry}")
    
    if state.retry_count < state.max_retry:
        return {
            "error": None,
            "failed_step": state.failed_step,
            "retry_count": state.retry_count + 1,
            "step": state.step + 1
        }
    
    logger.error("ğŸš« Max retries reached. Giving up.")
    return {"step": state.step + 1}

# ------------------------------------------------------------------
# 3. Routing Logic
# ------------------------------------------------------------------

def route_after_fetch(state: RecommendState) -> str:
    if state.error: return "check_ingest_error_node"
    return "analyze_readme_summary_node"

def route_after_analysis(state: RecommendState) -> str:
    if state.error: return "check_ingest_error_node"
    # âœ… ì„±ê³µ ì‹œ: ì¿¼ë¦¬ ìƒì„± ë…¸ë“œë¡œ ì´ë™
    return "generate_search_query_node"

def route_after_query_gen(state: RecommendState) -> str:
    """ì¿¼ë¦¬ ìƒì„± í›„ ë¼ìš°íŒ…"""
    if state.error: return "check_ingest_error_node"
    # ì¶”í›„ DB ê²€ìƒ‰ ë…¸ë“œê°€ ìˆë‹¤ë©´ ê±°ê¸°ë¡œ ì—°ê²°. ì§€ê¸ˆì€ í…ŒìŠ¤íŠ¸ ì¢…ë£Œ.
    return END

def route_after_error_check(state: RecommendState) -> str:
    if state.error: return END 
    
    # ì¬ì‹œë„ ë¼ìš°íŒ…
    if state.failed_step == "fetch_snapshot_node":
        return "fetch_snapshot_node"
    elif state.failed_step == "analyze_readme_summary_node":
        return "analyze_readme_summary_node"
    elif state.failed_step == "generate_search_query_node":
        return "generate_search_query_node"
    
    return END

# ------------------------------------------------------------------
# 4. Graph Construction & Execution
# ------------------------------------------------------------------

def build_graph():
    workflow = StateGraph(RecommendState)
    
    workflow.add_node("fetch_snapshot_node", fetch_snapshot_node)
    workflow.add_node("analyze_readme_summary_node", analyze_readme_summary_node)
    # ğŸ‘‡ [ì¶”ê°€] ì¿¼ë¦¬ ìƒì„± ë…¸ë“œ ë“±ë¡
    workflow.add_node("generate_search_query_node", generate_search_query_node)
    workflow.add_node("check_ingest_error_node", check_ingest_error_node)
    
    workflow.set_entry_point("fetch_snapshot_node")
    
    workflow.add_conditional_edges("fetch_snapshot_node", route_after_fetch)
    workflow.add_conditional_edges("analyze_readme_summary_node", route_after_analysis)
    # ğŸ‘‡ [ì¶”ê°€] ì¿¼ë¦¬ ìƒì„± í›„ ì—£ì§€
    workflow.add_conditional_edges("generate_search_query_node", route_after_query_gen)
    workflow.add_conditional_edges("check_ingest_error_node", route_after_error_check)
    
    return workflow.compile()

async def main():
    target_owner = "Unani0528"
    target_repo = "ai_cookbook"
    
    # ğŸ§ª [í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤]
    # ì‚¬ìš©ìê°€ OSSDoctor(ë¶„ì„/ëŒ€ì‹œë³´ë“œ ë„êµ¬)ë¥¼ ì£¼ë©´ì„œ
    # "ì´ê±°ë‘ ë¹„ìŠ·í•œë° Pythonìœ¼ë¡œ ëœ ê±¸ ì°¾ê³  ì‹¶ì–´" ë¼ê³  ìš”ì²­í•˜ëŠ” ìƒí™©
    user_request_scenario = "ì´ í”„ë¡œì íŠ¸ë‘ ê¸°ëŠ¥ì€ ë¹„ìŠ·í•œë°, ì–¸ì–´ëŠ” Pythonìœ¼ë¡œ ëœ í”„ë¡œì íŠ¸ ì°¾ì•„ì¤˜."

    print(f"\n======== ğŸ§ª TESTING REAL AGENT : {target_owner}/{target_repo} ========")
    print(f"ğŸ“ User Request: {user_request_scenario}\n")
    
    graph = build_graph()
    initial_state = RecommendState(
        repo_url=f"https://github.com/{target_owner}/{target_repo}",
        owner=target_owner,
        repo=target_repo,
        user_request=user_request_scenario # Stateì— ì…ë ¥ ì£¼ì…
    )
    
    final_state = None
    async for event in graph.astream(initial_state):
        for key, value in event.items():
            print(f" -> Node Completed: {key}")
            final_state = value 

    print("\n======== ğŸ“Š FINAL RESULT ========")
    if final_state:
        # 1. ë¬¸ì„œ í’ˆì§ˆ ì ìˆ˜
        if final_state.get("readme_summary"):
            summary = final_state["readme_summary"]
            print(f"ğŸ”¹ Doc Quality Score: {summary.get('documentation_quality')}")
        
        # 2. ìƒì„±ëœ ê²€ìƒ‰ ì¿¼ë¦¬ ê²°ê³¼ (ê°€ì¥ ì¤‘ìš”)
        print(f"\nğŸ” [Generated Search Params]")
        print(f"   - Query: {final_state.get('search_query')}")
        print(f"   - Keywords: {final_state.get('search_keywords')}")
        print(f"   - Filters: {final_state.get('search_filters')}")
        
        print(f"\nğŸ”¹ Timings: {final_state.get('timings')}")
    else:
        print("âŒ Analysis Failed.")

if __name__ == "__main__":
    asyncio.run(main())