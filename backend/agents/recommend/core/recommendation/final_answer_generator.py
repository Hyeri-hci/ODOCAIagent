import json
import logging
import re
import asyncio
from typing import Dict, Any, List, Optional
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate 
from backend.agents.recommend.config.setting import settings 

logger = logging.getLogger(__name__)

# LLM for reasoning (ìƒì„± í’ˆì§ˆ ë° JSON ì•ˆì •ì„±ì„ ìœ„í•´ ì„¤ì •)
try:
    llm = ChatOpenAI(
        base_url=settings.llm.api_base,
        api_key=settings.llm.api_key,
        model=settings.llm.model_name,
        temperature=0.3 
    )
except Exception as e:
    logger.error(f"LLM Client Initialization Failed in Core: {e}")
    raise e 


SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ GitHub í”„ë¡œì íŠ¸ ì¶”ì²œ ë° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ì‚¬ìš©ì ìš”ì²­, **ë¶„ì„ëœ í™œë™ ì¡°ê±´**, ê·¸ë¦¬ê³  ê²€ìƒ‰ëœ í”„ë¡œì íŠ¸ í›„ë³´ ëª©ë¡(JSON)ì„ ë°”íƒ•ìœ¼ë¡œ,
ê° í”„ë¡œì íŠ¸ê°€ ì™œ ì¶”ì²œë˜ëŠ”ì§€ **ê°€ì¥ í•µì‹¬ì ì¸ ì´ìœ **ë¥¼ ëª…í™•í•˜ê³  ì„¤ë“ë ¥ ìˆê²Œ **í•œêµ­ì–´ë¡œ í•œ ë¬¸ì¥**ìœ¼ë¡œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.

### ì—­í• 
1. ê° í›„ë³´ í”„ë¡œì íŠ¸ì˜ ì¥ì ì„ ë¶„ì„í•˜ê³ , **ì‚¬ìš©ìê°€ ìš”êµ¬í•œ í™œë™ ì¡°ê±´**ê³¼ ë°ì´í„°(`recent_commits` ë“±)ë¥¼ ì—°ê²°í•˜ì—¬ ì¶”ì²œ ê·¼ê±°ë¥¼ ì‘ì„±í•˜ì‹­ì‹œì˜¤.
2. ë§Œì•½ í›„ë³´ ë°ì´í„°ì— í™œë™ ì§€í‘œê°€ í¬í•¨ë˜ì–´ ìˆë‹¤ë©´ (ì˜ˆ: recent_commits), ì´ë¥¼ ê·¼ê±°ë¡œ ì œì‹œí•´ì•¼ í•©ë‹ˆë‹¤.
3. ìµœì¢… ê²°ê³¼ëŠ” ë°˜ë“œì‹œ ì•„ë˜ì˜ **JSON í˜•ì‹**ìœ¼ë¡œë§Œ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ì„œë¡ ì´ë‚˜ ì„¤ëª…ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.

### ì¶œë ¥ í˜•ì‹ (JSON ONLY)
{
    "summary_reasoning": "ì „ì²´ ì¶”ì²œì— ëŒ€í•œ ìš”ì•½ ë¬¸êµ¬ ë° ì¶”ì²œ ì´ìœ  (í•œêµ­ì–´)",
    "top_candidates": [
        {
            "name": "í”„ë¡œì íŠ¸ ì´ë¦„",
            "url": "URL",
            "recommendation_reason": "ì—¬ê¸°ì— í•µì‹¬ ì¶”ì²œ ì´ìœ ì™€ í•¨ê»˜ ì‚¬ìš©ìì˜ ìš”ì²­(ì˜ˆ: ì»¤ë°‹ ìˆ˜)ì— ë¶€í•©í•˜ëŠ” ê·¼ê±°ë¥¼ ì‘ì„±í•˜ì‹­ì‹œì˜¤."
        }
        // ... ìµœëŒ€ 5ê°œê¹Œì§€ ë°˜ë³µ ...
    ]
}
"""

async def generate_final_report(
    user_query: str, 
    candidates: List[Dict[str, Any]],
    other_conditions: Optional[str] = None # ğŸ‘ˆ [ì¶”ê°€] ì‚¬ìš©ìê°€ ìš”êµ¬í•œ í™œë™ ì¡°ê±´
) -> str:
    """
    [í•µì‹¬ ë¡œì§] í›„ë³´ ëª©ë¡ì„ ë°›ì•„ LLMì„ í˜¸ì¶œí•˜ì—¬ ì¶”ì²œ ì´ìœ ë¥¼ ìƒì„±í•˜ê³  ìµœì¢… ë³´ê³ ì„œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if not candidates:
        return json.dumps({"summary_reasoning": "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ ìµœì¢… ë³´ê³ ì„œë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}, ensure_ascii=False)

    print("âœ¨ [Core Logic] Generating Final Recommendation Report...")
    
    # í›„ë³´ ëª©ë¡ì„ ìµœëŒ€ 5ê°œë¡œ ì œí•œ ë° LLMì´ ì½ê¸° ì‰½ë„ë¡ ë°ì´í„° ê°„ì†Œí™”
    top_candidates = candidates[:5]
    simplified_candidates = []
    
    for cand in top_candidates:
        content_summary = cand.get('content', '')[:300] 
        
        # ğŸ’¡ [í•µì‹¬] í•„í„°ë§ ë„êµ¬(filter_exec)ì—ì„œ ì¶”ê°€ëœ í™œë™ì„± ì§€í‘œë¥¼ LLMì— ì£¼ì…
        simplified_candidates.append({
            "name": cand.get('name'),
            "stars": cand.get('stars', cand.get('stargazers_count', 0)),
            "language": cand.get('language', 'N/A'),
            "topics": cand.get('topics', []),
            "content_snippet": content_summary,
            "recent_commits": cand.get('recent_commits', 'N/A'), # filter_execì—ì„œ ì¶”ê°€ëœë‹¤ê³  ê°€ì •
            "recent_issues": cand.get('recent_issues', 'N/A'),   # filter_execì—ì„œ ì¶”ê°€ëœë‹¤ê³  ê°€ì •
            "score": cand.get('rerank_score', cand.get('score', 0))
        })

    # ğŸ’¡ [í•µì‹¬] LLM ë©”ì‹œì§€ì— ì‚¬ìš©ì ìš”ì²­ê³¼ í•„í„°ë§ ì¡°ê±´ì„ ëª…ì‹œì ìœ¼ë¡œ í¬í•¨
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": f"""
        [ì‚¬ìš©ì ì›ë³¸ ìš”ì²­]: {user_query}
        [ë¶„ì„ëœ í™œë™ ì¡°ê±´]: {other_conditions or 'ì¡°ê±´ ì—†ìŒ'}
        [ê²€ìƒ‰ëœ í›„ë³´ ë°ì´í„°]: {json.dumps(simplified_candidates, indent=2, ensure_ascii=False)}
        
        ìœ„ ìš”ì²­, í™œë™ ì¡°ê±´, ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬, ê° í”„ë¡œì íŠ¸ì˜ ì¶”ì²œ ì´ìœ ë¥¼ ë‹´ì€ ìµœì¢… JSON ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ì‹­ì‹œì˜¤.
        """}
    ]
    
    try:
        # LLM í˜¸ì¶œ
        response = await llm.ainvoke(messages)
        content = response.content
        
        # ğŸ’¡ [ë¡œê·¸ ì¶”ê°€] LLMì´ ìƒì„±í•œ ì›ë³¸ ì‘ë‹µì„ ë¡œê·¸ë¡œ ì¶œë ¥
        print("\n--- ğŸ¤– LLM Raw Response Log (for Debug) ---")
        print(content)
        print("-------------------------------------------\n")
        
        # JSON íŒŒì‹± ë° ì •ë¦¬ (ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ ì œê±°)
        content = content.strip()
        json_match = re.search(r'\{.*\}', content, re.DOTALL)
        if json_match: content = json_match.group(0)

        json_data = json.loads(content)
        
        # LLMì´ ìƒì„±í•œ ì¶”ì²œ ì´ìœ ë¥¼ ì›ë³¸ ë°ì´í„°ì— ë‹¤ì‹œ ë§¤í•‘
        final_list = []
        reason_map = {c.get('name'): c.get('recommendation_reason') for c in json_data.get('top_candidates', [])}
        
        for cand in top_candidates:
            reason = reason_map.get(cand.get('name'))
            cand_copy = cand.copy()
            
            # Note: ì—¬ê¸°ì„œëŠ” final_listì— cand_copyì˜ ëª¨ë“  í•„ë“œ(ì˜ˆ: recent_commits)ê°€ í¬í•¨ë˜ë„ë¡ í•©ë‹ˆë‹¤.
            # LLMì´ ìƒì„±í•œ reasonì„ ìµœì¢… ë°ì´í„°ì— ë®ì–´ì”ë‹ˆë‹¤.
            if reason:
                cand_copy["recommendation_reason"] = reason
            else:
                cand_copy["recommendation_reason"] = "LLMì´ ì¶”ì²œ ì‚¬ìœ ë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

            final_list.append(cand_copy)
                
        # ìµœì¢… ë³´ê³ ì„œ í˜•íƒœë¡œ JSON ë°˜í™˜
        return json.dumps({
            "summary_reasoning": json_data.get('summary_reasoning', "ìš”ì•½ ì‚¬ìœ  ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."),
            "top_candidates": final_list
        }, ensure_ascii=False)

    except Exception as e:
        logger.error(f"âŒ Final Report Generation Failed in Core: {e}")
        return json.dumps({"error": f"Final Report Generation Failed: {e}"}, ensure_ascii=False)