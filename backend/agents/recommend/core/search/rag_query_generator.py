import json
import logging
from typing import Dict, Any, Literal
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from config.setting import settings

logger = logging.getLogger(__name__)

try:
    llm = ChatOpenAI(
        base_url=settings.llm.api_base,
        api_key=settings.llm.api_key,
        model=settings.llm.model_name,
        temperature=0
    )
except Exception as e:
    logger.error(f"RAG Query Gen LLM Initialization Failed: {e}")
    # ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ í•¨ìˆ˜ê°€ í˜¸ì¶œë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì¬ë°œìƒì‹œí‚µë‹ˆë‹¤.
    raise e

router_prompt= ChatPromptTemplate.from_messages([
        ("system", """
        ë‹¹ì‹ ì€ GitHub RAG ì‹œìŠ¤í…œì„ ìœ„í•œ ì—„ê²©í•œ ê²€ìƒ‰ ì¿¼ë¦¬ ë¶„ì„ê°€ì…ë‹ˆë‹¤.
        ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ `query`(ê²€ìƒ‰ì–´), `keywords`(í•µì‹¬ í‚¤ì›Œë“œ), `filters`(ë©”íƒ€ë°ì´í„° í•„í„°)ë¥¼ ì¶”ì¶œí•˜ì‹­ì‹œì˜¤.

        ### ì…ë ¥ ë°ì´í„°
        - ìš”ì²­: {user_request}
        
        ### ê·œì¹™ (ì—„ê²©íˆ ì¤€ìˆ˜)

        1. **Query (ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ìš©)**: 
           - ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ **ê°„ê²°í•˜ê³  ëª…í™•í•œ ì˜ì–´ ëª…ì‚¬êµ¬(Phrase)**ë¡œ ë³€í™˜í•˜ì‹­ì‹œì˜¤.
           - "I am looking for...", "Can you recommend..." ê°™ì€ **ëŒ€í™”ì²´ ì„œìˆ ì–´ë¥¼ ì œê±°**í•˜ì‹­ì‹œì˜¤.
           - **ì°¾ê³ ì í•˜ëŠ” í”„ë¡œì íŠ¸ì˜ README ì œëª©ì´ë‚˜ í•œ ì¤„ ì„¤ëª…**ê³¼ ìœ ì‚¬í•œ í˜•íƒœë¡œ ë§Œë“œì‹­ì‹œì˜¤.
           - ì˜ˆì‹œ: "PyTorch ê°™ì€ ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬" -> **"Deep learning framework with GPU acceleration similar to PyTorch"**

        2. **Keywords (í‚¤ì›Œë“œ ë§¤ì¹­ìš©)**:
           - ë„ë©”ì¸ì´ë‚˜ íŠ¹ì • ì‘ì—…ì„ ì •ì˜í•˜ëŠ” **1~3ê°œì˜ í•µì‹¬ ëª…ì‚¬**ë¥¼ ì¶”ì¶œí•˜ì‹­ì‹œì˜¤.
           - **í¬í•¨ ëŒ€ìƒ**: "deep learning", "neural network", "autograd", "tensor" ë“±.
           - **ì œì™¸ ëŒ€ìƒ**: "project", "open source", "oss"
           - **ì£¼ì˜**: ì‚¬ìš©ìê°€ 'ëŒ€ì•ˆ(Alternative)'ì„ ì°¾ì„ ë•Œ, ê¸°ì¤€ì´ ë˜ëŠ” ê¸°ìˆ ëª…(ì˜ˆ: PyTorch)ì€ í‚¤ì›Œë“œì—ì„œ **ì œì™¸í•˜ê±°ë‚˜ ì‹ ì¤‘íˆ í¬í•¨**í•˜ì‹­ì‹œì˜¤. (ë‹¤ë¥¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ëª…ì— PyTorchê°€ ì—†ì„ ìˆ˜ë„ ìˆìŒ)

        3. **Filters (ë©”íƒ€ë°ì´í„° ì œì•½ì¡°ê±´) - í™˜ê° ê¸ˆì§€(NO HALLUCINATION)**:
           - **ë§¤ìš° ì¤‘ìš”**: ì‚¬ìš©ìê°€ **ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰í•œ ê²½ìš°ì—ë§Œ** í•„í„°ë¥¼ ì¶”ê°€í•˜ì‹­ì‹œì˜¤.
           - ì‚¬ìš©ìê°€ íŠ¹ì • **í”„ë ˆì„ì›Œí¬, ë¼ì´ë¸ŒëŸ¬ë¦¬, ê¸°ìˆ  ìŠ¤íƒ**ì„ ì–¸ê¸‰í–ˆë‹¤ë©´ `topics` ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•˜ì‹­ì‹œì˜¤.
           
           #### ğŸš¨ [ì¤‘ìš”] ëŒ€ì•ˆ/ìœ ì‚¬ ê²€ìƒ‰ ì‹œ ì˜ˆì™¸ ê·œì¹™:
           - ì‚¬ìš©ìê°€ "**~ê°™ì€ ê²ƒ**", "**~ëŒ€ì•ˆ**", "**~ì™€ ë¹„ìŠ·í•œ**" (Alternative/Similar to)ì„ ìš”ì²­í•œ ê²½ìš°, **ê¸°ì¤€ì´ ë˜ëŠ” ê·¸ ê¸°ìˆ ëª…ì„ `topics` í•„í„°ì— ì ˆëŒ€ ë„£ì§€ ë§ˆì‹­ì‹œì˜¤.**
           - ì´ìœ : í•„í„°ì— ë„£ìœ¼ë©´ ê·¸ ê¸°ìˆ ì´ íƒœê·¸ëœ í”„ë¡œì íŠ¸ë§Œ ê²€ìƒ‰ë˜ì–´, ì •ì‘ ê²½ìŸ í”„ë¡œì íŠ¸(ëŒ€ì•ˆ)ëŠ” ê²€ìƒ‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
           
           #### í•„í„° ì¶”ì¶œ ë¡œì§ ì˜ˆì‹œ:
           - **User**: "PyTorch í”„ë¡œì íŠ¸ ì°¾ì•„ì¤˜" -> **Filters: {{ "topics": ["pytorch"] }}** (PyTorchë¥¼ ì‚¬ìš©í•˜ëŠ” í”„ë¡œì íŠ¸ë¥¼ ì›í•¨ -> í•„í„° ì¶”ê°€ O)
           - **User**: "**PyTorch ê°™ì€** ë‹¤ë¥¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ìˆì–´?" -> **Filters: {{ "topics": [] }}** (PyTorchê°€ ì•„ë‹Œ ë‹¤ë¥¸ê±¸ ì›í•¨ -> í•„í„° ì¶”ê°€ X)
           - **User**: "React ëŒ€ì•ˆ í”„ë ˆì„ì›Œí¬" -> **Filters: {{ "topics": [] }}** (React í•„í„° X, Queryë¡œ ê²€ìƒ‰)

        ### ì¶œë ¥ í˜•ì‹ (JSON Only)
        ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì‹­ì‹œì˜¤.
        {{
            "query": "...",
            "keywords": ["...", "..."], 
            "filters": {{
                "language": "...",
                "min_stars": 0,
                "topics": ["...", "..."]
            }}
        }}
        """),
        ("user", "User Request: {user_request}")
    ])

async def generate_rag_query_and_filters(
    user_request: str,
    category: Literal["semantic_search", "url_analysis"],
    analyzed_data: Dict[str, Any] = None
) -> Dict[str, Any]:
    """
    [í•µì‹¬ ë¡œì§] ì‚¬ìš©ì ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ RAG ê²€ìƒ‰ì— í•„ìš”í•œ ì¿¼ë¦¬, í‚¤ì›Œë“œ, í•„í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    query = user_request # ë¡œê·¸ ì¶œë ¥ì„ ìœ„í•œ ë³€ìˆ˜
    print(f"âš™ï¸ [RAG Query Gen] Analyzing request for vector search: '{query}'")
    
    chain = router_prompt | llm
    
    try:
        # ğŸ’¡ [ìˆ˜ì •] ainvoke ì‚¬ìš© (ë¹„ë™ê¸° í™˜ê²½) ë° ì…ë ¥ ë³€ìˆ˜ë¥¼ {user_request}ë§Œ ì „ë‹¬
        response = await chain.ainvoke({
            "user_request": user_request,
            # 'category', 'summary'ëŠ” promptì— ë³€ìˆ˜ë¡œ ì •ì˜ë˜ì–´ ìˆì§€ ì•Šìœ¼ë¯€ë¡œ ì œê±°í•©ë‹ˆë‹¤.
        })
        
        content = response.content
        
        # ğŸ’¡ [ë¡œê·¸ ì¶”ê°€] LLMì˜ ì›ë³¸ ì‘ë‹µì„ ë””ë²„ê¹…ìš©ìœ¼ë¡œ ì¶œë ¥
        print("\n--- ğŸ¤– LLM Raw Response Log (RAG Query Gen) ---")
        print(content)
        print("--------------------------------------------------\n")
        
        # JSON íŒŒì‹± (ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°)
        content = content.strip()
        if content.startswith("```json"):
            content = content.replace("```json", "").replace("```", "").strip()
            
        result_data = json.loads(content)
        
        logger.info(f"[RAGQueryGen] Extracted Query: {result_data.get('query')}")
        
        return {
            "query": result_data.get("query", user_request),
            "keywords": result_data.get("keywords", []),
            "filters": result_data.get("filters", {})
        }
        
    except Exception as e:
        logger.error(f"[RAGQueryGen] Critical Error during LLM call or parsing: {e}")
        # ì‹¤íŒ¨ ì‹œ Fallback ì¿¼ë¦¬ ë°˜í™˜ (ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©)
        return {"query": user_request, "keywords": [], "filters": {}}