"""
Comparison Agent Graph - í•˜ì´ë¸Œë¦¬ë“œ íŒ¨í„´ (LangGraph + ì•ˆì „í•œ ì˜ˆì™¸ ì²˜ë¦¬)

í–¥ìƒëœ ì—ì´ì „íŠ¸ íë¦„:
validate_input â†’ batch_diagnosis â†’ compare â†’ summarize
       â†“ (ì—ëŸ¬ ì‹œ)        â†“ (ì—ëŸ¬ ì‹œ)    â†“ (ì—ëŸ¬ ì‹œ)
   error_handler     error_handler  error_handler

íŠ¹ì§•:
- ëª¨ë“  ë…¸ë“œì— @safe_node ë°ì½”ë ˆì´í„°ë¡œ ì˜ˆì™¸ ì²˜ë¦¬
- ì¡°ê±´ë¶€ ë¼ìš°íŒ…ìœ¼ë¡œ ì—ëŸ¬ ë°œìƒ ì‹œ ë¹ ë¥¸ ì¢…ë£Œ
- Core scoring í™œìš© (health_score, onboarding_score, levels)
- ë ˆí¬ì§€í† ë¦¬ ë­í‚¹ ë° ì¶”ì²œ
- ëª©ì ë³„ ìµœì  ì €ì¥ì†Œ íŒë‹¨
"""
from typing import Dict, Any, Optional, List, Callable, Literal
from langgraph.graph import StateGraph, END
import logging
from functools import wraps

from backend.agents.comparison.models import ComparisonState, ComparisonOutput
from backend.core.scoring_core import (
    compute_health_level,
    compute_onboarding_level,
    HEALTH_GOOD_THRESHOLD,
    HEALTH_WARNING_THRESHOLD,
    ONBOARDING_EASY_THRESHOLD,
    ONBOARDING_NORMAL_THRESHOLD,
)

logger = logging.getLogger(__name__)


# === ì˜ˆì™¸ ì²˜ë¦¬ ë°ì½”ë ˆì´í„° ===

def safe_node(default_updates: Dict[str, Any] = None):
    """
    ë…¸ë“œ í•¨ìˆ˜ì— ì•ˆì „í•œ ì˜ˆì™¸ ì²˜ë¦¬ë¥¼ ì¶”ê°€í•˜ëŠ” ë°ì½”ë ˆì´í„°
    
    Args:
        default_updates: ì˜ˆì™¸ ë°œìƒ ì‹œ ë°˜í™˜í•  ê¸°ë³¸ ìƒíƒœ ì—…ë°ì´íŠ¸
    """
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def wrapper(state: ComparisonState) -> Dict[str, Any]:
            node_name = func.__name__.replace("_node", "")
            try:
                return await func(state)
            except Exception as e:
                logger.error(f"[Comparison Agent] {node_name} failed: {e}", exc_info=True)
                
                # ê¸°ë³¸ ì—…ë°ì´íŠ¸ ê°’ ì„¤ì •
                updates = default_updates.copy() if default_updates else {}
                updates["error"] = str(e)
                updates["execution_path"] = (state.get("execution_path") or "") + f" â†’ {node_name}(ERROR)"
                
                return updates
        return wrapper
    return decorator


# === ì—ì´ì „íŠ¸ ê²°ì • ë¡œì§ ===

def _analyze_repo_strengths(
    health_score: int,
    onboarding_score: int,
    docs_score: int,
    activity_score: int,
) -> List[str]:
    """ì €ì¥ì†Œì˜ ê°•ì  ë¶„ì„"""
    strengths = []
    
    if health_score >= HEALTH_GOOD_THRESHOLD:
        strengths.append("excellent_maintenance")
    elif health_score >= HEALTH_WARNING_THRESHOLD:
        strengths.append("decent_maintenance")
    
    if onboarding_score >= ONBOARDING_EASY_THRESHOLD:
        strengths.append("beginner_friendly")
    elif onboarding_score >= ONBOARDING_NORMAL_THRESHOLD:
        strengths.append("moderate_learning_curve")
    
    if docs_score >= 70:
        strengths.append("well_documented")
    if activity_score >= 70:
        strengths.append("actively_maintained")
    
    return strengths


def _determine_best_for_purpose(comparison_data: List[Dict[str, Any]]) -> Dict[str, Any]:
    """ëª©ì ë³„ ìµœì  ì €ì¥ì†Œ ê²°ì •"""
    if not comparison_data:
        return {}
    
    recommendations = {
        "for_learning": None,
        "for_contribution": None,
        "for_production_reference": None,
        "overall_winner": None,
    }
    
    # í•™ìŠµìš©: ì˜¨ë³´ë”© ì ìˆ˜ ìµœê³ 
    learning_sorted = sorted(comparison_data, key=lambda x: x.get("onboarding_score", 0), reverse=True)
    if learning_sorted:
        recommendations["for_learning"] = {
            "repo": learning_sorted[0]["repo"],
            "reason": f"ì˜¨ë³´ë”© ì ìˆ˜ {learning_sorted[0].get('onboarding_score', 0)}ì ìœ¼ë¡œ ê°€ì¥ í•™ìŠµí•˜ê¸° ì¢‹ìŒ"
        }
    
    # ê¸°ì—¬ìš©: í™œë™ì„± + ë¬¸ì„œí™” ê· í˜•
    contribution_sorted = sorted(
        comparison_data, 
        key=lambda x: x.get("activity_score", 0) * 0.6 + x.get("docs_score", 0) * 0.4, 
        reverse=True
    )
    if contribution_sorted:
        recommendations["for_contribution"] = {
            "repo": contribution_sorted[0]["repo"],
            "reason": f"í™œë°œí•œ ìœ ì§€ë³´ìˆ˜ì™€ ë¬¸ì„œí™”ë¡œ ê¸°ì—¬í•˜ê¸° ì¢‹ìŒ"
        }
    
    # í”„ë¡œë•ì…˜ ì°¸ê³ ìš©: ì „ì²´ ê±´ê°•ë„ ìµœê³ 
    health_sorted = sorted(comparison_data, key=lambda x: x.get("health_score", 0), reverse=True)
    if health_sorted:
        recommendations["for_production_reference"] = {
            "repo": health_sorted[0]["repo"],
            "reason": f"ê±´ê°•ë„ ì ìˆ˜ {health_sorted[0].get('health_score', 0)}ì ìœ¼ë¡œ ê°€ì¥ ì•ˆì •ì "
        }
    
    # ì „ì²´ ìš°ìŠ¹ì: ì¢…í•© ì ìˆ˜
    overall_sorted = sorted(
        comparison_data,
        key=lambda x: x.get("health_score", 0) * 0.5 + x.get("onboarding_score", 0) * 0.5,
        reverse=True
    )
    if overall_sorted:
        recommendations["overall_winner"] = {
            "repo": overall_sorted[0]["repo"],
            "health_score": overall_sorted[0].get("health_score", 0),
            "onboarding_score": overall_sorted[0].get("onboarding_score", 0),
        }
    
    return recommendations


# === ë…¸ë“œ í•¨ìˆ˜ë“¤ (ì•ˆì „í•œ ì˜ˆì™¸ ì²˜ë¦¬ í¬í•¨) ===

@safe_node(default_updates={"validated_repos": [], "warnings": []})
async def validate_input_node(state: ComparisonState) -> Dict[str, Any]:
    """ì…ë ¥ ê²€ì¦ - ìµœì†Œ 2ê°œ ì €ì¥ì†Œ í•„ìš”"""
    logger.info(f"[Comparison Agent] Validating input: {len(state.get('repos', []))} repos")
    
    repos = state.get("repos", [])
    warnings = []
    validated_repos = []
    
    if len(repos) < 2:
        return {
            "error": "ë¹„êµ ë¶„ì„ì—ëŠ” ìµœì†Œ 2ê°œì˜ ì €ì¥ì†Œê°€ í•„ìš”í•©ë‹ˆë‹¤.",
            "warnings": ["ë¹„êµ ë¶„ì„ì—ëŠ” ìµœì†Œ 2ê°œì˜ ì €ì¥ì†Œê°€ í•„ìš”í•©ë‹ˆë‹¤."],
            "validated_repos": [],
            "execution_path": "comparison_graph:validate_input(ERROR)"
        }
    
    # ì €ì¥ì†Œ í˜•ì‹ ê²€ì¦
    for repo in repos:
        if "/" in repo:
            validated_repos.append(repo)
        else:
            warnings.append(f"ì˜ëª»ëœ ì €ì¥ì†Œ í˜•ì‹: {repo}")
    
    if len(validated_repos) < 2:
        return {
            "error": "ìœ íš¨í•œ ì €ì¥ì†Œê°€ 2ê°œ ì´ìƒ í•„ìš”í•©ë‹ˆë‹¤.",
            "warnings": warnings,
            "validated_repos": validated_repos,
            "execution_path": "comparison_graph:validate_input(ERROR)"
        }
    
    logger.info(f"[Comparison Agent] Validated {len(validated_repos)} repos")
    
    return {
        "validated_repos": validated_repos,
        "warnings": warnings,
        "execution_path": "comparison_graph:validate_input",
        "error": None
    }


@safe_node(default_updates={"batch_results": {}, "cache_hits": [], "cache_misses": []})
async def batch_diagnosis_node(state: ComparisonState) -> Dict[str, Any]:
    """ì—¬ëŸ¬ ì €ì¥ì†Œë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ë¶„ì„ (ìºì‹œ í™œìš©)"""
    logger.info("[Comparison Agent] Running batch diagnosis")
    
    # ì—ëŸ¬ê°€ ìˆìœ¼ë©´ ìŠ¤í‚µ
    if state.get("error"):
        return {}
    
    from backend.agents.comparison.nodes import batch_diagnosis
    
    validated_repos = state.get("validated_repos", [])
    ref = state.get("ref", "main")
    use_cache = state.get("use_cache", True)
    
    # batch_diagnosisëŠ” ì´ì œ ë¹„ë™ê¸° í•¨ìˆ˜
    batch_result = await batch_diagnosis(
        repos=validated_repos,
        ref=ref,
        use_cache=use_cache,
    )
    
    logger.info(
        f"[Comparison Agent] Batch diagnosis complete: {len(batch_result.get('results', {}))} results, "
        f"cache_hits={len(batch_result.get('cache_hits', []))}, "
        f"cache_misses={len(batch_result.get('cache_misses', []))}"
    )
    
    return {
        "batch_results": batch_result.get("results", {}),
        "cache_hits": batch_result.get("cache_hits", []),
        "cache_misses": batch_result.get("cache_misses", []),
        "warnings": (state.get("warnings") or []) + batch_result.get("warnings", []),
        "execution_path": state.get("execution_path", "") + " â†’ batch_diagnosis"
    }


@safe_node(default_updates={"comparison_data": [], "agent_analysis": {}})
async def compare_node(state: ComparisonState) -> Dict[str, Any]:
    """Core scoringì„ í™œìš©í•œ ë¹„êµ ë°ì´í„° ì¤€ë¹„ ë° ë¶„ì„"""
    logger.info("[Comparison Agent] Analyzing and comparing results")
    
    # ì—ëŸ¬ê°€ ìˆìœ¼ë©´ ìŠ¤í‚µ
    if state.get("error"):
        return {}
    
    batch_results = state.get("batch_results", {})
    
    if not batch_results or len(batch_results) < 2:
        return {
            "error": "ë¹„êµí•  ê²°ê³¼ê°€ 2ê°œ ì´ìƒ í•„ìš”í•©ë‹ˆë‹¤.",
            "comparison_data": [],
            "execution_path": state.get("execution_path", "") + " â†’ compare(ERROR)"
        }
    
    # ë¹„êµ ë°ì´í„° ì¤€ë¹„ ë° ê°•ì  ë¶„ì„
    comparison_data = []
    for repo_str, data in batch_results.items():
        health_score = data.get("health_score", 0)
        if health_score == 0 and data.get("health_level") == "unknown":
            logger.warning(f"[Comparison Agent] Skipping invalid result for {repo_str}")
            continue
        
        onboarding_score = data.get("onboarding_score", 0)
        docs_score = data.get("documentation_quality", data.get("docs", {}).get("total_score", 0))
        activity_score = data.get("activity_maintainability", data.get("activity", {}).get("total_score", 0))
        
        # Core scoring ë ˆë²¨ ì¬ê³„ì‚° (ì¼ê´€ì„± ë³´ì¥)
        health_level = compute_health_level(health_score)
        onboarding_level = compute_onboarding_level(onboarding_score)
        
        # ê°•ì  ë¶„ì„
        strengths = _analyze_repo_strengths(health_score, onboarding_score, docs_score, activity_score)
        
        comparison_data.append({
            "repo": repo_str,
            "health_score": health_score,
            "onboarding_score": onboarding_score,
            "docs_score": docs_score,
            "activity_score": activity_score,
            "health_level": health_level,
            "onboarding_level": onboarding_level,
            "strengths": strengths,
        })
    
    # ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    comparison_data.sort(key=lambda x: x["health_score"], reverse=True)
    
    # ëª©ì ë³„ ì¶”ì²œ ê²°ì •
    recommendations = _determine_best_for_purpose(comparison_data)
    
    logger.info(f"[Comparison Agent] Analyzed {len(comparison_data)} repos, overall winner: {recommendations.get('overall_winner', {}).get('repo', 'N/A')}")
    
    # ì—ì´ì „íŠ¸ ë¶„ì„ ê²°ê³¼
    agent_analysis = {
        "comparison_data": comparison_data,
        "recommendations": recommendations,
        "total_repos_compared": len(comparison_data),
        "reasoning": f"Compared {len(comparison_data)} repositories based on health_score and onboarding_score",
    }
    
    return {
        "comparison_data": comparison_data,
        "agent_analysis": agent_analysis,
        "error": None,
        "execution_path": state.get("execution_path", "") + " â†’ compare"
    }


@safe_node(default_updates={"comparison_summary": "", "result": {}})
async def summarize_node(state: ComparisonState) -> Dict[str, Any]:
    """LLMì„ ì‚¬ìš©í•˜ì—¬ ë¹„êµ ë¶„ì„ ìš”ì•½ ìƒì„± - ì—ì´ì „íŠ¸ ì¶”ì²œ í¬í•¨"""
    logger.info("[Comparison Agent] Generating comparison summary with recommendations")
    
    # ì—ëŸ¬ê°€ ìˆìœ¼ë©´ ì—ëŸ¬ ê²°ê³¼ ë°˜í™˜
    if state.get("error"):
        return {
            "result": ComparisonOutput(
                warnings=state.get("warnings", []) + [state.get("error", "")]
            ).dict(),
            "execution_path": state.get("execution_path", "") + " â†’ summarize(ERROR)"
        }
    
    from backend.agents.comparison.nodes import compare_results
    
    batch_results = state.get("batch_results", {})
    agent_analysis = state.get("agent_analysis", {})
    
    # compare_results í˜¸ì¶œí•˜ì—¬ LLM ìš”ì•½ ìƒì„± (ë¹„ë™ê¸°)
    comparison_result = await compare_results(batch_results)
    summary = comparison_result.get("summary", "")
    
    # ì—ì´ì „íŠ¸ ì¶”ì²œ ì •ë³´ ì¶”ê°€
    recommendations = agent_analysis.get("recommendations", {})
    if recommendations:
        rec_section = "\n\nğŸ“Š **ì—ì´ì „íŠ¸ ì¶”ì²œ**:\n"
        
        if recommendations.get("for_learning"):
            rec = recommendations["for_learning"]
            rec_section += f"- ğŸ“ **í•™ìŠµìš©**: `{rec['repo']}` - {rec['reason']}\n"
        
        if recommendations.get("for_contribution"):
            rec = recommendations["for_contribution"]
            rec_section += f"- ğŸ¤ **ê¸°ì—¬ìš©**: `{rec['repo']}` - {rec['reason']}\n"
        
        if recommendations.get("for_production_reference"):
            rec = recommendations["for_production_reference"]
            rec_section += f"- ğŸ­ **ì°¸ê³ ìš©**: `{rec['repo']}` - {rec['reason']}\n"
        
        if recommendations.get("overall_winner"):
            winner = recommendations["overall_winner"]
            rec_section += f"\nğŸ† **ì¢…í•© 1ìœ„**: `{winner['repo']}` (ê±´ê°•ë„: {winner['health_score']}ì , ì˜¨ë³´ë”©: {winner['onboarding_score']}ì )"
        
        summary += rec_section
    
    logger.info("[Comparison Agent] Comparison summary generated with agent recommendations")
    
    # ìµœì¢… ê²°ê³¼ ì¡°ë¦½ - ì—ì´ì „íŠ¸ ë¶„ì„ ê²°ê³¼ í¬í•¨
    result = ComparisonOutput(
        results=batch_results,
        comparison_summary=summary,
        warnings=state.get("warnings", []),
        cache_hits=state.get("cache_hits", []),
        cache_misses=state.get("cache_misses", []),
    )
    
    result_dict = result.dict()
    result_dict["agent_analysis"] = agent_analysis
    
    return {
        "comparison_summary": summary,
        "result": result_dict,
        "execution_path": state.get("execution_path", "") + " â†’ summarize"
    }


# === ì—ëŸ¬ í•¸ë“¤ëŸ¬ ë…¸ë“œ ===

async def error_handler_node(state: ComparisonState) -> Dict[str, Any]:
    """ì—ëŸ¬ ë°œìƒ ì‹œ ì•ˆì „í•œ ê²°ê³¼ ë°˜í™˜"""
    logger.warning(f"[Comparison Agent] Error handler triggered: {state.get('error')}")
    
    error_msg = state.get("error", "Unknown error occurred")
    warnings = state.get("warnings", []) + [error_msg]
    
    # ì—ëŸ¬ ê²°ê³¼ ìƒì„±
    result = ComparisonOutput(
        results=state.get("batch_results", {}),
        comparison_summary=f"ë¹„êµ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg}",
        warnings=warnings,
        cache_hits=state.get("cache_hits", []),
        cache_misses=state.get("cache_misses", []),
    )
    
    return {
        "result": result.dict(),
        "execution_path": (state.get("execution_path") or "") + " â†’ error_handler"
    }


# === ì¡°ê±´ë¶€ ë¼ìš°íŒ… (í•˜ì´ë¸Œë¦¬ë“œ íŒ¨í„´ í•µì‹¬) ===

def check_error_after_validate(state: ComparisonState) -> Literal["continue", "error_handler"]:
    """validate_input í›„ ì—ëŸ¬ ì²´í¬"""
    if state.get("error"):
        return "error_handler"
    return "continue"


def check_error_after_batch(state: ComparisonState) -> Literal["continue", "error_handler"]:
    """batch_diagnosis í›„ ì—ëŸ¬ ì²´í¬"""
    if state.get("error"):
        return "error_handler"
    return "continue"


def check_error_after_compare(state: ComparisonState) -> Literal["continue", "error_handler"]:
    """compare í›„ ì—ëŸ¬ ì²´í¬"""
    if state.get("error"):
        return "error_handler"
    return "continue"


# === ê·¸ë˜í”„ ë¹Œë“œ (í•˜ì´ë¸Œë¦¬ë“œ íŒ¨í„´) ===

def build_comparison_graph():
    """
    Comparison StateGraph ë¹Œë“œ (í•˜ì´ë¸Œë¦¬ë“œ íŒ¨í„´)
    
    íë¦„: 
    validate_input â†’ [check] â†’ batch_diagnosis â†’ [check] â†’ compare â†’ [check] â†’ summarize
         â†“ (ì—ëŸ¬)              â†“ (ì—ëŸ¬)               â†“ (ì—ëŸ¬)
    error_handler â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†
    
    íŠ¹ì§•:
    - ëª¨ë“  ë…¸ë“œì— @safe_node ë°ì½”ë ˆì´í„°ë¡œ ì˜ˆì™¸ ì²˜ë¦¬
    - ê° ì£¼ìš” ë…¸ë“œ í›„ ì—ëŸ¬ ì²´í¬ â†’ ë¹ ë¥¸ ì¢…ë£Œ (LangGraph ì¥ì  í™œìš©)
    - compare ë…¸ë“œê°€ Core scoringì„ í™œìš©í•˜ì—¬ ë…ë¦½ì ì¸ ë¶„ì„ ë° ì¶”ì²œ ìˆ˜í–‰
    """
    
    graph = StateGraph(ComparisonState)
    
    # ë…¸ë“œ ì¶”ê°€
    graph.add_node("validate_input", validate_input_node)
    graph.add_node("batch_diagnosis", batch_diagnosis_node)
    graph.add_node("compare", compare_node)
    graph.add_node("summarize", summarize_node)
    graph.add_node("error_handler", error_handler_node)  # ì—ëŸ¬ í•¸ë“¤ëŸ¬ ì¶”ê°€
    
    # ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸
    graph.set_entry_point("validate_input")
    
    # validate_input í›„ ì¡°ê±´ë¶€ ë¶„ê¸°
    graph.add_conditional_edges(
        "validate_input",
        check_error_after_validate,
        {
            "continue": "batch_diagnosis",
            "error_handler": "error_handler"
        }
    )
    
    # batch_diagnosis í›„ ì¡°ê±´ë¶€ ë¶„ê¸°
    graph.add_conditional_edges(
        "batch_diagnosis",
        check_error_after_batch,
        {
            "continue": "compare",
            "error_handler": "error_handler"
        }
    )
    
    # compare í›„ ì¡°ê±´ë¶€ ë¶„ê¸°
    graph.add_conditional_edges(
        "compare",
        check_error_after_compare,
        {
            "continue": "summarize",
            "error_handler": "error_handler"
        }
    )
    
    # ì¢…ë£Œ ì—£ì§€
    graph.add_edge("summarize", END)
    graph.add_edge("error_handler", END)
    
    return graph.compile()


# === ì‹±ê¸€í†¤ ê·¸ë˜í”„ ===
_comparison_graph = None


def get_comparison_graph():
    """Comparison Graph ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤"""
    global _comparison_graph
    if _comparison_graph is None:
        _comparison_graph = build_comparison_graph()
        logger.info("Comparison Graph initialized (hybrid pattern with error handling)")
    return _comparison_graph


# === í¸ì˜ í•¨ìˆ˜ ===

async def run_comparison_graph(
    repos: list,
    ref: str = "main",
    use_cache: bool = True,
    user_message: Optional[str] = None
) -> Dict[str, Any]:
    """
    Comparison Graph ì‹¤í–‰
    
    Args:
        repos: ë¹„êµí•  ì €ì¥ì†Œ ëª©ë¡ ["owner/repo", ...]
        ref: ë¶„ì„í•  ë¸Œëœì¹˜/íƒœê·¸
        use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€
        user_message: ì‚¬ìš©ì ë©”ì‹œì§€ (ìˆìœ¼ë©´)
    
    Returns:
        ComparisonOutput dict with agent_analysis
    """
    graph = get_comparison_graph()
    
    initial_state: ComparisonState = {
        "repos": repos,
        "ref": ref,
        "use_cache": use_cache,
        "user_message": user_message,
        "validated_repos": None,
        "batch_results": None,
        "comparison_data": None,
        # ì—ì´ì „íŠ¸ ë¶„ì„ í•„ë“œ ì´ˆê¸°í™”
        "agent_analysis": None,
        # ìºì‹œ ê´€ë ¨
        "cache_hits": None,
        "cache_misses": None,
        "warnings": None,
        # ê²°ê³¼ í•„ë“œ
        "comparison_summary": None,
        "result": None,
        "error": None,
        "execution_path": None
    }
    
    try:
        final_state = await graph.ainvoke(initial_state)
        return final_state.get("result", {})
    except Exception as e:
        logger.error(f"[Comparison Agent] Graph execution failed: {e}", exc_info=True)
        # ìµœìƒìœ„ ì˜ˆì™¸ ì²˜ë¦¬ - ì•ˆì „í•œ ê²°ê³¼ ë°˜í™˜
        return ComparisonOutput(
            results={},
            comparison_summary=f"ë¹„êµ ê·¸ë˜í”„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}",
            warnings=[str(e)]
        ).dict()
