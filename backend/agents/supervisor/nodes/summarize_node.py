from __future__ import annotations

import logging
from typing import Any

from backend.llm.base import ChatMessage, ChatRequest
from backend.llm.factory import fetch_llm_client

from ..models import SupervisorState

logger = logging.getLogger(__name__)

SUMMARIZE_SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ì˜¤í”ˆì†ŒìŠ¤ í”„ë¡œì íŠ¸ ë¶„ì„ ê²°ê³¼ë¥¼ ìš”ì•½í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì§„ë‹¨ ê²°ê³¼, ë³´ì•ˆ ë¶„ì„, ì¶”ì²œ ì •ë³´ ë“±ì„ ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.

ë‹¤ìŒ ì›ì¹™ì„ ë”°ë¥´ì„¸ìš”:
1. í•µì‹¬ ì •ë³´ë¥¼ ê°„ê²°í•˜ê²Œ ì „ë‹¬
2. ìˆ˜ì¹˜ê°€ ìˆìœ¼ë©´ ëª…í™•íˆ ì–¸ê¸‰
3. ì‚¬ìš©ìì˜ ì›ë˜ ì§ˆë¬¸ì— ë§ëŠ” ë‹µë³€ ì œê³µ
4. ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ì‚¬ìš© ê°€ëŠ¥
5. ë‹µë³€ ë§ˆì§€ë§‰ì— ì‚¬ìš©ìê°€ ë‹¤ìŒì— í•  ìˆ˜ ìˆëŠ” í–‰ë™ì„ ì¹œì ˆí•˜ê²Œ ì œì•ˆ
6. ì´ëª¨ì§€ë‚˜ ì´ëª¨í‹°ì½˜ì€ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš” (ì˜ˆ: ğŸ’¡, âœ…, ğŸ‰ ë“± ê¸ˆì§€)

## ë‹¤ìŒ í–‰ë™ ì œì•ˆ ì˜ˆì‹œ
ë‹µë³€ ë§ˆì§€ë§‰ì— ì•„ë˜ì™€ ê°™ì´ ì‚¬ìš©ìê°€ ì„ íƒí•  ìˆ˜ ìˆëŠ” í›„ì† ì§ˆë¬¸ì„ 2~3ê°œ ì œì•ˆí•˜ì„¸ìš”:

---
**ë‹¤ìŒìœ¼ë¡œ ì´ëŸ° ê²ƒë„ í•´ë“œë¦´ ìˆ˜ ìˆì–´ìš”:**
- "ì´ ì €ì¥ì†Œì—ì„œ ì´ˆë³´ìê°€ ì‹œì‘í•˜ê¸° ì¢‹ì€ ì´ìŠˆë¥¼ ì°¾ì•„ì¤˜"
- "ë¹„ìŠ·í•œ ë‹¤ë¥¸ ì €ì¥ì†Œì™€ ë¹„êµí•´ì¤˜"
- "ì˜¨ë³´ë”© í•™ìŠµ ê³„íšì„ ì„¸ì›Œì¤˜"
---

ì œì•ˆì€ ë¶„ì„ ê²°ê³¼ì™€ ì‚¬ìš©ì ë§¥ë½ì— ë§ê²Œ ì¡°ì •í•˜ì„¸ìš”.
"""


def summarize_node(state: SupervisorState) -> SupervisorState:
    """
    ëª¨ë“  Agent ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ìµœì¢… ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    diagnosis_result = state.get("diagnosis_result")
    security_result = state.get("security_result")
    recommend_result = state.get("recommend_result")
    history = state.get("history", [])

    # ë§ˆì§€ë§‰ ì‚¬ìš©ì ì§ˆë¬¸ ì¶”ì¶œ
    user_query = ""
    for turn in reversed(history):
        if turn.get("role") == "user":
            user_query = turn.get("content", "")
            break

    # ê²°ê³¼ ì¡°í•©
    context_parts = []

    if diagnosis_result:
        context_parts.append(f"## ì§„ë‹¨ ê²°ê³¼\n{_format_diagnosis(diagnosis_result)}")

    if security_result:
        context_parts.append(f"## ë³´ì•ˆ ë¶„ì„\n{_format_result(security_result)}")

    if recommend_result:
        context_parts.append(f"## ì¶”ì²œ ì •ë³´\n{_format_result(recommend_result)}")

    if not context_parts:
        summary = "ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
    else:
        context = "\n\n".join(context_parts)
        summary = _generate_summary_with_llm(user_query, context)

    logger.debug("[summarize_node] summary_length=%d", len(summary))

    new_state: SupervisorState = dict(state)  # type: ignore[assignment]

    # historyì— assistant ì‘ë‹µ ì¶”ê°€
    new_history = list(history)
    new_history.append({"role": "assistant", "content": summary})
    new_state["history"] = new_history
    new_state["llm_summary"] = summary

    return new_state


def _format_diagnosis(result: Any) -> str:
    """ì§„ë‹¨ ê²°ê³¼ë¥¼ ë¬¸ìì—´ë¡œ í¬ë§·íŒ…"""
    if isinstance(result, dict):
        parts = []
        if "health_score" in result:
            parts.append(f"- ê±´ê°• ì ìˆ˜: {result['health_score']}")
        if "grade" in result:
            parts.append(f"- ë“±ê¸‰: {result['grade']}")
        if "summary" in result:
            parts.append(f"- ìš”ì•½: {result['summary']}")
        if parts:
            return "\n".join(parts)
    return str(result)


def _format_result(result: Any) -> str:
    """ì¼ë°˜ ê²°ê³¼ë¥¼ ë¬¸ìì—´ë¡œ í¬ë§·íŒ…"""
    if isinstance(result, dict):
        import json
        return json.dumps(result, ensure_ascii=False, indent=2)
    return str(result)


def _generate_summary_with_llm(user_query: str, context: str) -> str:
    """LLMì„ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ìš”ì•½ ìƒì„±"""
    import os
    try:
        llm_client = fetch_llm_client()
        model_name = os.getenv("LLM_MODEL_NAME", "gpt-4o-mini")

        user_message = f"""
ì‚¬ìš©ì ì§ˆë¬¸: {user_query}

ë¶„ì„ ê²°ê³¼:
{context}

ìœ„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.
"""

        request = ChatRequest(
            model=model_name,
            messages=[
                ChatMessage(role="system", content=SUMMARIZE_SYSTEM_PROMPT),
                ChatMessage(role="user", content=user_message),
            ],
            temperature=0.7,
        )

        response = llm_client.chat(request)
        return response.content

    except Exception as e:
        logger.error("[summarize_node] LLM í˜¸ì¶œ ì‹¤íŒ¨: %s", e)
        return f"ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
