"""Supervisor V1 í…ŒìŠ¤íŠ¸: ê¸°ë³¸ ë¼ìš°íŒ… ë° ì‘ë‹µ ìƒì„± ê²€ì¦."""
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent.absolute()))

import pytest


class TestIntentConfig:
    """Intent ì„¤ì • í…ŒìŠ¤íŠ¸."""
    
    def test_v1_supported_intents(self):
        """V1 ì§€ì› Intent ê²€ì¦."""
        from backend.agents.supervisor.intent_config import is_v1_supported
        
        # V1 ì§€ì›
        assert is_v1_supported("analyze", "health")
        assert is_v1_supported("analyze", "onboarding")
        assert is_v1_supported("analyze", "compare")
        assert is_v1_supported("analyze", "onepager")
        assert is_v1_supported("followup", "explain")
        assert is_v1_supported("followup", "evidence")
        assert is_v1_supported("followup", "refine")
        assert is_v1_supported("general_qa", "chat")
        assert is_v1_supported("smalltalk", "greeting")
        
        # V1 ë¯¸ì§€ì›
        assert not is_v1_supported("unknown", "unknown")
    
    def test_intent_meta(self):
        """Intent ë©”íƒ€ë°ì´í„° ê²€ì¦."""
        from backend.agents.supervisor.intent_config import get_intent_meta
        
        # analyze/health: repo í•„ìˆ˜, diagnosis ì‹¤í–‰
        meta = get_intent_meta("analyze", "health")
        assert meta["requires_repo"] is True
        assert meta["runs_diagnosis"] is True
        
        # general_qa/chat: repo ë¶ˆí•„ìš”, diagnosis ë¶ˆí•„ìš”
        meta = get_intent_meta("general_qa", "chat")
        assert meta["requires_repo"] is False
        assert meta["runs_diagnosis"] is False
    
    def test_validate_intent(self):
        """Intent ìœ íš¨ì„± ê²€ì‚¬."""
        from backend.agents.supervisor.intent_config import validate_intent, validate_sub_intent
        
        assert validate_intent("analyze") == "analyze"
        assert validate_intent("invalid") == "analyze"  # default
        assert validate_intent(None) == "analyze"
        
        assert validate_sub_intent("health") == "health"
        assert validate_sub_intent("invalid") == "health"  # default


class TestIntentClassifier:
    """Intent ë¶„ë¥˜ ë…¸ë“œ í…ŒìŠ¤íŠ¸."""
    
    def test_greeting_classification(self):
        """ì¸ì‚¬ ë¶„ë¥˜."""
        from backend.agents.supervisor.nodes.intent_classifier import _tier1_heuristic
        
        result = _tier1_heuristic("ì•ˆë…•")
        assert result is not None
        assert result.intent == "smalltalk"
        assert result.sub_intent == "greeting"
    
    def test_help_classification(self):
        """ë„ì›€ë§ ë¶„ë¥˜."""
        from backend.agents.supervisor.nodes.intent_classifier import _tier1_heuristic
        
        result = _tier1_heuristic("ë­˜ í•  ìˆ˜ ìˆì–´?")
        assert result is not None
        assert result.intent == "help"
    
    def test_repo_extraction(self):
        """ì €ì¥ì†Œ ì¶”ì¶œ."""
        from backend.agents.supervisor.nodes.intent_classifier import _extract_repo
        
        # URL í˜•ì‹
        repo = _extract_repo("https://github.com/facebook/react ë¶„ì„í•´ì¤˜")
        assert repo is not None
        assert repo["owner"] == "facebook"
        assert repo["name"] == "react"
        
        # owner/repo í˜•ì‹
        repo = _extract_repo("vuejs/vue ë¶„ì„í•´ì¤˜")
        assert repo is not None
        assert repo["owner"] == "vuejs"
        assert repo["name"] == "vue"


class TestGraph:
    """Graph ë¼ìš°íŒ… í…ŒìŠ¤íŠ¸."""
    
    def test_should_run_diagnosis(self):
        """ì§„ë‹¨ ì‹¤í–‰ ì¡°ê±´ í…ŒìŠ¤íŠ¸."""
        from backend.agents.supervisor.graph import should_run_diagnosis
        
        # analyze + repo â†’ diagnosis
        state = {
            "intent": "analyze",
            "sub_intent": "health",
            "repo": {"owner": "test", "name": "repo", "url": ""},
        }
        assert should_run_diagnosis(state) == "diagnosis"
        
        # general_qa â†’ summarize
        state = {
            "intent": "general_qa",
            "sub_intent": "chat",
        }
        assert should_run_diagnosis(state) == "summarize"
        
        # error_message ìˆìŒ â†’ summarize
        state = {
            "intent": "analyze",
            "sub_intent": "health",
            "error_message": "ì˜¤ë¥˜ ë°œìƒ",
        }
        assert should_run_diagnosis(state) == "summarize"
    
    def test_graph_invocation_greeting(self):
        """Graph ì‹¤í–‰: ì¸ì‚¬."""
        from backend.agents.supervisor import get_supervisor_graph, build_initial_state
        
        state = build_initial_state("ì•ˆë…•!")
        graph = get_supervisor_graph()
        result = graph.invoke(state)
        
        assert result.get("intent") == "smalltalk"
        assert result.get("answer_kind") == "greeting"
        assert "ì•ˆë…•" in result.get("llm_summary", "").lower() or "ODOC" in result.get("llm_summary", "")
    
    def test_graph_invocation_general_qa(self):
        """Graph ì‹¤í–‰: ì¼ë°˜ QA."""
        from backend.agents.supervisor import get_supervisor_graph, build_initial_state
        
        state = build_initial_state("Health Scoreê°€ ë­ì•¼?")
        graph = get_supervisor_graph()
        result = graph.invoke(state)
        
        assert result.get("intent") == "general_qa"
        assert result.get("answer_kind") == "chat"
        assert result.get("llm_summary")


class TestSummarizeNode:
    """Summarize ë…¸ë“œ í…ŒìŠ¤íŠ¸."""
    
    def test_extract_target_metrics(self):
        """ì§€í‘œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸."""
        from backend.agents.supervisor.nodes.summarize_node import _extract_target_metrics
        
        metrics = _extract_target_metrics("health scoreê°€ ë­ì•¼?")
        assert "health_score" in metrics
        
        metrics = _extract_target_metrics("ì˜¨ë³´ë”© ì ìˆ˜ ì„¤ëª…í•´ì¤˜")
        assert "onboarding_score" in metrics
    
    def test_generate_last_brief(self):
        """ìš”ì•½ ìƒì„± í…ŒìŠ¤íŠ¸."""
        from backend.agents.supervisor.nodes.summarize_node import _generate_last_brief
        
        summary = "# ë¶„ì„ ê²°ê³¼\n\nì´ ì €ì¥ì†ŒëŠ” ê±´ê°•í•©ë‹ˆë‹¤."
        brief = _generate_last_brief(summary, "facebook/react")
        
        assert len(brief) <= 200
        assert "ì €ì¥ì†Œ" in brief or "ê±´ê°•" in brief


class TestRunnerOutputNormalization:
    """ëŸ¬ë„ˆ ì¶œë ¥ ì •ê·œí™” í…ŒìŠ¤íŠ¸."""
    
    def test_safe_get_none(self):
        """safe_get: None ì²˜ë¦¬."""
        from backend.agents.shared.contracts import safe_get
        
        assert safe_get(None, "key") is None
        assert safe_get(None, "key", "default") == "default"
    
    def test_safe_get_non_dict(self):
        """safe_get: dictê°€ ì•„ë‹Œ ê°’ ì²˜ë¦¬."""
        from backend.agents.shared.contracts import safe_get
        
        assert safe_get("string", "key") is None
        assert safe_get(123, "key", "default") == "default"
        assert safe_get([], "key", "default") == "default"
    
    def test_safe_get_dict(self):
        """safe_get: ì •ìƒ dict ì²˜ë¦¬."""
        from backend.agents.shared.contracts import safe_get
        
        d = {"a": 1, "b": {"c": 2}}
        assert safe_get(d, "a") == 1
        assert safe_get(d, "b") == {"c": 2}
        assert safe_get(d, "missing") is None
        assert safe_get(d, "missing", "default") == "default"
    
    def test_safe_get_nested(self):
        """safe_get_nested: ì¤‘ì²© ì ‘ê·¼."""
        from backend.agents.shared.contracts import safe_get_nested
        
        d = {"a": {"b": {"c": 3}}}
        assert safe_get_nested(d, "a", "b", "c") == 3
        assert safe_get_nested(d, "a", "b", "missing") is None
        assert safe_get_nested(d, "a", "b", "missing", default=0) == 0
        assert safe_get_nested(None, "a", "b") is None
    
    def test_normalize_none(self):
        """normalize_runner_output: None â†’ ë¹ˆ ì„±ê³µ."""
        from backend.agents.shared.contracts import normalize_runner_output, RunnerStatus
        
        output = normalize_runner_output(None)
        assert output.status == RunnerStatus.SUCCESS
        assert output.result == {}
    
    def test_normalize_dict(self):
        """normalize_runner_output: dict â†’ RunnerOutput."""
        from backend.agents.shared.contracts import normalize_runner_output, RunnerStatus
        
        # ì¼ë°˜ dict
        output = normalize_runner_output({"scores": {"health": 80}})
        assert output.status == RunnerStatus.SUCCESS
        assert output.result == {"scores": {"health": 80}}
        
        # ì—ëŸ¬ í‘œì‹œ dict
        output = normalize_runner_output({"error_message": "ì‹¤íŒ¨"})
        assert output.status == RunnerStatus.ERROR
        assert output.error_message == "ì‹¤íŒ¨"
    
    def test_normalize_runner_output_passthrough(self):
        """normalize_runner_output: RunnerOutputì€ ê·¸ëŒ€ë¡œ ë°˜í™˜."""
        from backend.agents.shared.contracts import normalize_runner_output, RunnerOutput, RunnerStatus
        
        original = RunnerOutput.success(result={"test": 1})
        output = normalize_runner_output(original)
        assert output is original
    
    def test_normalize_exception(self):
        """normalize_runner_output: Exception â†’ ì—ëŸ¬."""
        from backend.agents.shared.contracts import normalize_runner_output, RunnerStatus
        
        output = normalize_runner_output(ValueError("í…ŒìŠ¤íŠ¸ ì—ëŸ¬"))
        assert output.status == RunnerStatus.ERROR
        assert "í…ŒìŠ¤íŠ¸ ì—ëŸ¬" in output.error_message
    
    def test_validate_runner_output(self):
        """validate_runner_output: ê³„ì•½ ê²€ì¦."""
        from backend.agents.shared.contracts import (
            validate_runner_output, RunnerOutput, RunnerStatus, ContractViolation
        )
        
        # ìœ íš¨í•œ ì¶œë ¥
        valid = RunnerOutput.success(result={"data": 1})
        assert validate_runner_output(valid) is True
        
        # ERROR ìƒíƒœì¸ë° error_message ì—†ìŒ (strict=False)
        invalid = RunnerOutput(status=RunnerStatus.ERROR, result={})
        assert validate_runner_output(invalid, strict=False) is False
        
        # ERROR ìƒíƒœì¸ë° error_message ì—†ìŒ (strict=True)
        with pytest.raises(ContractViolation):
            validate_runner_output(invalid, strict=True)


class TestDegradeResponse:
    """ë””ê·¸ë ˆì´ë“œ ì‘ë‹µ í…ŒìŠ¤íŠ¸."""
    
    def test_build_response_no_artifact_has_source(self):
        """ì•„í‹°íŒ©íŠ¸ ì—†ì„ ë•Œë„ sources != []."""
        from backend.agents.supervisor.nodes.summarize_node import (
            _build_response, DEGRADE_SOURCE_ID
        )
        
        state = {"intent": "analyze", "sub_intent": "health"}
        result = _build_response(state, "í…ŒìŠ¤íŠ¸ ì‘ë‹µ", "report", degraded=True)
        
        contract = result.get("answer_contract", {})
        assert contract.get("sources") != [], "sourcesëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ˆì–´ì•¼ í•¨"
        assert DEGRADE_SOURCE_ID in contract.get("sources", [])
    
    def test_build_response_normal_path_has_diagnosis_source(self):
        """ì •ìƒ ê²½ë¡œì—ì„œëŠ” diagnosis source í¬í•¨."""
        from backend.agents.supervisor.nodes.summarize_node import _build_response
        
        state = {"repo": {"owner": "test", "name": "repo"}}
        diagnosis_result = {"scores": {"health_score": 80}}
        
        result = _build_response(state, "í…ŒìŠ¤íŠ¸ ì‘ë‹µ", "report", diagnosis_result)
        
        contract = result.get("answer_contract", {})
        assert contract.get("sources") != []
        assert "diagnosis_test_repo" in contract.get("sources", [])
    
    def test_llm_call_result_degraded_flag(self):
        """LLMCallResultì˜ degraded í”Œë˜ê·¸."""
        from backend.agents.supervisor.nodes.summarize_node import LLMCallResult
        
        # ì •ìƒ
        normal = LLMCallResult("content", success=True)
        assert normal.degraded is False
        
        # ë””ê·¸ë ˆì´ë“œ
        degraded = LLMCallResult("fallback", success=False, degraded=True)
        assert degraded.degraded is True
    
    def test_greeting_response_has_source(self):
        """ì¸ì‚¬ ì‘ë‹µë„ source í¬í•¨."""
        from backend.agents.supervisor.nodes.summarize_node import _build_response
        
        state = {"intent": "smalltalk", "sub_intent": "greeting"}
        result = _build_response(state, "ì•ˆë…•í•˜ì„¸ìš”!", "greeting")
        
        contract = result.get("answer_contract", {})
        # ì¸ì‚¬ëŠ” system_template source
        assert contract.get("sources") != []


class TestIdempotency:
    """Idempotency í…ŒìŠ¤íŠ¸."""
    
    def test_idempotency_store_basic(self):
        """IdempotencyStore ê¸°ë³¸ ë™ì‘."""
        from backend.common.cache import IdempotencyStore
        
        store = IdempotencyStore(ttl=10)
        
        # ì €ì¥
        entry = store.store_result("sess1", "turn1", "step1", {"data": 1})
        assert entry.answer_id.startswith("ans_")
        assert entry.result == {"data": 1}
        
        # ì¡°íšŒ
        cached = store.get_cached("sess1", "turn1", "step1")
        assert cached is not None
        assert cached.answer_id == entry.answer_id
    
    def test_idempotency_store_different_keys(self):
        """ë‹¤ë¥¸ í‚¤ëŠ” ë‹¤ë¥¸ ê²°ê³¼."""
        from backend.common.cache import IdempotencyStore
        
        store = IdempotencyStore(ttl=10)
        
        store.store_result("sess1", "turn1", "step1", {"a": 1})
        store.store_result("sess1", "turn2", "step1", {"b": 2})
        
        cached1 = store.get_cached("sess1", "turn1", "step1")
        cached2 = store.get_cached("sess1", "turn2", "step1")
        
        assert cached1.result == {"a": 1}
        assert cached2.result == {"b": 2}
        assert cached1.answer_id != cached2.answer_id
    
    def test_idempotency_store_disable(self):
        """ë¹„í™œì„±í™” ì‹œ ìºì‹œ ì•ˆí•¨."""
        from backend.common.cache import IdempotencyStore
        
        store = IdempotencyStore(ttl=10)
        store.disable()
        
        store.store_result("sess1", "turn1", "step1", {"data": 1})
        cached = store.get_cached("sess1", "turn1", "step1")
        
        assert cached is None  # ë¹„í™œì„±í™” ì‹œ None ë°˜í™˜
    
    def test_answer_id_in_graph_result(self):
        """Graph ê²°ê³¼ì— answer_id í¬í•¨."""
        from backend.agents.supervisor import get_supervisor_graph, build_initial_state
        
        state = build_initial_state("ì•ˆë…•!")
        graph = get_supervisor_graph()
        result = graph.invoke(state)
        
        assert "answer_id" in result
        assert result["answer_id"].startswith("ans_")
    
    def test_duplicate_execution_same_answer_id(self):
        """ë™ì¼ ì‹¤í–‰ì€ ê°™ì€ answer_id ë°˜í™˜."""
        from backend.common.cache import idempotency_store
        
        # ìºì‹œ ì´ˆê¸°í™”
        idempotency_store.clear()
        
        # ì²« ë²ˆì§¸ ì €ì¥
        entry1 = idempotency_store.store_result("sess_test", "turn_test", "summarize", {"x": 1})
        
        # ë™ì¼ í‚¤ë¡œ ì¡°íšŒ
        cached = idempotency_store.get_cached("sess_test", "turn_test", "summarize")
        
        assert cached is not None
        assert cached.answer_id == entry1.answer_id


class TestHierarchicalRouting:
    """ê³„ì¸µ ë¼ìš°íŒ… í…ŒìŠ¤íŠ¸ (Heuristic â†’ LLM)."""
    
    def test_tier1_greeting_heuristic(self):
        """Tier-1: ì¸ì‚¬ëŠ” íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ì¦‰ì‹œ ë¶„ë¥˜."""
        from backend.agents.supervisor.nodes.intent_classifier import _tier1_heuristic
        
        # ë‹¨ìˆœ ì¸ì‚¬
        result = _tier1_heuristic("ì•ˆë…•")
        assert result is not None
        assert result.intent == "smalltalk"
        assert result.sub_intent == "greeting"
        assert result.method == "heuristic"
        assert result.confidence == 1.0
        
        # ì˜ì–´ ì¸ì‚¬
        result = _tier1_heuristic("hello")
        assert result is not None
        assert result.intent == "smalltalk"
        assert result.sub_intent == "greeting"
    
    def test_tier1_help_heuristic(self):
        """Tier-1: ë„ì›€ë§ì€ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ì¦‰ì‹œ ë¶„ë¥˜."""
        from backend.agents.supervisor.nodes.intent_classifier import _tier1_heuristic
        
        # ê¸°ëŠ¥ ë¬¸ì˜
        result = _tier1_heuristic("ë­˜ í•  ìˆ˜ ìˆì–´?")
        assert result is not None
        assert result.intent == "help"
        assert result.sub_intent == "getting_started"
        
        # ì‚¬ìš©ë²•
        result = _tier1_heuristic("ì‚¬ìš©ë²• ì•Œë ¤ì¤˜")
        assert result is not None
        assert result.intent == "help"
        
        # ì˜¤ë¥˜ ë¬¸ì˜
        result = _tier1_heuristic("ì—ëŸ¬ê°€ ë‚˜ìš”")
        assert result is not None
        assert result.intent == "help"
    
    def test_tier1_overview_heuristic(self):
        """Tier-1: ë ˆí¬ ê°œìš”ëŠ” íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ë¶„ë¥˜."""
        from backend.agents.supervisor.nodes.intent_classifier import _tier1_heuristic
        
        result = _tier1_heuristic("facebook/react ë­ì•¼?")
        assert result is not None
        assert result.intent == "overview"
        assert result.sub_intent == "repo"
        assert result.repo is not None
        assert result.repo["owner"] == "facebook"
    
    def test_tier1_short_emoji_fallback(self):
        """Tier-1: ì§§ì€/ì´ëª¨ì§€ ì¿¼ë¦¬ëŠ” helpë¡œ í´ë°±."""
        from backend.agents.supervisor.nodes.intent_classifier import _tier1_heuristic
        
        # ì§§ì€ ì¿¼ë¦¬
        result = _tier1_heuristic("?")
        assert result is not None
        assert result.intent == "help"
        
        # ì´ëª¨ì§€ë§Œ
        result = _tier1_heuristic("ğŸ‘‹")
        assert result is not None
        assert result.intent == "help"
    
    def test_tier1_analysis_requires_llm(self):
        """Tier-1: ë¶„ì„ ìš”ì²­ì€ LLM ë¶„ë¥˜ í•„ìš”."""
        from backend.agents.supervisor.nodes.intent_classifier import _tier1_heuristic
        
        # ë¶„ì„ + repo â†’ LLM ë¶„ë¥˜ í•„ìš”
        result = _tier1_heuristic("facebook/react ê±´ê°•ë„ ë¶„ì„í•´ì¤˜")
        assert result is None  # LLMìœ¼ë¡œ ë„˜ê¹€
    
    def test_confidence_threshold(self):
        """Confidence ì„ê³„ê°’ ê²€ì¦."""
        from backend.agents.supervisor.intent_config import (
            get_confidence_threshold, 
            should_degrade_to_help
        )
        
        # ì„ê³„ê°’ í™•ì¸
        assert get_confidence_threshold("analyze") == 0.6
        assert get_confidence_threshold("help") == 0.4
        assert get_confidence_threshold("smalltalk") == 0.3
        
        # ë””ê·¸ë ˆì´ë“œ íŒë‹¨
        assert should_degrade_to_help("analyze", 0.5) is True   # 0.5 < 0.6
        assert should_degrade_to_help("analyze", 0.7) is False  # 0.7 >= 0.6
        assert should_degrade_to_help("help", 0.3) is True      # 0.3 < 0.4
        assert should_degrade_to_help("help", 0.5) is False     # 0.5 >= 0.4
    
    def test_routing_fast_path(self):
        """ë¼ìš°íŒ…: smalltalk/help/overviewëŠ” diagnosis ìŠ¤í‚µ."""
        from backend.agents.supervisor.graph import should_run_diagnosis
        
        # smalltalk â†’ summarize
        state = {"intent": "smalltalk", "sub_intent": "greeting"}
        assert should_run_diagnosis(state) == "summarize"
        
        # help â†’ summarize
        state = {"intent": "help", "sub_intent": "getting_started"}
        assert should_run_diagnosis(state) == "summarize"
        
        # overview â†’ summarize
        state = {
            "intent": "overview", 
            "sub_intent": "repo",
            "repo": {"owner": "test", "name": "repo", "url": ""},
        }
        assert should_run_diagnosis(state) == "summarize"
    
    def test_routing_analyze_requires_diagnosis(self):
        """ë¼ìš°íŒ…: analyzeëŠ” diagnosis ì‹¤í–‰."""
        from backend.agents.supervisor.graph import should_run_diagnosis
        
        state = {
            "intent": "analyze",
            "sub_intent": "health",
            "repo": {"owner": "test", "name": "repo", "url": ""},
        }
        assert should_run_diagnosis(state) == "diagnosis"
    
    def test_graph_greeting_no_diagnosis(self):
        """Graph: ì¸ì‚¬ëŠ” diagnosis ë…¸ë“œ ì§„ì… ì•ˆ í•¨."""
        from backend.agents.supervisor import get_supervisor_graph, build_initial_state
        
        state = build_initial_state("ì•ˆë…•í•˜ì„¸ìš”!")
        graph = get_supervisor_graph()
        result = graph.invoke(state)
        
        assert result.get("intent") == "smalltalk"
        assert result.get("diagnosis_result") is None  # diagnosis ì‹¤í–‰ ì•ˆ í•¨
        assert result.get("llm_summary")  # ì‘ë‹µì€ ìˆìŒ
    
    def test_graph_help_no_diagnosis(self):
        """Graph: ë„ì›€ë§ì€ diagnosis ë…¸ë“œ ì§„ì… ì•ˆ í•¨."""
        from backend.agents.supervisor import get_supervisor_graph, build_initial_state
        
        state = build_initial_state("ì–´ë–¤ ê¸°ëŠ¥ì´ ìˆì–´?")
        graph = get_supervisor_graph()
        result = graph.invoke(state)
        
        assert result.get("intent") == "help"
        assert result.get("diagnosis_result") is None
        assert result.get("llm_summary")


class TestLightweightPath:
    """Step 2: Smalltalk/Help ê²½ëŸ‰ ê²½ë¡œ í…ŒìŠ¤íŠ¸."""
    
    def test_smalltalk_greeting_has_next_actions(self):
        """ì¸ì‚¬ ì‘ë‹µì— ë‹¤ìŒ í–‰ë™ 2ê°œ í¬í•¨."""
        from backend.agents.supervisor import get_supervisor_graph, build_initial_state
        
        state = build_initial_state("ì•ˆë…•!")
        graph = get_supervisor_graph()
        result = graph.invoke(state)
        
        summary = result.get("llm_summary", "")
        assert "ë‹¤ìŒ í–‰ë™" in summary
        assert summary.count("`") >= 2  # ìµœì†Œ 2ê°œ ì½”ë“œ ë¸”ë¡ (í–‰ë™ ì œì•ˆ)
    
    def test_help_response_has_next_actions(self):
        """ë„ì›€ë§ ì‘ë‹µì— ë‹¤ìŒ í–‰ë™ 2ê°œ í¬í•¨."""
        from backend.agents.supervisor import get_supervisor_graph, build_initial_state
        
        state = build_initial_state("ë­˜ í•  ìˆ˜ ìˆì–´?")
        graph = get_supervisor_graph()
        result = graph.invoke(state)
        
        summary = result.get("llm_summary", "")
        assert "ë‹¤ìŒ í–‰ë™" in summary
        assert summary.count("`") >= 2
    
    def test_smalltalk_source_is_template(self):
        """ì¸ì‚¬ ì‘ë‹µ sourceê°€ SYS:TEMPLATES:SMALLTALK."""
        from backend.agents.supervisor import get_supervisor_graph, build_initial_state
        
        state = build_initial_state("ì•ˆë…•í•˜ì„¸ìš”")
        graph = get_supervisor_graph()
        result = graph.invoke(state)
        
        contract = result.get("answer_contract", {})
        sources = contract.get("sources", [])
        
        assert len(sources) > 0
        assert "SYS:TEMPLATES:SMALLTALK" in sources[0]
    
    def test_help_source_is_template(self):
        """ë„ì›€ë§ ì‘ë‹µ sourceê°€ SYS:TEMPLATES:HELP."""
        from backend.agents.supervisor import get_supervisor_graph, build_initial_state
        
        state = build_initial_state("ì‚¬ìš©ë²• ì•Œë ¤ì¤˜")
        graph = get_supervisor_graph()
        result = graph.invoke(state)
        
        contract = result.get("answer_contract", {})
        sources = contract.get("sources", [])
        
        assert len(sources) > 0
        assert "SYS:TEMPLATES:HELP" in sources[0]
    
    def test_lightweight_response_latency(self):
        """ê²½ëŸ‰ ì‘ë‹µ ì§€ì—°ì‹œê°„ < 100ms."""
        import time
        from backend.agents.supervisor import get_supervisor_graph, build_initial_state
        
        state = build_initial_state("ì•ˆë…•!")
        graph = get_supervisor_graph()
        
        start = time.perf_counter()
        result = graph.invoke(state)
        elapsed_ms = (time.perf_counter() - start) * 1000
        
        # p95 < 100ms ê²€ì¦ (ì‹¤ì œë¡œëŠ” í›¨ì”¬ ë¹ ë¦„)
        # í…ŒìŠ¤íŠ¸ í™˜ê²½ì—ì„œëŠ” 200ms ì´ë‚´ë©´ í†µê³¼
        assert elapsed_ms < 200, f"Latency too high: {elapsed_ms:.1f}ms"
        assert result.get("llm_summary")
    
    def test_overview_response_format(self):
        """overview ì‘ë‹µ í¬ë§· ê²€ì¦."""
        from backend.agents.supervisor.nodes.summarize_node import _build_lightweight_response
        from backend.agents.supervisor.prompts import OVERVIEW_REPO_TEMPLATE, OVERVIEW_SOURCE_ID
        
        state = {
            "repo": {"owner": "facebook", "name": "react", "url": ""},
        }
        template = OVERVIEW_REPO_TEMPLATE.format(owner="facebook", repo="react")
        result = _build_lightweight_response(state, template, "chat", OVERVIEW_SOURCE_ID)
        
        assert "facebook/react" in result["llm_summary"]
        assert "ë‹¤ìŒ í–‰ë™" in result["llm_summary"]
        assert result["answer_contract"]["sources"][0] == OVERVIEW_SOURCE_ID
    
    def test_chitchat_response(self):
        """chitchat ì‘ë‹µ ê²€ì¦."""
        from backend.agents.supervisor import get_supervisor_graph, build_initial_state
        
        state = build_initial_state("ë„¤ ì•Œê² ì–´ìš” ê³ ë§ˆì›Œìš”")
        graph = get_supervisor_graph()
        result = graph.invoke(state)
        
        # chitchat â†’ smalltalk.chitchat
        assert result.get("intent") == "smalltalk"
        assert "ë‹¤ìŒ í–‰ë™" in result.get("llm_summary", "")


class TestOverviewPath:
    """Step 3: Overview ê²½ë¡œ í…ŒìŠ¤íŠ¸."""
    
    def test_fetch_overview_artifacts(self):
        """ì•„í‹°íŒ©íŠ¸ ìˆ˜ì§‘ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸."""
        from backend.agents.supervisor.service import fetch_overview_artifacts
        
        # ì‹¤ì œ ì €ì¥ì†Œë¡œ í…ŒìŠ¤íŠ¸ (ìºì‹œë¨)
        artifacts = fetch_overview_artifacts("facebook", "react")
        
        # repo_factsëŠ” í•„ìˆ˜
        assert artifacts.repo_facts
        assert artifacts.repo_facts.get("full_name") == "facebook/react"
        
        # sources >= 1 (ìµœì†Œ repo_facts)
        assert len(artifacts.sources) >= 1
        assert any("REPO_FACTS" in s for s in artifacts.sources)
    
    def test_overview_artifacts_sources_count(self):
        """ì•„í‹°íŒ©íŠ¸ sources >= 2 ê²€ì¦."""
        from backend.agents.supervisor.service import fetch_overview_artifacts
        
        artifacts = fetch_overview_artifacts("microsoft", "vscode")
        
        # ì •ìƒ ì¼€ì´ìŠ¤: sources >= 2
        # (repo_facts + readme_head ë˜ëŠ” recent_activity)
        assert len(artifacts.sources) >= 2, f"Expected >= 2 sources, got {artifacts.sources}"
    
    def test_build_overview_prompt(self):
        """Overview í”„ë¡¬í”„íŠ¸ ë¹Œë“œ í…ŒìŠ¤íŠ¸."""
        from backend.agents.supervisor.prompts import build_overview_prompt
        
        system, user = build_overview_prompt(
            owner="test",
            repo="repo",
            repo_facts={"description": "Test repo", "stars": 100, "language": "Python"},
            readme_head="# Test\n\nThis is a test.",
            recent_activity={"commit_count_30d": 10},
        )
        
        assert "test/repo" in system
        assert "ë‹¤ìŒ í–‰ë™" in system
        assert "repo_facts" in user
        assert "readme_head" in user
        assert "recent_activity" in user
    
    def test_overview_response_has_sources(self):
        """Overview ì‘ë‹µì— sources í¬í•¨."""
        from backend.agents.supervisor.nodes.summarize_node import _build_overview_response
        
        sources = ["ARTIFACT:REPO_FACTS:test/repo", "ARTIFACT:README_HEAD:test/repo"]
        result = _build_overview_response(
            state={},
            summary="Test summary",
            sources=sources,
            repo_id="test/repo",
        )
        
        contract = result.get("answer_contract", {})
        assert len(contract.get("sources", [])) >= 2
        assert "github_artifact" in contract.get("source_kinds", [])
    
    def test_overview_fallback_template(self):
        """API ì œí•œ ì‹œ fallback í…œí”Œë¦¿ ì‚¬ìš©."""
        from backend.agents.supervisor.prompts import OVERVIEW_FALLBACK_TEMPLATE
        
        fallback = OVERVIEW_FALLBACK_TEMPLATE.format(
            owner="test",
            repo="repo",
            description="A test repo",
            language="Python",
            stars=100,
            forks=10,
        )
        
        assert "test/repo" in fallback
        assert "ë‹¤ìŒ í–‰ë™" in fallback
        assert "100" in fallback  # stars


class TestFollowupPlanner:
    """Follow-up Planner í…ŒìŠ¤íŠ¸: ì§ì „ í„´ ì•„í‹°íŒ©íŠ¸ ê¸°ë°˜ ê·¼ê±° ì„¤ëª…."""
    
    def test_detect_followup_patterns(self):
        """í›„ì† íŒ¨í„´ ê°ì§€."""
        from backend.agents.supervisor.nodes.intent_classifier import _detect_followup
        
        # ì§ì „ ì•„í‹°íŒ©íŠ¸ ìˆì„ ë•Œ
        assert _detect_followup("ê·¸ ê²°ê³¼ ì™œ ê·¸ë˜?", has_prev_artifacts=True)
        assert _detect_followup("ê·¼ê±°ê°€ ë­ì•¼?", has_prev_artifacts=True)
        assert _detect_followup("ì™œ?", has_prev_artifacts=True)
        assert _detect_followup("ì–´ë””ì„œ ë‚˜ì™”ì–´?", has_prev_artifacts=True)
        assert _detect_followup("ì¢€ë” ìì„¸íˆ ì„¤ëª…í•´ì¤˜", has_prev_artifacts=True)
        
        # ì§ì „ ì•„í‹°íŒ©íŠ¸ ì—†ì„ ë•Œ â†’ False
        assert not _detect_followup("ê·¸ ê²°ê³¼ ì™œ ê·¸ë˜?", has_prev_artifacts=False)
        assert not _detect_followup("ì™œ?", has_prev_artifacts=False)
    
    def test_followup_intent_classification(self):
        """follow-up intent ë¶„ë¥˜."""
        from backend.agents.supervisor.nodes.intent_classifier import _tier1_heuristic
        
        # has_prev_artifacts=Trueì¼ ë•Œë§Œ followup.evidenceë¡œ ë¶„ë¥˜
        result = _tier1_heuristic("ê·¼ê±°ê°€ ë­ì•¼?", has_prev_artifacts=True)
        assert result is not None
        assert result.intent == "followup"
        assert result.sub_intent == "evidence"
        
        # has_prev_artifacts=Falseì¼ ë•ŒëŠ” None (LLMìœ¼ë¡œ ë„˜ì–´ê°)
        result = _tier1_heuristic("ê·¼ê±°ê°€ ë­ì•¼?", has_prev_artifacts=False)
        assert result is None
    
    def test_followup_no_artifacts_fallback(self):
        """ì§ì „ ì•„í‹°íŒ©íŠ¸ ì—†ìœ¼ë©´ ì•ˆë‚´ + ì„ íƒì§€."""
        from backend.agents.supervisor.nodes.summarize_node import _handle_followup_evidence_mode
        
        state = {"user_query": "ì™œ ê·¸ë˜?"}
        result = _handle_followup_evidence_mode(state, "ì™œ ê·¸ë˜?", None)
        
        assert "ì´ì „ ë¶„ì„ ê²°ê³¼ê°€ ì—†ì–´" in result["llm_summary"]
        assert "ë‹¤ìŒ í–‰ë™" in result["llm_summary"]
    
    def test_followup_evidence_prompt(self):
        """Follow-up ê·¼ê±° ì„¤ëª… í”„ë¡¬í”„íŠ¸ ìƒì„±."""
        from backend.agents.supervisor.prompts import build_followup_evidence_prompt
        
        artifacts = {
            "scores": {"health_score": 75, "documentation_quality": 80},
            "labels": {"health_level": "good"},
        }
        
        system, user = build_followup_evidence_prompt(
            user_query="ì™œ ê·¸ëŸ° ì ìˆ˜ê°€ ë‚˜ì™”ì–´?",
            prev_intent="analyze",
            prev_answer_kind="report",
            repo_id="test/repo",
            artifacts=artifacts,
        )
        
        assert "ê·¼ê±°" in system
        assert "3-5ë¬¸ì¥" in system
        assert "ì°¸ì¡° ë°ì´í„°" in system
        assert "test/repo" in user
        assert "health_score" in user
        assert "75" in user
    
    def test_followup_response_has_sources(self):
        """Follow-up ì‘ë‹µì— ì§ì „ ì•„í‹°íŒ©íŠ¸ sources í¬í•¨."""
        from backend.agents.supervisor.nodes.summarize_node import _build_followup_response
        
        sources = ["PREV:test/repo:scores", "PREV:test/repo:labels"]
        result = _build_followup_response(
            state={},
            summary="Test evidence explanation",
            sources=sources,
            repo_id="test/repo",
            diagnosis_result=None,
        )
        
        contract = result.get("answer_contract", {})
        assert len(contract.get("sources", [])) >= 2
        assert "prev_turn_artifact" in contract.get("source_kinds", [])
        assert result["answer_kind"] == "explain"
    
    def test_followup_config_registered(self):
        """followup.evidenceê°€ V1 ì§€ì› ëª©ë¡ì— ë“±ë¡ë¨."""
        from backend.agents.supervisor.intent_config import (
            is_v1_supported,
            get_intent_meta,
            get_answer_kind,
        )
        
        assert is_v1_supported("followup", "evidence")
        
        meta = get_intent_meta("followup", "evidence")
        assert meta["requires_repo"] is False
        assert meta["runs_diagnosis"] is False
        
        assert get_answer_kind("followup", "evidence") == "explain"


class TestExpertRunner:
    """Expert Runner í…ŒìŠ¤íŠ¸: sources í•„ìˆ˜, ì—ëŸ¬ ì •ì±…, ë””ê·¸ë ˆì´ë“œ."""
    
    def test_runner_result_success(self):
        """RunnerResult.ok ìƒì„±."""
        from backend.agents.supervisor.runners.base import RunnerResult
        from backend.agents.shared.contracts import AnswerContract
        
        answer = AnswerContract(
            text="Test",
            sources=["ARTIFACT:TEST:repo"],
            source_kinds=["test"],
        )
        result = RunnerResult.ok(
            answer=answer,
            artifacts_out=["ARTIFACT:TEST:repo"],
        )
        
        assert result.success is True
        assert result.degraded is False
        assert len(result.artifacts_out) == 1
    
    def test_runner_result_degraded(self):
        """RunnerResult.degraded_ok ìƒì„±."""
        from backend.agents.supervisor.runners.base import RunnerResult
        from backend.agents.shared.contracts import AnswerContract
        
        answer = AnswerContract(
            text="Degraded response",
            sources=["FALLBACK:repo"],
            source_kinds=["fallback"],
        )
        result = RunnerResult.degraded_ok(
            answer=answer,
            artifacts_out=["FALLBACK:repo"],
            reason="test_degrade",
        )
        
        assert result.success is True
        assert result.degraded is True
        assert result.meta.get("degrade_reason") == "test_degrade"
    
    def test_artifact_collector(self):
        """ArtifactCollector ê¸°ëŠ¥."""
        from backend.agents.supervisor.runners.base import ArtifactCollector
        
        collector = ArtifactCollector("test/repo")
        
        # Add artifacts
        aid = collector.add("overview", {"stars": 100})
        assert "ARTIFACT:OVERVIEW:test/repo" in aid
        
        collector.add("readme", "# Title", required=False)
        
        # Get artifacts
        assert collector.get("overview") == {"stars": 100}
        assert collector.get("readme") == "# Title"
        assert collector.get("missing") is None
        
        # IDs and kinds
        assert len(collector.get_ids()) == 2
        assert "overview" in collector.get_kinds()
    
    def test_artifact_collector_missing_required(self):
        """í•„ìˆ˜ ì•„í‹°íŒ©íŠ¸ ëˆ„ë½ ê°ì§€."""
        from backend.agents.supervisor.runners.base import ArtifactCollector
        
        collector = ArtifactCollector("test/repo")
        collector.add("overview", None, required=True)  # None data
        collector.add("readme", "content", required=False)
        
        assert not collector.has_required()
        assert "overview" in collector.missing_required()
    
    def test_error_policy_mapping(self):
        """ì—ëŸ¬ ì¢…ë¥˜ë³„ ì •ì±… ë§¤í•‘."""
        from backend.agents.supervisor.runners.base import ExpertRunner, ErrorPolicy
        
        # Mock runner for testing
        class TestRunner(ExpertRunner):
            runner_name = "test"
            def _collect_artifacts(self): pass
            def _execute(self): pass
        
        runner = TestRunner("test/repo")
        
        assert runner._get_error_policy("rate limit exceeded") == ErrorPolicy.RETRY
        assert runner._get_error_policy("timeout error") == ErrorPolicy.RETRY
        assert runner._get_error_policy("not found") == ErrorPolicy.ASK_USER
        assert runner._get_error_policy("permission denied") == ErrorPolicy.ASK_USER
        assert runner._get_error_policy("no data available") == ErrorPolicy.FALLBACK
    
    def test_diagnosis_runner_builds_answer_with_sources(self):
        """DiagnosisRunnerê°€ sources í¬í•¨ AnswerContract ìƒì„±."""
        from backend.agents.supervisor.runners.base import ArtifactCollector
        from backend.agents.shared.contracts import AnswerContract
        
        # Test _build_answer method
        collector = ArtifactCollector("test/repo")
        collector.add("diagnosis_scores", {"health_score": 75})
        collector.add("diagnosis_labels", {"health_level": "good"})
        
        # Build answer with sources
        sources = collector.get_ids()
        kinds = collector.get_kinds()
        
        answer = AnswerContract(
            text="Test diagnosis",
            sources=sources,
            source_kinds=kinds,
        )
        
        assert len(answer.sources) >= 2
        assert "diagnosis_scores" in answer.source_kinds
    
    def test_runner_validates_empty_sources(self):
        """Empty sources ê²€ì¦ ë° ìë™ ì±„ì›€."""
        from backend.agents.supervisor.runners.base import ExpertRunner
        from backend.agents.shared.contracts import AnswerContract
        
        class TestRunner(ExpertRunner):
            runner_name = "test"
            def _collect_artifacts(self):
                self.collector.add("test_artifact", {"data": "value"})
            def _execute(self):
                pass
        
        runner = TestRunner("test/repo")
        runner._collect_artifacts()
        
        # Empty sources answer
        answer = AnswerContract(text="Test", sources=[], source_kinds=[])
        runner._validate_answer_contract(answer)
        
        # Should auto-fill from collector
        assert len(answer.sources) > 0
    
    def test_compare_runner_structure(self):
        """CompareRunner êµ¬ì¡° ê²€ì¦."""
        from backend.agents.supervisor.runners import CompareRunner
        
        runner = CompareRunner(
            repo_a="facebook/react",
            repo_b="vuejs/vue",
        )
        
        assert runner.runner_name == "compare"
        assert runner.repo_a == "facebook/react"
        assert runner.repo_b == "vuejs/vue"
        assert "repo_a_overview" in runner.required_artifacts
    
    def test_onepager_runner_structure(self):
        """OnepagerRunner êµ¬ì¡° ê²€ì¦."""
        from backend.agents.supervisor.runners import OnepagerRunner
        
        runner = OnepagerRunner(repo_id="test/repo")
        
        assert runner.runner_name == "onepager"
        assert "repo_overview" in runner.required_artifacts


class TestAgenticPlanning:
    """Agentic Planning í…ŒìŠ¤íŠ¸ (Step 12)."""
    
    def test_plan_model_creation(self):
        """Plan ëª¨ë¸ ìƒì„± ê²€ì¦."""
        from backend.agents.supervisor.planner import (
            Plan, PlanStep, PlanStatus, StepStatus, ErrorPolicy
        )
        
        step = PlanStep(
            id="test_step",
            runner="diagnosis",
            params={"repo": "test/repo"},
            needs=[],
            on_error=ErrorPolicy.FALLBACK,
        )
        
        assert step.id == "test_step"
        assert step.status == StepStatus.PENDING
        assert step.is_ready(set())
        
        plan = Plan(
            id="test_plan",
            intent="analyze",
            sub_intent="health",
            steps=[step],
        )
        
        assert plan.status == PlanStatus.PENDING
        assert len(plan.steps) == 1
        assert not plan.is_complete()
    
    def test_plan_step_dependencies(self):
        """PlanStep ì˜ì¡´ì„± ê²€ì¦."""
        from backend.agents.supervisor.planner import PlanStep, ErrorPolicy
        
        step_a = PlanStep(id="a", runner="diagnosis", needs=[])
        step_b = PlanStep(id="b", runner="compare", needs=["a"])
        
        # step_a is ready (no deps)
        assert step_a.is_ready(set())
        
        # step_b needs step_a
        assert not step_b.is_ready(set())
        assert step_b.is_ready({"a"})
    
    def test_plan_builder_analyze_health(self):
        """PlanBuilder: analyze.health ê³„íš ìƒì„±."""
        from backend.agents.supervisor.planner import build_plan
        
        plan = build_plan("analyze", "health", {"repo": {"owner": "test", "name": "repo"}})
        
        assert plan.intent == "analyze"
        assert plan.sub_intent == "health"
        assert len(plan.steps) >= 1
        assert plan.steps[0].runner == "diagnosis"
    
    def test_plan_builder_smalltalk(self):
        """PlanBuilder: smalltalk.greeting ê³„íš ìƒì„±."""
        from backend.agents.supervisor.planner import build_plan
        
        plan = build_plan("smalltalk", "greeting", {})
        
        assert plan.intent == "smalltalk"
        assert len(plan.steps) == 1
        assert plan.steps[0].runner == "smalltalk"
        assert plan.steps[0].timeout_sec == 5.0  # Fast path
    
    def test_plan_builder_compare(self):
        """PlanBuilder: analyze.compare ê³„íš ìƒì„± (ë³‘ë ¬ ì˜ì¡´ì„±)."""
        from backend.agents.supervisor.planner import build_plan
        
        plan = build_plan("analyze", "compare", {
            "repo": {"owner": "facebook", "name": "react"},
            "compare_repo": {"owner": "vuejs", "name": "vue"},
        })
        
        assert plan.intent == "analyze"
        assert plan.sub_intent == "compare"
        
        # fetch_repo_a, fetch_repo_b (parallel), compare (depends on both)
        step_ids = [s.id for s in plan.steps]
        assert "fetch_repo_a" in step_ids
        assert "fetch_repo_b" in step_ids
        assert "compare" in step_ids
        
        # Compare step depends on both fetch steps
        compare_step = next(s for s in plan.steps if s.id == "compare")
        assert "fetch_repo_a" in compare_step.needs
        assert "fetch_repo_b" in compare_step.needs
    
    def test_plan_get_ready_steps(self):
        """Plan.get_ready_steps() ê²€ì¦."""
        from backend.agents.supervisor.planner import Plan, PlanStep, StepStatus
        
        step_a = PlanStep(id="a", runner="diagnosis", needs=[])
        step_b = PlanStep(id="b", runner="diagnosis", needs=[])
        step_c = PlanStep(id="c", runner="compare", needs=["a", "b"])
        
        plan = Plan(
            id="test",
            intent="analyze",
            sub_intent="compare",
            steps=[step_a, step_b, step_c],
        )
        
        # Initially a and b are ready
        ready = plan.get_ready_steps()
        assert len(ready) == 2
        assert {s.id for s in ready} == {"a", "b"}
        
        # After a completes, still just b is ready (c needs both)
        step_a.status = StepStatus.SUCCESS
        ready = plan.get_ready_steps()
        assert len(ready) == 1
        assert ready[0].id == "b"
        
        # After both complete, c is ready
        step_b.status = StepStatus.SUCCESS
        ready = plan.get_ready_steps()
        assert len(ready) == 1
        assert ready[0].id == "c"
    
    def test_step_result_dataclass(self):
        """StepResult ë°ì´í„°í´ë˜ìŠ¤ ê²€ì¦."""
        from backend.agents.supervisor.planner import StepResult, StepStatus
        
        result = StepResult(
            step_id="test",
            status=StepStatus.SUCCESS,
            result={"data": "value"},
            execution_time_ms=100.5,
        )
        
        assert result.success
        assert result.step_id == "test"
        assert result.result["data"] == "value"
        
        failed = StepResult(
            step_id="fail",
            status=StepStatus.FAILED,
            error_message="Test error",
        )
        
        assert not failed.success
        assert failed.error_message == "Test error"
    
    def test_error_policy_enum(self):
        """ErrorPolicy enum ê²€ì¦."""
        from backend.agents.supervisor.planner import ErrorPolicy
        
        assert ErrorPolicy.RETRY.value == "retry"
        assert ErrorPolicy.FALLBACK.value == "fallback"
        assert ErrorPolicy.ASK_USER.value == "ask_user"
        assert ErrorPolicy.ABORT.value == "abort"
    
    def test_replanner_can_replan(self):
        """Replanner ì¬ê³„íš ê°€ëŠ¥ ì—¬ë¶€ ê²€ì¦."""
        from backend.agents.supervisor.planner import Plan, Replanner
        
        plan = Plan(id="test", intent="analyze", sub_intent="health")
        replanner = Replanner(plan)
        
        assert replanner.can_replan()
        
        # After max attempts
        plan.replan_count = 2
        replanner2 = Replanner(plan)
        assert not replanner2.can_replan()
    
    def test_replanner_step_failure(self):
        """Replanner: ìŠ¤í… ì‹¤íŒ¨ ì‹œ ì¬ê³„íš."""
        from backend.agents.supervisor.planner import (
            Plan, PlanStep, Replanner, ReplanReason, ErrorPolicy, StepStatus
        )
        
        step = PlanStep(
            id="failed_step",
            runner="diagnosis",
            on_error=ErrorPolicy.FALLBACK,
        )
        step.status = StepStatus.FAILED
        step.error_message = "Test failure"
        
        plan = Plan(
            id="original",
            intent="analyze",
            sub_intent="health",
            steps=[step],
        )
        
        replanner = Replanner(plan)
        new_plan = replanner.replan(step, ReplanReason.STEP_FAILED)
        
        assert new_plan is not None
        assert new_plan.replan_count == 1
        assert "_r1" in new_plan.id
    
    def test_plan_status_transitions(self):
        """Plan ìƒíƒœ ì „í™˜ ê²€ì¦."""
        from backend.agents.supervisor.planner import Plan, PlanStatus
        
        plan = Plan(id="test", intent="analyze", sub_intent="health")
        
        assert plan.status == PlanStatus.PENDING
        
        plan.mark_running()
        assert plan.status == PlanStatus.RUNNING
        
        plan.mark_success()
        assert plan.status == PlanStatus.SUCCESS
        
        plan2 = Plan(id="test2", intent="analyze", sub_intent="health")
        plan2.mark_failed("Error occurred")
        assert plan2.status == PlanStatus.FAILED
        assert plan2.error_message == "Error occurred"
        
        plan3 = Plan(id="test3", intent="analyze", sub_intent="health")
        plan3.mark_ask_user("Need clarification")
        assert plan3.status == PlanStatus.ASK_USER


class TestPlanningErrorRecovery:
    """Planning ì˜¤ë¥˜ ë³µêµ¬ í…ŒìŠ¤íŠ¸: ì •ìƒ ì¢…ë£Œìœ¨ â‰¥ 95% ê²€ì¦."""
    
    def test_error_terminates_gracefully(self):
        """ì˜¤ë¥˜ ë°œìƒ ì‹œ ì •ìƒ ì¢…ë£Œ (ask_user ë˜ëŠ” ì—ëŸ¬ ë©”ì‹œì§€)."""
        from backend.agents.supervisor.planner import (
            Plan, PlanStep, PlanExecutor, PlanStatus, StepStatus, ErrorPolicy
        )
        
        # Mock executor that always fails
        def failing_runner(step, inputs):
            from backend.agents.supervisor.planner import StepResult
            return StepResult(
                step_id=step.id,
                status=StepStatus.FAILED,
                error_message="Simulated failure",
            )
        
        step = PlanStep(
            id="fail_step",
            runner="mock",
            on_error=ErrorPolicy.ASK_USER,  # Should escalate
            max_retries=0,
        )
        
        plan = Plan(
            id="test",
            intent="analyze",
            sub_intent="health",
            steps=[step],
        )
        
        executor = PlanExecutor(step_executors={"mock": failing_runner})
        result = executor.execute(plan, {})
        
        # Should terminate gracefully (either ASK_USER or FAILED with message)
        assert result.status in (PlanStatus.ASK_USER, PlanStatus.FAILED)
        assert result.error_message is not None
    
    def test_retry_then_fallback(self):
        """retry â†’ fallback ì •ì±… ê²€ì¦."""
        from backend.agents.supervisor.planner import (
            Plan, PlanStep, PlanExecutor, StepStatus, ErrorPolicy
        )
        
        call_count = [0]
        
        def retry_then_succeed(step, inputs):
            from backend.agents.supervisor.planner import StepResult
            call_count[0] += 1
            
            if call_count[0] <= 1:
                return StepResult(
                    step_id=step.id,
                    status=StepStatus.FAILED,
                    error_message="First call fails",
                )
            
            return StepResult(
                step_id=step.id,
                status=StepStatus.SUCCESS,
                result={"data": "success"},
            )
        
        step = PlanStep(
            id="retry_step",
            runner="mock",
            on_error=ErrorPolicy.RETRY,
            max_retries=1,
        )
        
        plan = Plan(
            id="test",
            intent="analyze",
            sub_intent="health",
            steps=[step],
        )
        
        executor = PlanExecutor(step_executors={"mock": retry_then_succeed})
        result = executor.execute(plan, {})
        
        # Should succeed after retry
        assert call_count[0] == 2
    
    def test_parallel_execution_all_succeed(self):
        """ë³‘ë ¬ ì‹¤í–‰ ì‹œ ëª¨ë‘ ì„±ê³µ."""
        from backend.agents.supervisor.planner import (
            Plan, PlanStep, PlanExecutor, PlanStatus, StepStatus
        )
        
        def success_runner(step, inputs):
            from backend.agents.supervisor.planner import StepResult
            return StepResult(
                step_id=step.id,
                status=StepStatus.SUCCESS,
                result={"step": step.id},
            )
        
        step_a = PlanStep(id="a", runner="mock", needs=[])
        step_b = PlanStep(id="b", runner="mock", needs=[])
        
        plan = Plan(
            id="test",
            intent="analyze",
            sub_intent="compare",
            steps=[step_a, step_b],
        )
        
        executor = PlanExecutor(step_executors={"mock": success_runner})
        result = executor.execute(plan, {})
        
        assert result.status == PlanStatus.SUCCESS
        assert len(result.execution_order) == 2


class TestIntentThresholds:
    """Step 8: ì˜ë„ë³„ ì„ê³„/ì •ì±… ë¶„ë¦¬ í…ŒìŠ¤íŠ¸."""
    
    def test_confidence_thresholds_by_cost(self):
        """ë¹„ìš©ì— ë”°ë¥¸ ì„ê³„ê°’ ê²€ì¦: ê³ ë¹„ìš©=ë†’ì€ì„ê³„, ì €ë¹„ìš©=ë‚®ì€ì„ê³„."""
        from backend.agents.supervisor.intent_config import (
            get_confidence_threshold,
            get_disambiguation_threshold,
        )
        
        # ê³ ë¹„ìš© (analyze, compare): ë†’ì€ ì„ê³„
        assert get_confidence_threshold("analyze") == 0.6
        assert get_confidence_threshold("compare") == 0.6
        
        # ì¤‘ë¹„ìš© (followup, recommendation): ì¤‘ê°„ ì„ê³„
        assert get_confidence_threshold("followup") == 0.5
        assert get_confidence_threshold("recommendation") == 0.5
        
        # ì €ë¹„ìš© (overview, general_qa): ë‚®ì€ ì„ê³„
        assert get_confidence_threshold("overview") == 0.4
        assert get_confidence_threshold("general_qa") == 0.5
        
        # ê²½ëŸ‰ (smalltalk, help): ê°€ì¥ ë‚®ì€ ì„ê³„
        assert get_confidence_threshold("smalltalk") == 0.3
        assert get_confidence_threshold("help") == 0.4
    
    def test_disambiguation_thresholds(self):
        """Disambiguation ì„ê³„ê°’ ê²€ì¦."""
        from backend.agents.supervisor.intent_config import (
            get_disambiguation_threshold,
            should_disambiguate,
        )
        
        # ê³ ë¹„ìš©ì€ ë†’ì€ disambiguation ì„ê³„
        assert get_disambiguation_threshold("analyze") == 0.4
        assert get_disambiguation_threshold("compare") == 0.4
        
        # ê²½ëŸ‰ì€ ë‚®ì€ disambiguation ì„ê³„
        assert get_disambiguation_threshold("smalltalk") == 0.15
        assert get_disambiguation_threshold("help") == 0.2
        
        # should_disambiguate ë¡œì§
        assert should_disambiguate("analyze", 0.35) is True
        assert should_disambiguate("analyze", 0.45) is False
        assert should_disambiguate("smalltalk", 0.10) is True
        assert should_disambiguate("smalltalk", 0.20) is False
    
    def test_calibration_store(self):
        """CalibrationStore ê¸°ë³¸ ë™ì‘ ê²€ì¦."""
        from backend.agents.supervisor.calibration import CalibrationStore
        
        store = CalibrationStore()
        
        # ì¿¼ë¦¬ ê¸°ë¡
        store.record_query("analyze", 0.7, {"analyze": 0.7, "followup": 0.2})
        store.record_query("analyze", 0.55, {"analyze": 0.55, "followup": 0.3}, was_disambiguation=True)
        
        # ë©”íŠ¸ë¦­ ì¡°íšŒ
        metrics = store.get_metrics("analyze")
        assert metrics is not None
        assert metrics["total_queries"] == 2
        assert metrics["disambiguation_rate"] == 0.5
    
    def test_calibration_weekly_adjustment(self):
        """ì£¼ê°„ ì„ê³„ê°’ ì¡°ì • ê²€ì¦."""
        from backend.agents.supervisor.calibration import CalibrationStore
        
        store = CalibrationStore()
        
        # ì¶©ë¶„í•œ ë°ì´í„° ê¸°ë¡ (disambiguationì´ ë„ˆë¬´ ì ìŒ â†’ ì„ê³„ ì˜¬ë¦¼)
        for _ in range(20):
            store.record_query("analyze", 0.8, {"analyze": 0.8})
        
        adjustment = store.compute_weekly_adjustment("analyze")
        # Disambiguationì´ 0%ë¼ ì„ê³„ê°’ ì˜¬ë ¤ì•¼ í•¨
        assert adjustment > 0
    
    def test_check_disambiguation(self):
        """Disambiguation ì²´í¬ ë¡œì§ ê²€ì¦."""
        from backend.agents.supervisor.calibration import check_disambiguation
        
        # ë‚®ì€ confidence â†’ disambiguation
        result = check_disambiguation("analyze", 0.3, has_repo=True)
        assert result.should_disambiguate is True
        
        # repo ì—†ìœ¼ë©´ â†’ disambiguation
        result = check_disambiguation("analyze", 0.8, has_repo=False)
        assert result.should_disambiguate is True
        assert "ì €ì¥ì†Œ" in result.reason
        
        # ì¶©ë¶„í•œ confidence + repo â†’ í†µê³¼
        result = check_disambiguation("analyze", 0.7, has_repo=True)
        assert result.should_disambiguate is False
    
    def test_temperature_scaling(self):
        """Temperature scaling í•¨ìˆ˜ ê²€ì¦."""
        from backend.agents.supervisor.calibration import temperature_scale
        
        logits = {"analyze": 2.0, "followup": 1.0, "general_qa": 0.5}
        
        # temp=1.0: ì›ë˜ softmax
        probs_1 = temperature_scale(logits, temperature=1.0)
        assert probs_1["analyze"] > probs_1["followup"] > probs_1["general_qa"]
        
        # temp=2.0: ë” ë¶€ë“œëŸ¬ìš´ ë¶„í¬
        probs_2 = temperature_scale(logits, temperature=2.0)
        # ë†’ì€ temp â†’ í™•ë¥  ì°¨ì´ ì¤„ì–´ë“¦
        assert (probs_2["analyze"] - probs_2["followup"]) < (probs_1["analyze"] - probs_1["followup"])


class TestToneGuide:
    """Step 9: í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸/í†¤ ê°€ì´ë“œ í…ŒìŠ¤íŠ¸."""
    
    def test_mode_mapping(self):
        """Intent â†’ Mode ë§¤í•‘ ê²€ì¦."""
        from backend.agents.supervisor.tone_guide import (
            get_mode_for_intent,
            PromptMode,
        )
        
        # Fast mode
        assert get_mode_for_intent("smalltalk") == PromptMode.FAST
        assert get_mode_for_intent("help") == PromptMode.FAST
        assert get_mode_for_intent("general_qa") == PromptMode.FAST
        
        # Expert mode
        assert get_mode_for_intent("analyze") == PromptMode.EXPERT
        assert get_mode_for_intent("compare") == PromptMode.EXPERT
        assert get_mode_for_intent("followup") == PromptMode.EXPERT
    
    def test_tone_config_params(self):
        """í†¤ ì„¤ì • íŒŒë¼ë¯¸í„° ê²€ì¦."""
        from backend.agents.supervisor.tone_guide import (
            get_tone_config,
            PromptMode,
        )
        
        fast = get_tone_config(PromptMode.FAST)
        expert = get_tone_config(PromptMode.EXPERT)
        
        # Fast: ë†’ì€ temperature, ì ì€ ë¶ˆë¦¿
        assert fast.temperature == 0.7
        assert fast.max_bullets == 3
        assert fast.max_sentences == 5
        assert fast.allow_chitchat is True
        
        # Expert: ë‚®ì€ temperature, ë§ì€ ë¶ˆë¦¿
        assert expert.temperature == 0.25
        assert expert.max_bullets == 7
        assert expert.max_sentences == 15
        assert expert.allow_chitchat is False
    
    def test_llm_params_for_intent(self):
        """Intentë³„ LLM íŒŒë¼ë¯¸í„° ê²€ì¦."""
        from backend.agents.supervisor.tone_guide import get_llm_params_for_intent
        
        # Fast mode intents
        chat_params = get_llm_params_for_intent("general_qa")
        assert chat_params["temperature"] == 0.7
        
        # Expert mode intents
        analyze_params = get_llm_params_for_intent("analyze")
        assert analyze_params["temperature"] == 0.25
    
    def test_tone_compliance_check(self):
        """í†¤ ì¤€ìˆ˜ ì²´í¬ ê²€ì¦."""
        from backend.agents.supervisor.tone_guide import (
            check_tone_compliance,
            is_tone_compliant,
            PromptMode,
        )
        
        # ì¢‹ì€ Expert ì‘ë‹µ
        good_expert = """### ë¶„ì„ ê²°ê³¼

| ì§€í‘œ | ì ìˆ˜ |
|------|------|
| ê±´ê°• ì ìˆ˜ | 78ì  |

í™œë™ì„±ì´ 85ì ìœ¼ë¡œ ìš°ìˆ˜í•©ë‹ˆë‹¤.

**ë‹¤ìŒ í–‰ë™**
- ì ìˆ˜ ìì„¸íˆ ì„¤ëª…í•´ì¤˜"""
        
        results = check_tone_compliance(good_expert, PromptMode.EXPERT)
        assert results["ì¡´ëŒ“ë§ ì‚¬ìš©"] is True
        assert results["ì´ëª¨ì§€ ì—†ìŒ"] is True
        assert results["ë°ì´í„° ì¸ìš©"] is True
        
        # ë‚˜ìœ ì‘ë‹µ (ì¶”ì¸¡ í‘œí˜„)
        bad_expert = "ì•„ë§ˆ ì´ê²ƒì€ ì¢‹ì€ ê²ƒ ê°™ìŠµë‹ˆë‹¤."
        results = check_tone_compliance(bad_expert, PromptMode.EXPERT)
        assert results["ì¶”ì¸¡ í‘œí˜„ ì—†ìŒ"] is False
    
    def test_response_length_validation(self):
        """ì‘ë‹µ ê¸¸ì´ ê²€ì¦."""
        from backend.agents.supervisor.tone_guide import (
            validate_response_length,
            truncate_response,
            PromptMode,
        )
        
        short_text = "ì§§ì€ ì‘ë‹µì…ë‹ˆë‹¤. ì¢‹ì•„ìš”."
        is_valid, warning = validate_response_length(short_text, PromptMode.FAST)
        assert is_valid is True
        assert warning is None
        
        # ê¸´ í…ìŠ¤íŠ¸ ìë¥´ê¸°
        long_text = "í…ŒìŠ¤íŠ¸ " * 500
        truncated = truncate_response(long_text, PromptMode.FAST)
        assert len(truncated) < len(long_text)
        assert "ìƒëµ" in truncated
    
    def test_prompts_llm_params_updated(self):
        """prompts.py LLM_PARAMSê°€ Step 9 ê¸°ì¤€ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë¨."""
        from backend.agents.supervisor.prompts import LLM_PARAMS
        
        # Expert mode: ë‚®ì€ temperature
        assert LLM_PARAMS["health_report"]["temperature"] == 0.25
        assert LLM_PARAMS["score_explain"]["temperature"] == 0.2
        assert LLM_PARAMS["followup_evidence"]["temperature"] == 0.2
        
        # Fast mode: ë†’ì€ temperature
        assert LLM_PARAMS["chat"]["temperature"] == 0.7
        assert LLM_PARAMS["greeting"]["temperature"] == 0.7


# Step 10: ê´€ì¸¡/ê²€ì¦ ìš´ì˜ ê²Œì´íŠ¸ í…ŒìŠ¤íŠ¸
class TestObservability:
    """Step 10: ê´€ì¸¡ì„± ë° SLO ê²€ì¦ í…ŒìŠ¤íŠ¸."""
    
    def test_required_event_types(self):
        """í•„ìˆ˜ ì´ë²¤íŠ¸ 5ì¢… ì •ì˜."""
        from backend.agents.supervisor.observability import REQUIRED_EVENT_TYPES
        from backend.common.events import EventType
        
        assert len(REQUIRED_EVENT_TYPES) == 5
        assert EventType.SUPERVISOR_INTENT_DETECTED in REQUIRED_EVENT_TYPES
        assert EventType.SUPERVISOR_ROUTE_SELECTED in REQUIRED_EVENT_TYPES
        assert EventType.NODE_STARTED in REQUIRED_EVENT_TYPES
        assert EventType.NODE_FINISHED in REQUIRED_EVENT_TYPES
        assert EventType.ANSWER_GENERATED in REQUIRED_EVENT_TYPES
    
    def test_slo_config_defaults(self):
        """SLO ê¸°ë³¸ ì„¤ì • ê²€ì¦."""
        from backend.agents.supervisor.observability import SLOConfig
        
        config = SLOConfig()
        
        # Latency SLO
        assert config.greeting_p95_ms == 100.0
        assert config.overview_p95_ms == 1500.0
        assert config.expert_p95_ms == 10000.0
        
        # Quality SLO
        assert config.disambiguation_min_pct == 10.0
        assert config.disambiguation_max_pct == 25.0
        assert config.wrong_proceed_max_pct == 1.0
        assert config.empty_sources_max_pct == 0.0
        assert config.duplicate_cards_max_count == 0
    
    def test_metrics_collector_latency(self):
        """Latency ì§€í‘œ ìˆ˜ì§‘."""
        from backend.agents.supervisor.observability import MetricsCollector, percentile
        
        collector = MetricsCollector()
        
        # greeting latency ê¸°ë¡
        for i in range(100):
            collector.record_request("smalltalk", latency_ms=50 + i)
        
        metrics = collector.get_current_metrics()
        
        # p95 ê³„ì‚° í™•ì¸
        assert metrics["latency"]["greeting_p95_ms"] > 140
        assert metrics["latency"]["greeting_p95_ms"] < 150
        assert metrics["total_requests"] == 100
    
    def test_metrics_collector_quality(self):
        """Quality ì§€í‘œ ìˆ˜ì§‘."""
        from backend.agents.supervisor.observability import MetricsCollector
        
        collector = MetricsCollector()
        
        # 100ê°œ ìš”ì²­ ì¤‘ 15ê°œ disambiguation, 0ê°œ wrong_proceed
        for i in range(100):
            collector.record_request(
                "analyze",
                latency_ms=100,
                disambiguated=(i < 15),
                wrong_proceed=False,
                sources_empty=False,
            )
        
        metrics = collector.get_current_metrics()
        
        assert metrics["quality"]["disambiguation_pct"] == 15.0
        assert metrics["quality"]["wrong_proceed_pct"] == 0.0
        assert metrics["quality"]["empty_sources_pct"] == 0.0
    
    def test_slo_checker_pass(self):
        """SLO ê²€ì‚¬ í†µê³¼."""
        from backend.agents.supervisor.observability import SLOChecker, SLOConfig
        
        config = SLOConfig()
        checker = SLOChecker(config)
        
        # ëª¨ë“  SLO í†µê³¼í•˜ëŠ” metrics
        metrics = {
            "latency": {
                "greeting_p95_ms": 50,
                "overview_p95_ms": 1000,
                "expert_p95_ms": 5000,
            },
            "quality": {
                "disambiguation_pct": 15.0,  # 10-25% ë²”ìœ„ ë‚´
                "wrong_proceed_pct": 0.5,     # < 1%
                "empty_sources_pct": 0.0,
                "duplicate_cards_count": 0,
            },
            "events": {
                "missing_count": 0,
            },
        }
        
        assert checker.is_healthy(metrics) is True
        results = checker.check_all(metrics)
        assert all(r.passed for r in results)
    
    def test_slo_checker_fail(self):
        """SLO ê²€ì‚¬ ì‹¤íŒ¨."""
        from backend.agents.supervisor.observability import SLOChecker
        
        checker = SLOChecker()
        
        # Latency SLO ìœ„ë°˜
        metrics = {
            "latency": {
                "greeting_p95_ms": 200,  # > 100ms ìœ„ë°˜
                "overview_p95_ms": 1000,
                "expert_p95_ms": 5000,
            },
            "quality": {
                "disambiguation_pct": 5.0,  # < 10% ìœ„ë°˜
                "wrong_proceed_pct": 2.0,   # > 1% ìœ„ë°˜
                "empty_sources_pct": 0.0,
                "duplicate_cards_count": 0,
            },
            "events": {
                "missing_count": 0,
            },
        }
        
        assert checker.is_healthy(metrics) is False
        results = checker.check_all(metrics)
        failed = [r for r in results if not r.passed]
        assert len(failed) >= 3


class TestCanaryDeployment:
    """Canary ë°°í¬ ë° ë¡¤ë°± í…ŒìŠ¤íŠ¸."""
    
    def test_canary_phases(self):
        """Canary ë°°í¬ ë‹¨ê³„."""
        from backend.agents.supervisor.observability import CanaryManager
        
        canary = CanaryManager()
        
        # ì´ˆê¸°: FULL
        assert canary.current_phase == CanaryManager.DeploymentPhase.FULL
        assert canary.get_traffic_percentage() == 100
    
    def test_canary_rollback(self):
        """ë¡¤ë°± ì‹œ í”¼ì²˜ ë¹„í™œì„±í™” ë° ì„ê³„ê°’ ìƒí–¥."""
        from backend.agents.supervisor.observability import CanaryManager
        
        canary = CanaryManager()
        
        # ë¡¤ë°±
        canary.rollback(
            reason="expert_p95 SLO violation",
            disable_features=["expert_runner", "agentic_planning"]
        )
        
        assert canary.current_phase == CanaryManager.DeploymentPhase.ROLLBACK
        assert canary.feature_toggles["expert_runner"] is False
        assert canary.feature_toggles["agentic_planning"] is False
        assert canary.feature_toggles["lightweight_path"] is True  # ìœ ì§€
        assert canary.threshold_override == 0.7
    
    def test_canary_effective_threshold(self):
        """ë¡¤ë°± ì‹œ ì„ê³„ê°’ ì˜¤ë²„ë¼ì´ë“œ."""
        from backend.agents.supervisor.observability import CanaryManager
        
        canary = CanaryManager()
        
        # ë¡¤ë°± ì „: ê¸°ë³¸ ì„ê³„ê°’ ì‚¬ìš©
        assert canary.get_effective_threshold(0.4) == 0.4
        
        # ë¡¤ë°± í›„: ì˜¤ë²„ë¼ì´ë“œ ì ìš©
        canary.rollback("test", disable_features=[])
        assert canary.get_effective_threshold(0.4) == 0.7
        assert canary.get_effective_threshold(0.8) == 0.8  # ë” ë†’ìœ¼ë©´ ê·¸ëŒ€ë¡œ
    
    def test_canary_promote(self):
        """ë‹¨ê³„ë³„ ìŠ¹ê²©."""
        from backend.agents.supervisor.observability import CanaryManager
        
        canary = CanaryManager()
        canary.current_phase = CanaryManager.DeploymentPhase.CANARY
        
        assert canary.get_traffic_percentage() == 10
        
        canary.promote()
        assert canary.current_phase == CanaryManager.DeploymentPhase.GRADUAL_25
        assert canary.get_traffic_percentage() == 25
        
        canary.promote()
        assert canary.get_traffic_percentage() == 50
    
    def test_feature_toggles(self):
        """í”¼ì²˜ í† ê¸€ í™•ì¸."""
        from backend.agents.supervisor.observability import CanaryManager
        
        canary = CanaryManager()
        
        # ê¸°ë³¸: ëª¨ë‘ í™œì„±í™”
        assert canary.should_use_new_feature("lightweight_path") is True
        assert canary.should_use_new_feature("followup_planner") is True
        assert canary.should_use_new_feature("expert_runner") is True
        
        # ê°œë³„ ë¹„í™œì„±í™”
        canary.feature_toggles["expert_runner"] = False
        assert canary.should_use_new_feature("expert_runner") is False


class TestErrorMeta:
    """ì—ëŸ¬ ë©”íƒ€ ê¸°ë¡ í…ŒìŠ¤íŠ¸."""
    
    def test_error_meta_retry_logic(self):
        """ì—ëŸ¬ ë©”íƒ€ ì¬ì‹œë„ ë¡œì§."""
        from backend.agents.supervisor.observability import ErrorMeta
        import time
        
        # ì¬ì‹œë„ ê°€ëŠ¥í•œ ì—ëŸ¬
        meta = ErrorMeta(
            error_type="github_api_timeout",
            message="GitHub API timeout",
            timestamp=time.time(),
            can_retry=True,
            retry_count=0,
            max_retries=3,
        )
        
        assert meta.should_retry() is True
        
        meta.retry_count = 3
        assert meta.should_retry() is False  # ìµœëŒ€ ì¬ì‹œë„ ë„ë‹¬
        
        # ì¬ì‹œë„ ë¶ˆê°€ ì—ëŸ¬
        fatal = ErrorMeta(
            error_type="invalid_repo",
            message="Repository not found",
            timestamp=time.time(),
            can_retry=False,
            retry_count=0,
        )
        assert fatal.should_retry() is False
    
    def test_error_meta_to_dict(self):
        """ì—ëŸ¬ ë©”íƒ€ ì§ë ¬í™”."""
        from backend.agents.supervisor.observability import ErrorMeta
        import time
        
        meta = ErrorMeta(
            error_type="test_error",
            message="Test message",
            timestamp=time.time(),
            can_retry=True,
            retry_count=1,
            context={"key": "value"},
        )
        
        d = meta.to_dict()
        
        assert d["error_type"] == "test_error"
        assert d["can_retry"] is True
        assert d["retry_count"] == 1
        assert d["should_retry"] is True
        assert "timestamp_iso" in d
        assert d["context"]["key"] == "value"


class TestWeeklyReport:
    """ì£¼ê°„ ë¦¬í¬íŠ¸ í…ŒìŠ¤íŠ¸."""
    
    def test_generate_weekly_report(self):
        """ì£¼ê°„ ë¦¬í¬íŠ¸ ìƒì„±."""
        from backend.agents.supervisor.observability import (
            MetricsCollector,
            generate_weekly_report,
        )
        
        collector = MetricsCollector()
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ê°€
        for _ in range(50):
            collector.record_request("smalltalk", latency_ms=50)
        for _ in range(30):
            collector.record_request("analyze", latency_ms=3000, disambiguated=True)
        for _ in range(20):
            collector.record_request("overview", latency_ms=1000)
        
        report = generate_weekly_report(collector)
        
        assert report.total_requests == 100
        assert isinstance(report.slo_results, list)
        assert len(report.slo_results) > 0
        assert isinstance(report.recommendations, list)
    
    def test_dashboard_metrics(self):
        """ëŒ€ì‹œë³´ë“œ ì§€í‘œ API."""
        from backend.agents.supervisor.observability import get_dashboard_metrics
        
        metrics = get_dashboard_metrics()
        
        assert "timestamp" in metrics
        assert "metrics" in metrics
        assert "slo_status" in metrics
        assert "deployment" in metrics
        assert "all_passed" in metrics["slo_status"]


# Compare/Onepager/Followup í…ŒìŠ¤íŠ¸
class TestCompareOnepagerFollowup:
    """Compare, One-pager, Follow-up ê²½ë¡œ í…ŒìŠ¤íŠ¸."""
    
    def test_compare_pattern_detection(self):
        """Compare íŒ¨í„´ ê°ì§€."""
        from backend.agents.supervisor.nodes.intent_classifier import (
            _detect_compare,
            _tier1_heuristic,
        )
        
        # ì •ìƒ compare ê°ì§€
        is_compare, repo_a, repo_b = _detect_compare("facebook/reactë‘ vuejs/core ë¹„êµí•´ì¤˜")
        assert is_compare is True
        assert repo_a is not None
        assert repo_b is not None
        assert repo_a["owner"] == "facebook"
        assert repo_b["owner"] == "vuejs"
        
        # heuristicì—ì„œë„ compareë¡œ ë¶„ë¥˜
        result = _tier1_heuristic("facebook/reactë‘ vuejs/core ë¹„êµí•´ì¤˜", False)
        assert result is not None
        assert result.intent == "analyze"
        assert result.sub_intent == "compare"
        assert result.compare_repo is not None
    
    def test_compare_various_patterns(self):
        """ë‹¤ì–‘í•œ compare íŒ¨í„´."""
        from backend.agents.supervisor.nodes.intent_classifier import _detect_compare
        
        patterns = [
            ("facebook/react vs vuejs/core ë¹„êµ", True),
            ("reactì™€ vue ë¹„êµí•´ì¤˜", False),  # repo í˜•ì‹ ì•„ë‹˜
            ("facebook/reactë‘ vuejs/core ë¹„êµí•´ì¤˜", True),
            ("vuejs/coreê³¼ facebook/react ë¹„êµ", True),
        ]
        
        for query, expected in patterns:
            is_compare, _, _ = _detect_compare(query)
            assert is_compare == expected, f"Failed for: {query}"
    
    def test_onepager_pattern_detection(self):
        """One-pager íŒ¨í„´ ê°ì§€."""
        from backend.agents.supervisor.nodes.intent_classifier import (
            _detect_onepager,
            _tier1_heuristic,
        )
        
        # One-pager íŒ¨í„´
        assert _detect_onepager("facebook/react í•œ ì¥ ìš”ì•½ ë§Œë“¤ì–´ì¤˜") is True
        assert _detect_onepager("ì›í˜ì´ì € ë§Œë“¤ì–´ì¤˜") is True
        assert _detect_onepager("ë°œí‘œ ìë£Œ ë§Œë“¤ì–´ì¤˜") is True
        
        # heuristicì—ì„œ onepagerë¡œ ë¶„ë¥˜
        result = _tier1_heuristic("facebook/react í•œ ì¥ ìš”ì•½ ë§Œë“¤ì–´ì¤˜", False)
        assert result is not None
        assert result.intent == "analyze"
        assert result.sub_intent == "onepager"
    
    def test_refine_pattern_detection(self):
        """Refine íŒ¨í„´ ê°ì§€ (ì§ì „ ì•„í‹°íŒ©íŠ¸ ìˆì„ ë•Œë§Œ)."""
        from backend.agents.supervisor.nodes.intent_classifier import (
            _detect_refine,
            _tier1_heuristic,
        )
        
        # ì•„í‹°íŒ©íŠ¸ ì—†ìœ¼ë©´ ê°ì§€ ì•ˆ ë¨
        assert _detect_refine("ê¸‰í•œ ê±° 3ê°œë§Œ ì •ë¦¬í•´ì¤˜", False) is False
        
        # ì•„í‹°íŒ©íŠ¸ ìˆìœ¼ë©´ ê°ì§€
        assert _detect_refine("ê¸‰í•œ ê±° 3ê°œë§Œ ì •ë¦¬í•´ì¤˜", True) is True
        assert _detect_refine("ìš°ì„ ìˆœìœ„ ì •ë ¬í•´ì¤˜", True) is True
        
        # heuristic (ì•„í‹°íŒ©íŠ¸ ìˆì„ ë•Œ)
        result = _tier1_heuristic("ê¸‰í•œ ê±° 3ê°œë§Œ ì •ë¦¬í•´ì¤˜", True)
        assert result is not None
        assert result.intent == "followup"
        assert result.sub_intent == "refine"
    
    def test_graph_routing_compare(self):
        """Compare ê²½ë¡œê°€ expertë¡œ ë¼ìš°íŒ…."""
        from backend.agents.supervisor.graph import should_run_diagnosis
        
        state = {
            "intent": "analyze",
            "sub_intent": "compare",
            "repo": {"owner": "facebook", "name": "react", "url": ""},
            "compare_repo": {"owner": "vuejs", "name": "core", "url": ""},
        }
        
        route = should_run_diagnosis(state)  # type: ignore
        assert route == "expert"
    
    def test_graph_routing_onepager(self):
        """Onepager ê²½ë¡œê°€ expertë¡œ ë¼ìš°íŒ…."""
        from backend.agents.supervisor.graph import should_run_diagnosis
        
        state = {
            "intent": "analyze",
            "sub_intent": "onepager",
            "repo": {"owner": "facebook", "name": "react", "url": ""},
        }
        
        route = should_run_diagnosis(state)  # type: ignore
        assert route == "expert"
    
    def test_graph_routing_followup(self):
        """Followup ê²½ë¡œê°€ summarizeë¡œ ë¼ìš°íŒ…."""
        from backend.agents.supervisor.graph import should_run_diagnosis
        
        state = {
            "intent": "followup",
            "sub_intent": "evidence",
        }
        
        route = should_run_diagnosis(state)  # type: ignore
        assert route == "summarize"
    
    def test_followup_evidence_detection(self):
        """Followup evidence ê°ì§€ (ì§ì „ ì•„í‹°íŒ©íŠ¸ ìˆì„ ë•Œ)."""
        from backend.agents.supervisor.nodes.intent_classifier import _tier1_heuristic
        
        # ì•„í‹°íŒ©íŠ¸ ì—†ìœ¼ë©´ followup ì•„ë‹˜
        result = _tier1_heuristic("ê·¸ ê²°ê³¼ëŠ” ì–´ë””ì„œ ë‚˜ì˜¨ ê±°ì•¼?", False)
        assert result is None or result.intent != "followup"
        
        # ì•„í‹°íŒ©íŠ¸ ìˆìœ¼ë©´ followup
        result = _tier1_heuristic("ê·¸ ê²°ê³¼ëŠ” ì–´ë””ì„œ ë‚˜ì˜¨ ê±°ì•¼?", True)
        assert result is not None
        assert result.intent == "followup"
        assert result.sub_intent == "evidence"
    
    def test_answer_kind_mapping(self):
        """ìƒˆ sub_intentë“¤ì˜ answer_kind ë§¤í•‘."""
        from backend.agents.supervisor.intent_config import get_answer_kind
        
        assert get_answer_kind("analyze", "compare") == "compare"
        assert get_answer_kind("analyze", "onepager") == "onepager"
        assert get_answer_kind("followup", "refine") == "refine"
        assert get_answer_kind("followup", "evidence") == "explain"


# ============================================================================
# ì‹œë‚˜ë¦¬ì˜¤ 7: ì—ëŸ¬Â·ì¬ê³„íš(Agentic Re-planning)
# ============================================================================
class TestAgenticReplanning:
    """ì—ëŸ¬ ë°œìƒ ì‹œ ì¬ê³„íš ë° ë³µêµ¬ í…ŒìŠ¤íŠ¸: ì •ìƒ ì¢…ë£Œìœ¨ â‰¥ 95%."""
    
    def test_rate_limit_retry_then_fallback(self):
        """ë ˆì´íŠ¸ ë¦¬ë°‹ ì‹œ retry â†’ fallback â†’ ì •ìƒ ì¢…ë£Œ."""
        from backend.agents.supervisor.planner import (
            Plan, PlanStep, PlanExecutor, PlanStatus, StepStatus, ErrorPolicy, StepResult
        )
        
        attempt_count = [0]
        
        def rate_limit_then_succeed(step, inputs):
            attempt_count[0] += 1
            if attempt_count[0] <= 2:
                return StepResult(
                    step_id=step.id,
                    status=StepStatus.FAILED,
                    error_message="rate limit exceeded",
                )
            return StepResult(
                step_id=step.id,
                status=StepStatus.SUCCESS,
                result={"recovered": True},
            )
        
        step = PlanStep(
            id="api_step",
            runner="mock",
            on_error=ErrorPolicy.RETRY,
            max_retries=2,  # 2ë²ˆ ì¬ì‹œë„
        )
        
        plan = Plan(id="rate_limit_test", intent="analyze", sub_intent="health", steps=[step])
        executor = PlanExecutor(step_executors={"mock": rate_limit_then_succeed})
        result = executor.execute(plan, {})
        
        # ìµœì¢… ì„±ê³µ
        assert attempt_count[0] == 3
        assert result.status == PlanStatus.SUCCESS
    
    def test_network_error_retry(self):
        """ë„¤íŠ¸ì›Œí¬ ì¼ì‹œ ì˜¤ë¥˜ ì‹œ ì¬ì‹œë„ í›„ ì •ìƒ ì§„í–‰."""
        from backend.agents.supervisor.planner import (
            Plan, PlanStep, PlanExecutor, PlanStatus, StepStatus, ErrorPolicy, StepResult
        )
        from backend.common.events import get_event_store, EventType
        
        event_store = get_event_store()
        event_store.clear()
        
        call_count = [0]
        
        def network_error_once(step, inputs):
            call_count[0] += 1
            if call_count[0] == 1:
                return StepResult(
                    step_id=step.id,
                    status=StepStatus.FAILED,
                    error_message="connection timeout",
                )
            return StepResult(
                step_id=step.id,
                status=StepStatus.SUCCESS,
                result={"data": "recovered"},
            )
        
        step = PlanStep(
            id="network_step",
            runner="mock",
            on_error=ErrorPolicy.RETRY,
            max_retries=1,
        )
        
        plan = Plan(id="network_test", intent="analyze", sub_intent="health", steps=[step])
        executor = PlanExecutor(step_executors={"mock": network_error_once})
        result = executor.execute(plan, {})
        
        assert result.status == PlanStatus.SUCCESS
        assert call_count[0] == 2
    
    def test_fallback_then_ask_user(self):
        """fallback ì‹¤íŒ¨ ì‹œ ask_user ì—ìŠ¤ì»¬ë ˆì´ì…˜."""
        from backend.agents.supervisor.planner import (
            Plan, PlanStep, PlanExecutor, PlanStatus, StepStatus, ErrorPolicy, StepResult
        )
        
        def always_fail(step, inputs):
            return StepResult(
                step_id=step.id,
                status=StepStatus.FAILED,
                error_message="permanent failure",
            )
        
        step = PlanStep(
            id="fail_step",
            runner="mock",
            on_error=ErrorPolicy.FALLBACK,  # fallback ì‹¤íŒ¨ ì‹œ ask_userë¡œ ì—ìŠ¤ì»¬ë ˆì´ì…˜
            max_retries=0,
        )
        
        plan = Plan(id="fallback_test", intent="analyze", sub_intent="health", steps=[step])
        executor = PlanExecutor(step_executors={"mock": always_fail})
        result = executor.execute(plan, {})
        
        # ask_userë¡œ ì¢…ë£Œ
        assert result.status in (PlanStatus.ASK_USER, PlanStatus.FAILED)
        assert result.error_message is not None
    
    def test_error_recovery_rate(self):
        """NíšŒ ì‹œë‚˜ë¦¬ì˜¤ ì¤‘ ì •ìƒ ì¢…ë£Œìœ¨ â‰¥ 95%."""
        from backend.agents.supervisor.planner import (
            Plan, PlanStep, PlanExecutor, PlanStatus, StepStatus, ErrorPolicy, StepResult
        )
        import random
        
        N = 100
        success_count = 0
        
        for i in range(N):
            fail_times = random.randint(0, 2)  # 0~2íšŒ ì‹¤íŒ¨
            call_count = [0]
            
            def intermittent_failure(step, inputs, fails=fail_times, counter=call_count):
                counter[0] += 1
                if counter[0] <= fails:
                    return StepResult(
                        step_id=step.id,
                        status=StepStatus.FAILED,
                        error_message=f"fail {counter[0]}",
                    )
                return StepResult(
                    step_id=step.id,
                    status=StepStatus.SUCCESS,
                    result={"success": True},
                )
            
            step = PlanStep(
                id=f"step_{i}",
                runner="mock",
                on_error=ErrorPolicy.RETRY,
                max_retries=2,
            )
            
            plan = Plan(id=f"test_{i}", intent="analyze", sub_intent="health", steps=[step])
            executor = PlanExecutor(step_executors={"mock": intermittent_failure})
            result = executor.execute(plan, {})
            
            # ì •ìƒ ì¢…ë£Œ: SUCCESS, PARTIAL, ASK_USER ëª¨ë‘ ì •ìƒ ì¢…ë£Œë¡œ ê°„ì£¼
            if result.status in (PlanStatus.SUCCESS, PlanStatus.PARTIAL, PlanStatus.ASK_USER):
                success_count += 1
        
        success_rate = success_count / N
        assert success_rate >= 0.95, f"ì •ìƒ ì¢…ë£Œìœ¨ {success_rate:.2%} < 95%"


# ============================================================================
# ì‹œë‚˜ë¦¬ì˜¤ 8: ì„ê³„Â·ë¼ìš°íŒ… íŠœë‹
# ============================================================================
class TestRoutingThresholds:
    """ê²½ê³„ ì…ë ¥ ë° ì„ê³„ê°’ ê²€ì¦."""
    
    def test_ambiguous_query_no_expert_misroute(self):
        """ëª¨í˜¸í•œ ì§ˆì˜ê°€ ì „ë¬¸ê°€ ë…¸ë“œë¡œ ì˜¤ì§„ì…í•˜ì§€ ì•ŠìŒ."""
        from backend.agents.supervisor.nodes.intent_classifier import _tier1_heuristic
        
        ambiguous_queries = [
            "react ë¬¸ì„œ ì–´ë–¤ í¸ì´ì•¼?",
            "vue ê´œì°®ì•„?",
            "ê·¸ê²Œ ë­ì•¼?",
        ]
        
        for query in ambiguous_queries:
            result = _tier1_heuristic(query, False)
            # ëª¨í˜¸í•œ ì§ˆì˜ëŠ” tier1ì—ì„œ None (LLMìœ¼ë¡œ ìœ„ì„) ë˜ëŠ” help/overviewë¡œ ë¶„ë¥˜
            if result is not None:
                assert result.sub_intent not in ("health", "onboarding", "compare"), \
                    f"ëª¨í˜¸í•œ ì§ˆì˜ '{query}'ê°€ ì§„ë‹¨ ë…¸ë“œë¡œ ì˜¤ì§„ì…: {result.sub_intent}"
    
    def test_disambiguation_rate_target(self):
        """Disambiguation Rate ëª©í‘œ ë²”ìœ„ (10-25%) ê²€ì¦."""
        from backend.agents.supervisor.calibration import (
            CalibrationStore,
            DISAMBIGUATION_TARGET_MIN,
            DISAMBIGUATION_TARGET_MAX,
        )
        
        store = CalibrationStore()
        
        # ëª©í‘œ ë²”ìœ„ í™•ì¸
        assert DISAMBIGUATION_TARGET_MIN == 0.10
        assert DISAMBIGUATION_TARGET_MAX == 0.25
    
    def test_wrong_proceed_rate(self):
        """Wrong-Proceed < 1% ê²€ì¦ ë¡œì§."""
        from backend.agents.supervisor.calibration import CalibrationStore
        
        store = CalibrationStore()
        
        # 100ê°œ ì¤‘ 0ê°œ wrong proceed
        for _ in range(100):
            store.record("analyze", "health", proceed=True, correct=True)
        
        metrics = store.get_metrics()
        wrong_proceed_rate = metrics.get("wrong_proceed_rate", 0)
        
        assert wrong_proceed_rate < 0.01, f"Wrong-Proceed {wrong_proceed_rate:.2%} >= 1%"


# ============================================================================
# ì‹œë‚˜ë¦¬ì˜¤ 9: ì•„ì´ë¤í¬í„´ì‹œÂ·ì¤‘ë³µ ë°©ì§€
# ============================================================================
class TestIdempotencyAdvanced:
    """ê³ ê¸‰ ì•„ì´ë¤í¬í„´ì‹œ í…ŒìŠ¤íŠ¸."""
    
    def test_duplicate_request_same_answer_id(self):
        """ë™ì¼ í”„ë¡¬í”„íŠ¸ ë‘ ë²ˆ ì „ì†¡ ì‹œ answer_id ë™ì¼."""
        from backend.agents.supervisor.models import IdempotencyStore
        
        store = IdempotencyStore()
        
        turn_id = "turn_001"
        step_id = "classify"
        result1 = {"intent": "analyze"}
        result2 = {"intent": "analyze"}  # ë™ì¼ ê²°ê³¼
        
        # ì²« ë²ˆì§¸ ì €ì¥
        answer_id1 = store.store_result(turn_id, step_id, result1)
        
        # ë™ì¼ í‚¤ë¡œ ë‘ ë²ˆì§¸ ì‹œë„ - ì´ë¯¸ ìˆìœ¼ë©´ ê¸°ì¡´ ê²ƒ ë°˜í™˜
        existing = store.get_result(turn_id, step_id)
        assert existing is not None
        assert existing == result1
    
    def test_llm_schema_failure_then_retry_success(self):
        """LLM ìŠ¤í‚¤ë§ˆ ì‹¤íŒ¨ í›„ ì¬ì‹œë„ ì„±ê³µ ì‹œ 1ê°œ ì‘ë‹µë§Œ."""
        from backend.agents.supervisor.planner import (
            Plan, PlanStep, PlanExecutor, PlanStatus, StepStatus, ErrorPolicy, StepResult
        )
        from backend.common.events import get_event_store, EventType
        
        event_store = get_event_store()
        event_store.clear()
        
        call_count = [0]
        results_generated = []
        
        def schema_fail_then_success(step, inputs):
            call_count[0] += 1
            if call_count[0] == 1:
                result = StepResult(
                    step_id=step.id,
                    status=StepStatus.FAILED,
                    error_message="JSON schema validation failed",
                )
                results_generated.append(("fail", result))
                return result
            
            result = StepResult(
                step_id=step.id,
                status=StepStatus.SUCCESS,
                result={"response": "valid"},
            )
            results_generated.append(("success", result))
            return result
        
        step = PlanStep(
            id="llm_step",
            runner="mock",
            on_error=ErrorPolicy.RETRY,
            max_retries=1,
        )
        
        plan = Plan(id="schema_test", intent="analyze", sub_intent="health", steps=[step])
        executor = PlanExecutor(step_executors={"mock": schema_fail_then_success})
        result = executor.execute(plan, {})
        
        # ìµœì¢… ì„±ê³µ
        assert result.status == PlanStatus.SUCCESS
        
        # ì´ë²¤íŠ¸ì—ëŠ” ì‹¤íŒ¨/ì„±ê³µ ëª¨ë‘ ê¸°ë¡
        assert len(results_generated) == 2
        assert results_generated[0][0] == "fail"
        assert results_generated[1][0] == "success"
    
    def test_no_duplicate_cards(self):
        """ì¤‘ë³µ ì¹´ë“œ 0ê±´ ê²€ì¦."""
        from backend.agents.supervisor.models import IdempotencyStore
        
        store = IdempotencyStore()
        
        # ë™ì¼ í„´ì—ì„œ ë™ì¼ ìŠ¤í… ê²°ê³¼ëŠ” 1ë²ˆë§Œ ì €ì¥
        turn_id = "turn_dup"
        step_id = "summarize"
        
        answer_id1 = store.store_result(turn_id, step_id, {"card": 1})
        answer_id2 = store.store_result(turn_id, step_id, {"card": 2})  # ë®ì–´ì”Œì›Œì§
        
        # ê²°ê³¼ ì¡°íšŒ ì‹œ 1ê°œë§Œ ë°˜í™˜
        result = store.get_result(turn_id, step_id)
        assert result is not None
        # ë§ˆì§€ë§‰ ê²°ê³¼ë§Œ ì¡´ì¬
        assert result["card"] == 2


# ============================================================================
# ì‹œë‚˜ë¦¬ì˜¤ 10: ìš´ì˜ ì§€í‘œ ì§‘ê³„
# ============================================================================
class TestOperationalMetrics:
    """ìš´ì˜ ì§€í‘œ í…ŒìŠ¤íŠ¸: p95 ë ˆì´í„´ì‹œ, Disambiguation, sources ë“±."""
    
    def test_greeting_latency_p95(self):
        """ì¸ì‚¬ p95 < 100ms."""
        from backend.agents.supervisor.observability import MetricsCollector
        
        collector = MetricsCollector()
        
        # 100ê°œ ìƒ˜í”Œ - ëŒ€ë¶€ë¶„ 50ms ì´í•˜
        for i in range(100):
            latency = 30 + (i % 40)  # 30~70ms
            collector.record_latency("greeting", latency)
        
        p95 = collector.get_percentile("greeting", 95)
        assert p95 < 100, f"ì¸ì‚¬ p95 {p95}ms >= 100ms"
    
    def test_overview_latency_p95(self):
        """ê°œìš” p95 â‰¤ 1.5s."""
        from backend.agents.supervisor.observability import MetricsCollector
        
        collector = MetricsCollector()
        
        # 100ê°œ ìƒ˜í”Œ - ëŒ€ë¶€ë¶„ 1ì´ˆ ì´í•˜
        for i in range(100):
            latency = 500 + (i * 10)  # 500~1500ms
            collector.record_latency("overview", latency)
        
        p95 = collector.get_percentile("overview", 95)
        assert p95 <= 1500, f"ê°œìš” p95 {p95}ms > 1500ms"
    
    def test_sources_never_empty(self):
        """sources == [] 0% ê²€ì¦."""
        from backend.agents.supervisor.runners.base import validate_runner_output
        
        # ë¹ˆ sourcesëŠ” validation ì‹¤íŒ¨
        output_without_sources = {
            "summary": "test",
            "sources": [],
        }
        
        is_valid, errors = validate_runner_output(output_without_sources)
        assert not is_valid
        assert "sources" in str(errors).lower()
    
    def test_metrics_dashboard_structure(self):
        """ëŒ€ì‹œë³´ë“œ ë©”íŠ¸ë¦­ êµ¬ì¡° ê²€ì¦."""
        from backend.agents.supervisor.observability import (
            SLOChecker,
            DEFAULT_SLOS,
        )
        
        checker = SLOChecker(DEFAULT_SLOS)
        
        # í•„ìˆ˜ ë©”íŠ¸ë¦­ í‚¤
        required_keys = {
            "greeting_latency_p95",
            "overview_latency_p95",
            "disambiguation_rate",
            "wrong_proceed_rate",
            "sources_empty_rate",
            "duplicate_card_rate",
            "error_recovery_rate",
        }
        
        # DEFAULT_SLOSì— ëª¨ë“  í•„ìˆ˜ í‚¤ í¬í•¨
        slo_keys = set(DEFAULT_SLOS.keys())
        missing = required_keys - slo_keys
        assert not missing, f"ëˆ„ë½ëœ SLO í‚¤: {missing}"


if __name__ == "__main__":
    pytest.main([__file__, "-v"])

